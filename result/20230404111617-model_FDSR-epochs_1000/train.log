2023-04-04 11:17:18,785 ===> Epoch[1/1000]
2023-04-04 11:18:18,207 >>> Training Loss: 0.000021913350793 ### Testing Loss: 0.000004028447620
2023-04-04 11:18:18,209 ===> Epoch[2/1000]
2023-04-04 11:19:00,426 >>> Training Loss: 0.000009385345038 ### Testing Loss: 0.000003895399004
2023-04-04 11:19:00,426 ===> Epoch[3/1000]
2023-04-04 11:19:41,767 >>> Training Loss: 0.000008827239071 ### Testing Loss: 0.000003869514330
2023-04-04 11:19:41,767 ===> Epoch[4/1000]
2023-04-04 11:20:24,035 >>> Training Loss: 0.000009457939086 ### Testing Loss: 0.000003871654371
2023-04-04 11:20:24,036 ===> Epoch[5/1000]
2023-04-04 11:21:07,028 >>> Training Loss: 0.000008592019185 ### Testing Loss: 0.000003515898015
2023-04-04 11:21:07,028 ===> Epoch[6/1000]
2023-04-04 11:21:50,206 >>> Training Loss: 0.000007888067557 ### Testing Loss: 0.000003364931217
2023-04-04 11:21:50,206 ===> Epoch[7/1000]
2023-04-04 11:22:32,947 >>> Training Loss: 0.000006867548109 ### Testing Loss: 0.000002996155899
2023-04-04 11:22:32,947 ===> Epoch[8/1000]
2023-04-04 11:23:15,589 >>> Training Loss: 0.000008013023944 ### Testing Loss: 0.000002609420108
2023-04-04 11:23:15,589 ===> Epoch[9/1000]
2023-04-04 11:23:58,308 >>> Training Loss: 0.000005569725545 ### Testing Loss: 0.000002412084314
2023-04-04 11:23:58,308 ===> Epoch[10/1000]
2023-04-04 11:24:41,377 >>> Training Loss: 0.000005410229733 ### Testing Loss: 0.000002267857326
2023-04-04 11:24:41,378 ===> Epoch[11/1000]
2023-04-04 11:25:24,509 >>> Training Loss: 0.000005147754109 ### Testing Loss: 0.000002350907835
2023-04-04 11:25:24,509 ===> Epoch[12/1000]
2023-04-04 11:26:07,736 >>> Training Loss: 0.000004972439910 ### Testing Loss: 0.000002245622454
2023-04-04 11:26:07,737 ===> Epoch[13/1000]
2023-04-04 11:26:51,298 >>> Training Loss: 0.000004936478945 ### Testing Loss: 0.000006156005384
2023-04-04 11:26:51,298 ===> Epoch[14/1000]
2023-04-04 11:27:35,048 >>> Training Loss: 0.000004514650300 ### Testing Loss: 0.000001983617040
2023-04-04 11:27:35,048 ===> Epoch[15/1000]
2023-04-04 11:28:18,828 >>> Training Loss: 0.000004296970019 ### Testing Loss: 0.000001921143166
2023-04-04 11:28:18,829 ===> Epoch[16/1000]
2023-04-04 11:29:02,736 >>> Training Loss: 0.000004337934115 ### Testing Loss: 0.000001969581717
2023-04-04 11:29:02,736 ===> Epoch[17/1000]
2023-04-04 11:29:46,715 >>> Training Loss: 0.000004037337021 ### Testing Loss: 0.000001856327799
2023-04-04 11:29:46,716 ===> Epoch[18/1000]
2023-04-04 11:30:30,627 >>> Training Loss: 0.000004301639365 ### Testing Loss: 0.000001837639957
2023-04-04 11:30:30,627 ===> Epoch[19/1000]
2023-04-04 11:31:14,576 >>> Training Loss: 0.000003793835504 ### Testing Loss: 0.000001767827598
2023-04-04 11:31:14,576 ===> Epoch[20/1000]
2023-04-04 11:31:58,668 >>> Training Loss: 0.000003765917882 ### Testing Loss: 0.000001764836384
2023-04-04 11:31:58,669 ===> Epoch[21/1000]
2023-04-04 11:32:42,647 >>> Training Loss: 0.000003775748837 ### Testing Loss: 0.000001725558718
2023-04-04 11:32:42,647 ===> Epoch[22/1000]
2023-04-04 11:33:26,770 >>> Training Loss: 0.000003654077318 ### Testing Loss: 0.000001683437290
2023-04-04 11:33:26,770 ===> Epoch[23/1000]
2023-04-04 11:34:11,492 >>> Training Loss: 0.000003508391501 ### Testing Loss: 0.000001638746994
2023-04-04 11:34:11,492 ===> Epoch[24/1000]
2023-04-04 11:34:55,636 >>> Training Loss: 0.000003403921710 ### Testing Loss: 0.000001610963409
2023-04-04 11:34:55,636 ===> Epoch[25/1000]
2023-04-04 11:35:39,837 >>> Training Loss: 0.000003359300308 ### Testing Loss: 0.000001573251666
2023-04-04 11:35:39,837 ===> Epoch[26/1000]
2023-04-04 11:36:24,005 >>> Training Loss: 0.000003787014066 ### Testing Loss: 0.000001738765263
2023-04-04 11:36:24,006 ===> Epoch[27/1000]
2023-04-04 11:37:08,269 >>> Training Loss: 0.000003270060233 ### Testing Loss: 0.000001553236075
2023-04-04 11:37:08,269 ===> Epoch[28/1000]
2023-04-04 11:37:52,622 >>> Training Loss: 0.000003145198207 ### Testing Loss: 0.000001510359311
2023-04-04 11:37:52,622 ===> Epoch[29/1000]
2023-04-04 11:38:37,044 >>> Training Loss: 0.000003076700750 ### Testing Loss: 0.000001502320401
2023-04-04 11:38:37,044 ===> Epoch[30/1000]
2023-04-04 11:39:21,379 >>> Training Loss: 0.000003146602921 ### Testing Loss: 0.000001494266940
2023-04-04 11:39:21,379 ===> Epoch[31/1000]
2023-04-04 11:40:05,656 >>> Training Loss: 0.000002979363899 ### Testing Loss: 0.000001481900654
2023-04-04 11:40:05,656 ===> Epoch[32/1000]
2023-04-04 11:40:50,040 >>> Training Loss: 0.000002975532198 ### Testing Loss: 0.000001705266641
2023-04-04 11:40:50,040 ===> Epoch[33/1000]
2023-04-04 11:41:34,486 >>> Training Loss: 0.000002944487505 ### Testing Loss: 0.000001430592874
2023-04-04 11:41:34,486 ===> Epoch[34/1000]
2023-04-04 11:42:19,023 >>> Training Loss: 0.000002870573098 ### Testing Loss: 0.000001413990276
2023-04-04 11:42:19,023 ===> Epoch[35/1000]
2023-04-04 11:43:03,520 >>> Training Loss: 0.000003083492402 ### Testing Loss: 0.000001527190307
2023-04-04 11:43:03,520 ===> Epoch[36/1000]
2023-04-04 11:43:48,029 >>> Training Loss: 0.000002772622565 ### Testing Loss: 0.000001371537337
2023-04-04 11:43:48,029 ===> Epoch[37/1000]
2023-04-04 11:44:32,559 >>> Training Loss: 0.000002659573738 ### Testing Loss: 0.000001393197863
2023-04-04 11:44:32,559 ===> Epoch[38/1000]
2023-04-04 11:45:17,194 >>> Training Loss: 0.000002678853889 ### Testing Loss: 0.000001353694870
2023-04-04 11:45:17,194 ===> Epoch[39/1000]
2023-04-04 11:46:01,835 >>> Training Loss: 0.000002626662535 ### Testing Loss: 0.000001404024147
2023-04-04 11:46:01,835 ===> Epoch[40/1000]
2023-04-04 11:46:46,357 >>> Training Loss: 0.000002610624506 ### Testing Loss: 0.000001328044277
2023-04-04 11:46:46,357 ===> Epoch[41/1000]
2023-04-04 11:47:30,932 >>> Training Loss: 0.000002588416692 ### Testing Loss: 0.000001685375651
2023-04-04 11:47:30,932 ===> Epoch[42/1000]
2023-04-04 11:48:15,647 >>> Training Loss: 0.000002665624834 ### Testing Loss: 0.000001308857577
2023-04-04 11:48:15,647 ===> Epoch[43/1000]
2023-04-04 11:49:01,139 >>> Training Loss: 0.000002469522997 ### Testing Loss: 0.000001271987344
2023-04-04 11:49:01,139 ===> Epoch[44/1000]
2023-04-04 11:49:45,770 >>> Training Loss: 0.000002483748176 ### Testing Loss: 0.000001264597927
2023-04-04 11:49:45,770 ===> Epoch[45/1000]
2023-04-04 11:50:30,494 >>> Training Loss: 0.000002433370810 ### Testing Loss: 0.000001253697747
2023-04-04 11:50:30,494 ===> Epoch[46/1000]
2023-04-04 11:51:15,190 >>> Training Loss: 0.000002381810418 ### Testing Loss: 0.000001278533432
2023-04-04 11:51:15,190 ===> Epoch[47/1000]
2023-04-04 11:51:59,799 >>> Training Loss: 0.000002458738891 ### Testing Loss: 0.000001242476969
2023-04-04 11:51:59,799 ===> Epoch[48/1000]
2023-04-04 11:52:44,443 >>> Training Loss: 0.000002379532361 ### Testing Loss: 0.000001235266382
2023-04-04 11:52:44,443 ===> Epoch[49/1000]
2023-04-04 11:53:29,122 >>> Training Loss: 0.000002371929440 ### Testing Loss: 0.000001219607839
2023-04-04 11:53:29,122 ===> Epoch[50/1000]
2023-04-04 11:54:15,019 >>> Training Loss: 0.000002273320888 ### Testing Loss: 0.000001210420464
2023-04-04 11:54:15,019 ===> Epoch[51/1000]
2023-04-04 11:55:01,196 >>> Training Loss: 0.000002804709538 ### Testing Loss: 0.000001261014745
2023-04-04 11:55:01,196 ===> Epoch[52/1000]
2023-04-04 11:55:47,243 >>> Training Loss: 0.000002298169647 ### Testing Loss: 0.000001184869916
2023-04-04 11:55:47,243 ===> Epoch[53/1000]
2023-04-04 11:56:33,219 >>> Training Loss: 0.000002201922825 ### Testing Loss: 0.000001200937731
2023-04-04 11:56:33,219 ===> Epoch[54/1000]
2023-04-04 11:57:19,492 >>> Training Loss: 0.000002177891929 ### Testing Loss: 0.000001191513888
2023-04-04 11:57:19,492 ===> Epoch[55/1000]
2023-04-04 11:58:05,820 >>> Training Loss: 0.000002331782298 ### Testing Loss: 0.000001226894142
2023-04-04 11:58:05,820 ===> Epoch[56/1000]
2023-04-04 11:58:52,137 >>> Training Loss: 0.000002257879942 ### Testing Loss: 0.000001246917691
2023-04-04 11:58:52,137 ===> Epoch[57/1000]
2023-04-04 11:59:38,635 >>> Training Loss: 0.000002161578323 ### Testing Loss: 0.000001148903607
2023-04-04 11:59:38,635 ===> Epoch[58/1000]
2023-04-04 12:00:24,484 >>> Training Loss: 0.000002129028189 ### Testing Loss: 0.000001142726205
2023-04-04 12:00:24,484 ===> Epoch[59/1000]
2023-04-04 12:01:10,528 >>> Training Loss: 0.000002289136546 ### Testing Loss: 0.000001137167715
2023-04-04 12:01:10,528 ===> Epoch[60/1000]
2023-04-04 12:01:56,031 >>> Training Loss: 0.000002057082838 ### Testing Loss: 0.000001187302018
2023-04-04 12:01:56,031 ===> Epoch[61/1000]
2023-04-04 12:02:41,338 >>> Training Loss: 0.000002084911330 ### Testing Loss: 0.000001162985086
2023-04-04 12:02:41,338 ===> Epoch[62/1000]
2023-04-04 12:03:26,633 >>> Training Loss: 0.000002073570840 ### Testing Loss: 0.000001107061507
2023-04-04 12:03:26,633 ===> Epoch[63/1000]
2023-04-04 12:04:11,977 >>> Training Loss: 0.000002001471103 ### Testing Loss: 0.000001154887514
2023-04-04 12:04:11,977 ===> Epoch[64/1000]
2023-04-04 12:04:56,599 >>> Training Loss: 0.000002010246590 ### Testing Loss: 0.000001126358711
2023-04-04 12:04:56,599 ===> Epoch[65/1000]
2023-04-04 12:05:41,303 >>> Training Loss: 0.000001975935902 ### Testing Loss: 0.000001105010142
2023-04-04 12:05:41,303 ===> Epoch[66/1000]
2023-04-04 12:06:26,028 >>> Training Loss: 0.000002275000497 ### Testing Loss: 0.000001138306288
2023-04-04 12:06:26,028 ===> Epoch[67/1000]
2023-04-04 12:07:10,711 >>> Training Loss: 0.000001926158575 ### Testing Loss: 0.000001084004339
2023-04-04 12:07:10,711 ===> Epoch[68/1000]
2023-04-04 12:07:55,396 >>> Training Loss: 0.000001907357500 ### Testing Loss: 0.000001096123128
2023-04-04 12:07:55,396 ===> Epoch[69/1000]
2023-04-04 12:08:40,033 >>> Training Loss: 0.000001880157697 ### Testing Loss: 0.000001065445872
2023-04-04 12:08:40,033 ===> Epoch[70/1000]
2023-04-04 12:09:24,750 >>> Training Loss: 0.000001928212214 ### Testing Loss: 0.000001069467999
2023-04-04 12:09:24,750 ===> Epoch[71/1000]
2023-04-04 12:10:09,312 >>> Training Loss: 0.000001875394105 ### Testing Loss: 0.000001092089633
2023-04-04 12:10:09,312 ===> Epoch[72/1000]
2023-04-04 12:10:53,866 >>> Training Loss: 0.000001927617859 ### Testing Loss: 0.000001083380198
2023-04-04 12:10:53,867 ===> Epoch[73/1000]
2023-04-04 12:11:38,526 >>> Training Loss: 0.000001883367304 ### Testing Loss: 0.000001081330083
2023-04-04 12:11:38,527 ===> Epoch[74/1000]
2023-04-04 12:12:23,139 >>> Training Loss: 0.000001865741297 ### Testing Loss: 0.000001103066211
2023-04-04 12:12:23,139 ===> Epoch[75/1000]
2023-04-04 12:13:07,740 >>> Training Loss: 0.000001829978260 ### Testing Loss: 0.000001059032570
2023-04-04 12:13:07,740 ===> Epoch[76/1000]
2023-04-04 12:13:52,379 >>> Training Loss: 0.000001853401727 ### Testing Loss: 0.000001046925149
2023-04-04 12:13:52,379 ===> Epoch[77/1000]
2023-04-04 12:14:36,932 >>> Training Loss: 0.000001806692921 ### Testing Loss: 0.000001046570787
2023-04-04 12:14:36,932 ===> Epoch[78/1000]
2023-04-04 12:15:21,192 >>> Training Loss: 0.000001773396889 ### Testing Loss: 0.000001031159059
2023-04-04 12:15:21,192 ===> Epoch[79/1000]
2023-04-04 12:16:05,422 >>> Training Loss: 0.000001806031378 ### Testing Loss: 0.000001032571163
2023-04-04 12:16:05,422 ===> Epoch[80/1000]
2023-04-04 12:16:50,935 >>> Training Loss: 0.000001740963285 ### Testing Loss: 0.000001176936053
2023-04-04 12:16:50,936 ===> Epoch[81/1000]
2023-04-04 12:17:36,203 >>> Training Loss: 0.000001758744020 ### Testing Loss: 0.000001019558113
2023-04-04 12:17:36,203 ===> Epoch[82/1000]
2023-04-04 12:18:21,719 >>> Training Loss: 0.000001759474230 ### Testing Loss: 0.000001033089006
2023-04-04 12:18:21,719 ===> Epoch[83/1000]
2023-04-04 12:19:07,869 >>> Training Loss: 0.000001797712571 ### Testing Loss: 0.000001058266321
2023-04-04 12:19:07,870 ===> Epoch[84/1000]
2023-04-04 12:19:53,115 >>> Training Loss: 0.000001770266977 ### Testing Loss: 0.000001021140520
2023-04-04 12:19:53,115 ===> Epoch[85/1000]
2023-04-04 12:20:38,397 >>> Training Loss: 0.000001718590283 ### Testing Loss: 0.000001078512923
2023-04-04 12:20:38,397 ===> Epoch[86/1000]
2023-04-04 12:21:23,735 >>> Training Loss: 0.000002052570153 ### Testing Loss: 0.000001028063252
2023-04-04 12:21:23,735 ===> Epoch[87/1000]
2023-04-04 12:22:09,015 >>> Training Loss: 0.000001736027684 ### Testing Loss: 0.000001020661443
2023-04-04 12:22:09,015 ===> Epoch[88/1000]
2023-04-04 12:22:54,351 >>> Training Loss: 0.000001685666689 ### Testing Loss: 0.000001014547593
2023-04-04 12:22:54,352 ===> Epoch[89/1000]
2023-04-04 12:23:39,759 >>> Training Loss: 0.000001844574626 ### Testing Loss: 0.000001018904186
2023-04-04 12:23:39,760 ===> Epoch[90/1000]
2023-04-04 12:24:25,177 >>> Training Loss: 0.000001684341100 ### Testing Loss: 0.000001016505166
2023-04-04 12:24:25,177 ===> Epoch[91/1000]
2023-04-04 12:25:10,543 >>> Training Loss: 0.000001661935585 ### Testing Loss: 0.000001003940042
2023-04-04 12:25:10,543 ===> Epoch[92/1000]
2023-04-04 12:25:55,926 >>> Training Loss: 0.000001652890774 ### Testing Loss: 0.000000998387463
2023-04-04 12:25:55,926 ===> Epoch[93/1000]
2023-04-04 12:26:41,361 >>> Training Loss: 0.000001643068458 ### Testing Loss: 0.000001015865450
2023-04-04 12:26:41,361 ===> Epoch[94/1000]
2023-04-04 12:27:26,736 >>> Training Loss: 0.000001659670829 ### Testing Loss: 0.000001028203769
2023-04-04 12:27:26,736 ===> Epoch[95/1000]
2023-04-04 12:28:12,229 >>> Training Loss: 0.000001635920626 ### Testing Loss: 0.000000997873599
2023-04-04 12:28:12,229 ===> Epoch[96/1000]
2023-04-04 12:28:57,569 >>> Training Loss: 0.000001691258944 ### Testing Loss: 0.000001032314003
2023-04-04 12:28:57,569 ===> Epoch[97/1000]
2023-04-04 12:29:42,964 >>> Training Loss: 0.000001620812100 ### Testing Loss: 0.000000995682512
2023-04-04 12:29:42,964 ===> Epoch[98/1000]
2023-04-04 12:30:28,358 >>> Training Loss: 0.000001599748202 ### Testing Loss: 0.000001010006827
2023-04-04 12:30:28,358 ===> Epoch[99/1000]
2023-04-04 12:31:13,798 >>> Training Loss: 0.000001584880579 ### Testing Loss: 0.000001009989546
2023-04-04 12:31:13,799 ===> Epoch[100/1000]
2023-04-04 12:31:59,173 >>> Training Loss: 0.000001640314395 ### Testing Loss: 0.000000973232432
2023-04-04 12:31:59,246 ===> Epoch[101/1000]
2023-04-04 12:32:44,732 >>> Training Loss: 0.000001597021537 ### Testing Loss: 0.000000987490012
2023-04-04 12:32:44,732 ===> Epoch[102/1000]
2023-04-04 12:33:31,650 >>> Training Loss: 0.000001588217401 ### Testing Loss: 0.000000979534889
2023-04-04 12:33:31,651 ===> Epoch[103/1000]
2023-04-04 12:34:16,770 >>> Training Loss: 0.000001567622917 ### Testing Loss: 0.000000965420440
2023-04-04 12:34:16,770 ===> Epoch[104/1000]
2023-04-04 12:35:02,094 >>> Training Loss: 0.000001656237600 ### Testing Loss: 0.000001073728640
2023-04-04 12:35:02,094 ===> Epoch[105/1000]
2023-04-04 12:35:47,498 >>> Training Loss: 0.000001561018394 ### Testing Loss: 0.000000969660505
2023-04-04 12:35:47,499 ===> Epoch[106/1000]
2023-04-04 12:36:32,917 >>> Training Loss: 0.000001552421281 ### Testing Loss: 0.000000987263547
2023-04-04 12:36:32,917 ===> Epoch[107/1000]
2023-04-04 12:37:18,326 >>> Training Loss: 0.000001536242053 ### Testing Loss: 0.000000959441763
2023-04-04 12:37:18,326 ===> Epoch[108/1000]
2023-04-04 12:38:03,604 >>> Training Loss: 0.000001570821496 ### Testing Loss: 0.000000959860131
2023-04-04 12:38:03,604 ===> Epoch[109/1000]
2023-04-04 12:38:48,996 >>> Training Loss: 0.000001532123633 ### Testing Loss: 0.000000990222134
2023-04-04 12:38:48,996 ===> Epoch[110/1000]
2023-04-04 12:39:34,372 >>> Training Loss: 0.000001520297133 ### Testing Loss: 0.000000962882382
2023-04-04 12:39:34,373 ===> Epoch[111/1000]
2023-04-04 12:40:19,742 >>> Training Loss: 0.000001511996516 ### Testing Loss: 0.000000998152132
2023-04-04 12:40:19,742 ===> Epoch[112/1000]
2023-04-04 12:41:05,067 >>> Training Loss: 0.000001511270852 ### Testing Loss: 0.000000965843697
2023-04-04 12:41:05,067 ===> Epoch[113/1000]
2023-04-04 12:41:50,360 >>> Training Loss: 0.000001537050025 ### Testing Loss: 0.000001027172857
2023-04-04 12:41:50,361 ===> Epoch[114/1000]
2023-04-04 12:42:35,737 >>> Training Loss: 0.000001498205393 ### Testing Loss: 0.000000971204145
2023-04-04 12:42:35,738 ===> Epoch[115/1000]
2023-04-04 12:43:20,998 >>> Training Loss: 0.000001521240506 ### Testing Loss: 0.000000958779765
2023-04-04 12:43:20,999 ===> Epoch[116/1000]
2023-04-04 12:44:06,486 >>> Training Loss: 0.000001480588480 ### Testing Loss: 0.000000950184130
2023-04-04 12:44:06,486 ===> Epoch[117/1000]
2023-04-04 12:44:51,768 >>> Training Loss: 0.000001602500220 ### Testing Loss: 0.000001287555961
2023-04-04 12:44:51,768 ===> Epoch[118/1000]
2023-04-04 12:45:37,087 >>> Training Loss: 0.000001514090286 ### Testing Loss: 0.000000967003416
2023-04-04 12:45:37,088 ===> Epoch[119/1000]
2023-04-04 12:46:22,353 >>> Training Loss: 0.000001614718485 ### Testing Loss: 0.000000966212497
2023-04-04 12:46:22,354 ===> Epoch[120/1000]
2023-04-04 12:47:07,624 >>> Training Loss: 0.000001473898578 ### Testing Loss: 0.000000950721358
2023-04-04 12:47:07,624 ===> Epoch[121/1000]
2023-04-04 12:47:52,853 >>> Training Loss: 0.000001442535108 ### Testing Loss: 0.000000938788219
2023-04-04 12:47:52,853 ===> Epoch[122/1000]
2023-04-04 12:48:39,335 >>> Training Loss: 0.000001434943101 ### Testing Loss: 0.000000938446362
2023-04-04 12:48:39,336 ===> Epoch[123/1000]
2023-04-04 12:49:24,579 >>> Training Loss: 0.000001446626811 ### Testing Loss: 0.000000952486687
2023-04-04 12:49:24,579 ===> Epoch[124/1000]
2023-04-04 12:50:09,786 >>> Training Loss: 0.000001450734089 ### Testing Loss: 0.000000946710884
2023-04-04 12:50:09,786 ===> Epoch[125/1000]
2023-04-04 12:50:55,106 >>> Training Loss: 0.000001432278850 ### Testing Loss: 0.000000930075032
2023-04-04 12:50:55,106 ===> Epoch[126/1000]
2023-04-04 12:51:40,358 >>> Training Loss: 0.000001473390626 ### Testing Loss: 0.000001130521468
2023-04-04 12:51:40,358 ===> Epoch[127/1000]
2023-04-04 12:52:25,543 >>> Training Loss: 0.000001411852395 ### Testing Loss: 0.000000934852267
2023-04-04 12:52:25,544 ===> Epoch[128/1000]
2023-04-04 12:53:10,729 >>> Training Loss: 0.000001403837587 ### Testing Loss: 0.000000921499236
2023-04-04 12:53:10,729 ===> Epoch[129/1000]
2023-04-04 12:53:56,065 >>> Training Loss: 0.000001459794817 ### Testing Loss: 0.000000952332812
2023-04-04 12:53:56,066 ===> Epoch[130/1000]
2023-04-04 12:54:41,239 >>> Training Loss: 0.000001393456500 ### Testing Loss: 0.000000923864889
2023-04-04 12:54:41,239 ===> Epoch[131/1000]
2023-04-04 12:55:26,398 >>> Training Loss: 0.000001511971050 ### Testing Loss: 0.000000931191153
2023-04-04 12:55:26,398 ===> Epoch[132/1000]
2023-04-04 12:56:11,651 >>> Training Loss: 0.000001380854656 ### Testing Loss: 0.000000942801421
2023-04-04 12:56:11,651 ===> Epoch[133/1000]
2023-04-04 12:56:56,851 >>> Training Loss: 0.000001386669283 ### Testing Loss: 0.000000916867009
2023-04-04 12:56:56,851 ===> Epoch[134/1000]
2023-04-04 12:57:42,104 >>> Training Loss: 0.000001400368774 ### Testing Loss: 0.000001280709171
2023-04-04 12:57:42,105 ===> Epoch[135/1000]
2023-04-04 12:58:27,296 >>> Training Loss: 0.000001485646408 ### Testing Loss: 0.000000928404233
2023-04-04 12:58:27,296 ===> Epoch[136/1000]
2023-04-04 12:59:12,496 >>> Training Loss: 0.000001382185474 ### Testing Loss: 0.000000936140566
2023-04-04 12:59:12,496 ===> Epoch[137/1000]
2023-04-04 12:59:57,664 >>> Training Loss: 0.000001375598117 ### Testing Loss: 0.000000961936848
2023-04-04 12:59:57,664 ===> Epoch[138/1000]
2023-04-04 13:00:42,950 >>> Training Loss: 0.000001393567118 ### Testing Loss: 0.000000928591817
2023-04-04 13:00:42,950 ===> Epoch[139/1000]
2023-04-04 13:01:28,157 >>> Training Loss: 0.000001382982759 ### Testing Loss: 0.000000923860910
2023-04-04 13:01:28,158 ===> Epoch[140/1000]
2023-04-04 13:02:13,367 >>> Training Loss: 0.000001590641887 ### Testing Loss: 0.000000918098863
2023-04-04 13:02:13,367 ===> Epoch[141/1000]
2023-04-04 13:02:58,626 >>> Training Loss: 0.000001344987595 ### Testing Loss: 0.000000909641471
2023-04-04 13:02:58,626 ===> Epoch[142/1000]
2023-04-04 13:03:44,957 >>> Training Loss: 0.000001329389534 ### Testing Loss: 0.000000912678786
2023-04-04 13:03:44,957 ===> Epoch[143/1000]
2023-04-04 13:04:30,320 >>> Training Loss: 0.000001344033194 ### Testing Loss: 0.000000912757059
2023-04-04 13:04:30,320 ===> Epoch[144/1000]
2023-04-04 13:05:15,657 >>> Training Loss: 0.000001362866897 ### Testing Loss: 0.000000913796384
2023-04-04 13:05:15,657 ===> Epoch[145/1000]
2023-04-04 13:06:01,038 >>> Training Loss: 0.000001354027518 ### Testing Loss: 0.000000941169958
2023-04-04 13:06:01,038 ===> Epoch[146/1000]
2023-04-04 13:06:46,485 >>> Training Loss: 0.000001413508244 ### Testing Loss: 0.000000992251330
2023-04-04 13:06:46,485 ===> Epoch[147/1000]
2023-04-04 13:07:31,940 >>> Training Loss: 0.000001368765538 ### Testing Loss: 0.000000928561235
2023-04-04 13:07:31,940 ===> Epoch[148/1000]
2023-04-04 13:08:16,629 >>> Training Loss: 0.000001350178877 ### Testing Loss: 0.000000905072795
2023-04-04 13:08:16,629 ===> Epoch[149/1000]
2023-04-04 13:09:01,335 >>> Training Loss: 0.000001337097046 ### Testing Loss: 0.000000920613388
2023-04-04 13:09:01,336 ===> Epoch[150/1000]
2023-04-04 13:09:46,160 >>> Training Loss: 0.000001323414494 ### Testing Loss: 0.000000916727515
2023-04-04 13:09:46,160 ===> Epoch[151/1000]
2023-04-04 13:10:31,022 >>> Training Loss: 0.000001379923560 ### Testing Loss: 0.000000929987380
2023-04-04 13:10:31,022 ===> Epoch[152/1000]
2023-04-04 13:11:15,845 >>> Training Loss: 0.000001439608695 ### Testing Loss: 0.000000897874031
2023-04-04 13:11:15,846 ===> Epoch[153/1000]
2023-04-04 13:12:00,687 >>> Training Loss: 0.000001333494083 ### Testing Loss: 0.000000903360046
2023-04-04 13:12:00,687 ===> Epoch[154/1000]
2023-04-04 13:12:45,378 >>> Training Loss: 0.000001299787527 ### Testing Loss: 0.000000894341156
2023-04-04 13:12:45,378 ===> Epoch[155/1000]
2023-04-04 13:13:30,238 >>> Training Loss: 0.000001551355467 ### Testing Loss: 0.000000902060947
2023-04-04 13:13:30,239 ===> Epoch[156/1000]
2023-04-04 13:14:14,976 >>> Training Loss: 0.000001299238079 ### Testing Loss: 0.000000894201150
2023-04-04 13:14:14,976 ===> Epoch[157/1000]
2023-04-04 13:14:59,734 >>> Training Loss: 0.000001306044396 ### Testing Loss: 0.000000910885660
2023-04-04 13:14:59,734 ===> Epoch[158/1000]
2023-04-04 13:15:44,526 >>> Training Loss: 0.000001328864414 ### Testing Loss: 0.000000894549260
2023-04-04 13:15:44,526 ===> Epoch[159/1000]
2023-04-04 13:16:29,360 >>> Training Loss: 0.000001310807875 ### Testing Loss: 0.000000918896262
2023-04-04 13:16:29,360 ===> Epoch[160/1000]
2023-04-04 13:17:13,561 >>> Training Loss: 0.000001312018185 ### Testing Loss: 0.000000895249968
2023-04-04 13:17:13,561 ===> Epoch[161/1000]
2023-04-04 13:17:57,751 >>> Training Loss: 0.000001309348818 ### Testing Loss: 0.000000889412888
2023-04-04 13:17:57,751 ===> Epoch[162/1000]
2023-04-04 13:18:43,081 >>> Training Loss: 0.000001284543146 ### Testing Loss: 0.000000905516742
2023-04-04 13:18:43,081 ===> Epoch[163/1000]
2023-04-04 13:19:27,452 >>> Training Loss: 0.000001459752639 ### Testing Loss: 0.000000900916746
2023-04-04 13:19:27,452 ===> Epoch[164/1000]
2023-04-04 13:20:11,830 >>> Training Loss: 0.000001285477083 ### Testing Loss: 0.000000921353717
2023-04-04 13:20:11,830 ===> Epoch[165/1000]
2023-04-04 13:20:56,101 >>> Training Loss: 0.000001284310088 ### Testing Loss: 0.000000884182214
2023-04-04 13:20:56,101 ===> Epoch[166/1000]
2023-04-04 13:21:40,431 >>> Training Loss: 0.000001280147330 ### Testing Loss: 0.000000885203292
2023-04-04 13:21:40,431 ===> Epoch[167/1000]
2023-04-04 13:22:24,701 >>> Training Loss: 0.000001261758371 ### Testing Loss: 0.000000888480088
2023-04-04 13:22:24,701 ===> Epoch[168/1000]
2023-04-04 13:23:09,051 >>> Training Loss: 0.000001261961870 ### Testing Loss: 0.000000904264937
2023-04-04 13:23:09,051 ===> Epoch[169/1000]
2023-04-04 13:23:53,337 >>> Training Loss: 0.000001289303782 ### Testing Loss: 0.000000895447556
2023-04-04 13:23:53,337 ===> Epoch[170/1000]
2023-04-04 13:24:37,627 >>> Training Loss: 0.000001287523219 ### Testing Loss: 0.000000960911507
2023-04-04 13:24:37,627 ===> Epoch[171/1000]
2023-04-04 13:25:21,918 >>> Training Loss: 0.000001377919148 ### Testing Loss: 0.000000922686979
2023-04-04 13:25:21,918 ===> Epoch[172/1000]
2023-04-04 13:26:06,239 >>> Training Loss: 0.000001267299581 ### Testing Loss: 0.000000879867912
2023-04-04 13:26:06,239 ===> Epoch[173/1000]
2023-04-04 13:26:50,429 >>> Training Loss: 0.000001263202080 ### Testing Loss: 0.000000967695541
2023-04-04 13:26:50,429 ===> Epoch[174/1000]
2023-04-04 13:27:34,702 >>> Training Loss: 0.000001273727435 ### Testing Loss: 0.000000930042575
2023-04-04 13:27:34,702 ===> Epoch[175/1000]
2023-04-04 13:28:18,932 >>> Training Loss: 0.000001492480351 ### Testing Loss: 0.000000883301368
2023-04-04 13:28:18,932 ===> Epoch[176/1000]
2023-04-04 13:29:03,252 >>> Training Loss: 0.000001288688054 ### Testing Loss: 0.000000906126502
2023-04-04 13:29:03,252 ===> Epoch[177/1000]
2023-04-04 13:29:47,442 >>> Training Loss: 0.000001252659104 ### Testing Loss: 0.000000892956564
2023-04-04 13:29:47,442 ===> Epoch[178/1000]
2023-04-04 13:30:31,718 >>> Training Loss: 0.000001243487304 ### Testing Loss: 0.000000896659230
2023-04-04 13:30:31,718 ===> Epoch[179/1000]
2023-04-04 13:31:15,912 >>> Training Loss: 0.000001240494612 ### Testing Loss: 0.000000880857442
2023-04-04 13:31:15,912 ===> Epoch[180/1000]
2023-04-04 13:32:00,242 >>> Training Loss: 0.000001281331720 ### Testing Loss: 0.000000902965496
2023-04-04 13:32:00,242 ===> Epoch[181/1000]
2023-04-04 13:32:44,332 >>> Training Loss: 0.000001251464823 ### Testing Loss: 0.000000877225148
2023-04-04 13:32:44,332 ===> Epoch[182/1000]
2023-04-04 13:33:29,806 >>> Training Loss: 0.000001273887165 ### Testing Loss: 0.000000889865476
2023-04-04 13:33:29,806 ===> Epoch[183/1000]
2023-04-04 13:34:13,920 >>> Training Loss: 0.000001218756893 ### Testing Loss: 0.000000884928795
2023-04-04 13:34:13,920 ===> Epoch[184/1000]
2023-04-04 13:34:58,240 >>> Training Loss: 0.000001224837547 ### Testing Loss: 0.000000874087732
2023-04-04 13:34:58,240 ===> Epoch[185/1000]
2023-04-04 13:35:42,451 >>> Training Loss: 0.000001237623337 ### Testing Loss: 0.000000905145953
2023-04-04 13:35:42,451 ===> Epoch[186/1000]
2023-04-04 13:36:26,681 >>> Training Loss: 0.000001246856073 ### Testing Loss: 0.000000878034655
2023-04-04 13:36:26,681 ===> Epoch[187/1000]
2023-04-04 13:37:10,910 >>> Training Loss: 0.000001224933953 ### Testing Loss: 0.000000880705727
2023-04-04 13:37:10,910 ===> Epoch[188/1000]
2023-04-04 13:37:55,095 >>> Training Loss: 0.000001250953119 ### Testing Loss: 0.000000876462593
2023-04-04 13:37:55,095 ===> Epoch[189/1000]
2023-04-04 13:38:39,446 >>> Training Loss: 0.000001219962087 ### Testing Loss: 0.000000870103179
2023-04-04 13:38:39,446 ===> Epoch[190/1000]
2023-04-04 13:39:23,655 >>> Training Loss: 0.000001202878138 ### Testing Loss: 0.000000876243121
2023-04-04 13:39:23,655 ===> Epoch[191/1000]
2023-04-04 13:40:07,886 >>> Training Loss: 0.000001329905672 ### Testing Loss: 0.000001064104822
2023-04-04 13:40:07,886 ===> Epoch[192/1000]
2023-04-04 13:40:52,145 >>> Training Loss: 0.000001238530103 ### Testing Loss: 0.000000880924460
2023-04-04 13:40:52,145 ===> Epoch[193/1000]
2023-04-04 13:41:36,399 >>> Training Loss: 0.000001218645139 ### Testing Loss: 0.000000877797731
2023-04-04 13:41:36,399 ===> Epoch[194/1000]
2023-04-04 13:42:20,769 >>> Training Loss: 0.000001229323743 ### Testing Loss: 0.000000869987559
2023-04-04 13:42:20,769 ===> Epoch[195/1000]
2023-04-04 13:43:05,049 >>> Training Loss: 0.000001222525611 ### Testing Loss: 0.000000874323234
2023-04-04 13:43:05,049 ===> Epoch[196/1000]
2023-04-04 13:43:49,269 >>> Training Loss: 0.000001230737439 ### Testing Loss: 0.000000883491055
2023-04-04 13:43:49,269 ===> Epoch[197/1000]
2023-04-04 13:44:33,536 >>> Training Loss: 0.000001239506560 ### Testing Loss: 0.000000881279220
2023-04-04 13:44:33,536 ===> Epoch[198/1000]
2023-04-04 13:45:17,769 >>> Training Loss: 0.000001234131332 ### Testing Loss: 0.000000876237493
2023-04-04 13:45:17,769 ===> Epoch[199/1000]
2023-04-04 13:46:02,070 >>> Training Loss: 0.000001194189508 ### Testing Loss: 0.000000888917725
2023-04-04 13:46:02,070 ===> Epoch[200/1000]
2023-04-04 13:46:46,259 >>> Training Loss: 0.000001265936817 ### Testing Loss: 0.000000865112895
2023-04-04 13:46:46,279 ===> Epoch[201/1000]
2023-04-04 13:47:30,559 >>> Training Loss: 0.000001179590072 ### Testing Loss: 0.000000861655053
2023-04-04 13:47:30,559 ===> Epoch[202/1000]
2023-04-04 13:48:14,905 >>> Training Loss: 0.000001192120635 ### Testing Loss: 0.000000862528793
2023-04-04 13:48:14,905 ===> Epoch[203/1000]
2023-04-04 13:48:59,947 >>> Training Loss: 0.000001185972110 ### Testing Loss: 0.000000891684010
2023-04-04 13:48:59,947 ===> Epoch[204/1000]
2023-04-04 13:49:44,257 >>> Training Loss: 0.000001370129667 ### Testing Loss: 0.000000869407870
2023-04-04 13:49:44,257 ===> Epoch[205/1000]
2023-04-04 13:50:28,567 >>> Training Loss: 0.000001197714710 ### Testing Loss: 0.000000864855508
2023-04-04 13:50:28,567 ===> Epoch[206/1000]
2023-04-04 13:51:12,977 >>> Training Loss: 0.000001183879135 ### Testing Loss: 0.000000857198415
2023-04-04 13:51:12,977 ===> Epoch[207/1000]
2023-04-04 13:51:57,457 >>> Training Loss: 0.000001216077067 ### Testing Loss: 0.000000875639387
2023-04-04 13:51:57,457 ===> Epoch[208/1000]
2023-04-04 13:52:42,057 >>> Training Loss: 0.000001199505277 ### Testing Loss: 0.000000873340014
2023-04-04 13:52:42,057 ===> Epoch[209/1000]
2023-04-04 13:53:26,667 >>> Training Loss: 0.000001426091671 ### Testing Loss: 0.000000906240246
2023-04-04 13:53:26,667 ===> Epoch[210/1000]
2023-04-04 13:54:11,218 >>> Training Loss: 0.000001192148261 ### Testing Loss: 0.000000866267271
2023-04-04 13:54:11,218 ===> Epoch[211/1000]
2023-04-04 13:54:55,888 >>> Training Loss: 0.000001192057539 ### Testing Loss: 0.000000901558735
2023-04-04 13:54:55,888 ===> Epoch[212/1000]
2023-04-04 13:55:40,622 >>> Training Loss: 0.000001166122047 ### Testing Loss: 0.000000855890903
2023-04-04 13:55:40,622 ===> Epoch[213/1000]
2023-04-04 13:56:25,353 >>> Training Loss: 0.000001167628852 ### Testing Loss: 0.000000855566952
2023-04-04 13:56:25,353 ===> Epoch[214/1000]
2023-04-04 13:57:10,063 >>> Training Loss: 0.000001221394768 ### Testing Loss: 0.000000860888690
2023-04-04 13:57:10,063 ===> Epoch[215/1000]
2023-04-04 13:57:54,813 >>> Training Loss: 0.000001188138071 ### Testing Loss: 0.000000853076926
2023-04-04 13:57:54,813 ===> Epoch[216/1000]
2023-04-04 13:58:39,646 >>> Training Loss: 0.000001189084855 ### Testing Loss: 0.000000859329134
2023-04-04 13:58:39,646 ===> Epoch[217/1000]
2023-04-04 13:59:24,386 >>> Training Loss: 0.000001161096293 ### Testing Loss: 0.000000861018066
2023-04-04 13:59:24,386 ===> Epoch[218/1000]
2023-04-04 14:00:09,176 >>> Training Loss: 0.000001225485789 ### Testing Loss: 0.000000858188685
2023-04-04 14:00:09,176 ===> Epoch[219/1000]
2023-04-04 14:00:53,946 >>> Training Loss: 0.000001156386929 ### Testing Loss: 0.000000881481469
2023-04-04 14:00:53,946 ===> Epoch[220/1000]
2023-04-04 14:01:38,876 >>> Training Loss: 0.000001179055403 ### Testing Loss: 0.000000864909737
2023-04-04 14:01:38,876 ===> Epoch[221/1000]
2023-04-04 14:02:23,554 >>> Training Loss: 0.000001169097118 ### Testing Loss: 0.000000879420611
2023-04-04 14:02:23,554 ===> Epoch[222/1000]
2023-04-04 14:03:08,284 >>> Training Loss: 0.000001178316097 ### Testing Loss: 0.000000866846676
2023-04-04 14:03:08,284 ===> Epoch[223/1000]
2023-04-04 14:03:53,915 >>> Training Loss: 0.000001258326961 ### Testing Loss: 0.000000869315159
2023-04-04 14:03:53,915 ===> Epoch[224/1000]
2023-04-04 14:04:38,786 >>> Training Loss: 0.000001151676770 ### Testing Loss: 0.000000878439721
2023-04-04 14:04:38,786 ===> Epoch[225/1000]
2023-04-04 14:05:23,616 >>> Training Loss: 0.000001159983299 ### Testing Loss: 0.000000852892867
2023-04-04 14:05:23,616 ===> Epoch[226/1000]
2023-04-04 14:06:08,429 >>> Training Loss: 0.000001188021997 ### Testing Loss: 0.000000855091059
2023-04-04 14:06:08,429 ===> Epoch[227/1000]
2023-04-04 14:06:53,319 >>> Training Loss: 0.000001135984348 ### Testing Loss: 0.000000848720049
2023-04-04 14:06:53,319 ===> Epoch[228/1000]
2023-04-04 14:07:38,130 >>> Training Loss: 0.000001156102599 ### Testing Loss: 0.000000862921468
2023-04-04 14:07:38,130 ===> Epoch[229/1000]
2023-04-04 14:08:23,066 >>> Training Loss: 0.000001160893703 ### Testing Loss: 0.000000859808949
2023-04-04 14:08:23,066 ===> Epoch[230/1000]
2023-04-04 14:09:07,985 >>> Training Loss: 0.000001242694452 ### Testing Loss: 0.000000858823796
2023-04-04 14:09:07,985 ===> Epoch[231/1000]
2023-04-04 14:09:52,795 >>> Training Loss: 0.000001163731895 ### Testing Loss: 0.000000867622759
2023-04-04 14:09:52,795 ===> Epoch[232/1000]
2023-04-04 14:10:37,625 >>> Training Loss: 0.000001138595053 ### Testing Loss: 0.000000854419341
2023-04-04 14:10:37,625 ===> Epoch[233/1000]
2023-04-04 14:11:22,486 >>> Training Loss: 0.000001141275220 ### Testing Loss: 0.000000854628922
2023-04-04 14:11:22,486 ===> Epoch[234/1000]
2023-04-04 14:12:07,306 >>> Training Loss: 0.000001128367444 ### Testing Loss: 0.000000905310799
2023-04-04 14:12:07,306 ===> Epoch[235/1000]
2023-04-04 14:12:52,083 >>> Training Loss: 0.000001173882083 ### Testing Loss: 0.000000862866216
2023-04-04 14:12:52,083 ===> Epoch[236/1000]
2023-04-04 14:13:36,883 >>> Training Loss: 0.000001163877187 ### Testing Loss: 0.000000862529873
2023-04-04 14:13:36,883 ===> Epoch[237/1000]
2023-04-04 14:14:21,723 >>> Training Loss: 0.000001139303436 ### Testing Loss: 0.000000856002146
2023-04-04 14:14:21,723 ===> Epoch[238/1000]
2023-04-04 14:15:06,653 >>> Training Loss: 0.000027190011679 ### Testing Loss: 0.000003879874839
2023-04-04 14:15:06,653 ===> Epoch[239/1000]
2023-04-04 14:15:51,544 >>> Training Loss: 0.000008431698916 ### Testing Loss: 0.000003727900321
2023-04-04 14:15:51,544 ===> Epoch[240/1000]
2023-04-04 14:16:36,438 >>> Training Loss: 0.000007749606993 ### Testing Loss: 0.000003484720764
2023-04-04 14:16:36,438 ===> Epoch[241/1000]
2023-04-04 14:17:21,257 >>> Training Loss: 0.000007074338555 ### Testing Loss: 0.000003111172873
2023-04-04 14:17:21,257 ===> Epoch[242/1000]
2023-04-04 14:18:06,067 >>> Training Loss: 0.000006057699466 ### Testing Loss: 0.000002363454769
2023-04-04 14:18:06,067 ===> Epoch[243/1000]
2023-04-04 14:18:52,168 >>> Training Loss: 0.000004434471521 ### Testing Loss: 0.000001897088850
2023-04-04 14:18:52,168 ===> Epoch[244/1000]
2023-04-04 14:19:36,970 >>> Training Loss: 0.000003681791213 ### Testing Loss: 0.000001606786554
2023-04-04 14:19:36,970 ===> Epoch[245/1000]
2023-04-04 14:20:21,870 >>> Training Loss: 0.000003083517186 ### Testing Loss: 0.000001419179512
2023-04-04 14:20:21,870 ===> Epoch[246/1000]
2023-04-04 14:21:06,840 >>> Training Loss: 0.000002630519475 ### Testing Loss: 0.000001268332994
2023-04-04 14:21:06,840 ===> Epoch[247/1000]
2023-04-04 14:21:51,751 >>> Training Loss: 0.000002276400210 ### Testing Loss: 0.000001188388296
2023-04-04 14:21:51,751 ===> Epoch[248/1000]
2023-04-04 14:22:36,630 >>> Training Loss: 0.000002058703785 ### Testing Loss: 0.000001083588018
2023-04-04 14:22:36,630 ===> Epoch[249/1000]
2023-04-04 14:23:21,615 >>> Training Loss: 0.000001822796321 ### Testing Loss: 0.000001018409421
2023-04-04 14:23:21,615 ===> Epoch[250/1000]
2023-04-04 14:24:06,506 >>> Training Loss: 0.000001686767291 ### Testing Loss: 0.000000995800178
2023-04-04 14:24:06,506 ===> Epoch[251/1000]
2023-04-04 14:24:51,355 >>> Training Loss: 0.000001612368692 ### Testing Loss: 0.000000985895440
2023-04-04 14:24:51,355 ===> Epoch[252/1000]
2023-04-04 14:25:36,326 >>> Training Loss: 0.000002969881962 ### Testing Loss: 0.000001939784397
2023-04-04 14:25:36,326 ===> Epoch[253/1000]
2023-04-04 14:26:21,241 >>> Training Loss: 0.000002609347803 ### Testing Loss: 0.000001111402185
2023-04-04 14:26:21,241 ===> Epoch[254/1000]
2023-04-04 14:27:06,102 >>> Training Loss: 0.000001764398462 ### Testing Loss: 0.000000984274948
2023-04-04 14:27:06,102 ===> Epoch[255/1000]
2023-04-04 14:27:50,942 >>> Training Loss: 0.000001507257025 ### Testing Loss: 0.000000932224111
2023-04-04 14:27:50,942 ===> Epoch[256/1000]
2023-04-04 14:28:35,842 >>> Training Loss: 0.000001417766953 ### Testing Loss: 0.000000910238441
2023-04-04 14:28:35,852 ===> Epoch[257/1000]
2023-04-04 14:29:20,772 >>> Training Loss: 0.000001364062996 ### Testing Loss: 0.000000904660624
2023-04-04 14:29:20,772 ===> Epoch[258/1000]
2023-04-04 14:30:05,675 >>> Training Loss: 0.000001393405341 ### Testing Loss: 0.000000920461105
2023-04-04 14:30:05,675 ===> Epoch[259/1000]
2023-04-04 14:30:50,646 >>> Training Loss: 0.000001333016598 ### Testing Loss: 0.000000899867302
2023-04-04 14:30:50,646 ===> Epoch[260/1000]
2023-04-04 14:31:35,486 >>> Training Loss: 0.000001341147708 ### Testing Loss: 0.000000893930974
2023-04-04 14:31:35,486 ===> Epoch[261/1000]
2023-04-04 14:32:20,506 >>> Training Loss: 0.000001302116175 ### Testing Loss: 0.000000885633597
2023-04-04 14:32:20,506 ===> Epoch[262/1000]
2023-04-04 14:33:05,470 >>> Training Loss: 0.000001291858325 ### Testing Loss: 0.000000880777463
2023-04-04 14:33:05,470 ===> Epoch[263/1000]
2023-04-04 14:33:51,431 >>> Training Loss: 0.000001408188155 ### Testing Loss: 0.000000879033962
2023-04-04 14:33:51,431 ===> Epoch[264/1000]
2023-04-04 14:34:36,420 >>> Training Loss: 0.000001266346771 ### Testing Loss: 0.000000904369074
2023-04-04 14:34:36,420 ===> Epoch[265/1000]
2023-04-04 14:35:21,471 >>> Training Loss: 0.000001283378083 ### Testing Loss: 0.000001363520482
2023-04-04 14:35:21,471 ===> Epoch[266/1000]
2023-04-04 14:36:06,471 >>> Training Loss: 0.000001246180432 ### Testing Loss: 0.000000858928900
2023-04-04 14:36:06,471 ===> Epoch[267/1000]
2023-04-04 14:36:51,575 >>> Training Loss: 0.000001214202371 ### Testing Loss: 0.000000860020805
2023-04-04 14:36:51,575 ===> Epoch[268/1000]
2023-04-04 14:37:36,655 >>> Training Loss: 0.000001213101541 ### Testing Loss: 0.000000868586824
2023-04-04 14:37:36,655 ===> Epoch[269/1000]
2023-04-04 14:38:21,610 >>> Training Loss: 0.000001194259767 ### Testing Loss: 0.000000860370221
2023-04-04 14:38:21,610 ===> Epoch[270/1000]
2023-04-04 14:39:06,539 >>> Training Loss: 0.000001206920615 ### Testing Loss: 0.000000853531105
2023-04-04 14:39:06,539 ===> Epoch[271/1000]
2023-04-04 14:39:51,559 >>> Training Loss: 0.000001225320261 ### Testing Loss: 0.000000862188926
2023-04-04 14:39:51,559 ===> Epoch[272/1000]
2023-04-04 14:40:36,503 >>> Training Loss: 0.000001185962219 ### Testing Loss: 0.000000858778492
2023-04-04 14:40:36,503 ===> Epoch[273/1000]
2023-04-04 14:41:21,493 >>> Training Loss: 0.000001590083684 ### Testing Loss: 0.000000863632636
2023-04-04 14:41:21,493 ===> Epoch[274/1000]
2023-04-04 14:42:06,403 >>> Training Loss: 0.000001221442517 ### Testing Loss: 0.000000857368548
2023-04-04 14:42:06,403 ===> Epoch[275/1000]
2023-04-04 14:42:51,343 >>> Training Loss: 0.000001164895252 ### Testing Loss: 0.000000843048895
2023-04-04 14:42:51,343 ===> Epoch[276/1000]
2023-04-04 14:43:36,447 >>> Training Loss: 0.000001148786282 ### Testing Loss: 0.000000850353899
2023-04-04 14:43:36,447 ===> Epoch[277/1000]
2023-04-04 14:44:21,407 >>> Training Loss: 0.000001151454398 ### Testing Loss: 0.000000852600238
2023-04-04 14:44:21,407 ===> Epoch[278/1000]
2023-04-04 14:45:06,397 >>> Training Loss: 0.000001142491215 ### Testing Loss: 0.000000860574403
2023-04-04 14:45:06,397 ===> Epoch[279/1000]
2023-04-04 14:45:51,318 >>> Training Loss: 0.000001147053808 ### Testing Loss: 0.000000858451870
2023-04-04 14:45:51,318 ===> Epoch[280/1000]
2023-04-04 14:46:36,248 >>> Training Loss: 0.000001164845912 ### Testing Loss: 0.000000896528320
2023-04-04 14:46:36,248 ===> Epoch[281/1000]
2023-04-04 14:47:21,203 >>> Training Loss: 0.000001203536499 ### Testing Loss: 0.000000856485428
2023-04-04 14:47:21,203 ===> Epoch[282/1000]
2023-04-04 14:48:06,223 >>> Training Loss: 0.000001133681053 ### Testing Loss: 0.000000844147564
2023-04-04 14:48:06,223 ===> Epoch[283/1000]
2023-04-04 14:48:52,447 >>> Training Loss: 0.000001149618356 ### Testing Loss: 0.000000909507889
2023-04-04 14:48:52,447 ===> Epoch[284/1000]
2023-04-04 14:49:37,487 >>> Training Loss: 0.000001119353328 ### Testing Loss: 0.000000856663803
2023-04-04 14:49:37,487 ===> Epoch[285/1000]
2023-04-04 14:50:22,657 >>> Training Loss: 0.000001199566327 ### Testing Loss: 0.000000885027816
2023-04-04 14:50:22,657 ===> Epoch[286/1000]
2023-04-04 14:51:07,711 >>> Training Loss: 0.000001139322535 ### Testing Loss: 0.000000840652945
2023-04-04 14:51:07,711 ===> Epoch[287/1000]
2023-04-04 14:51:52,621 >>> Training Loss: 0.000001185192446 ### Testing Loss: 0.000000845615943
2023-04-04 14:51:52,621 ===> Epoch[288/1000]
2023-04-04 14:52:37,581 >>> Training Loss: 0.000001116744329 ### Testing Loss: 0.000000852721087
2023-04-04 14:52:37,581 ===> Epoch[289/1000]
2023-04-04 14:53:22,520 >>> Training Loss: 0.000001115138389 ### Testing Loss: 0.000000879948630
2023-04-04 14:53:22,520 ===> Epoch[290/1000]
2023-04-04 14:54:07,524 >>> Training Loss: 0.000001123218340 ### Testing Loss: 0.000000842621773
2023-04-04 14:54:07,524 ===> Epoch[291/1000]
2023-04-04 14:54:52,506 >>> Training Loss: 0.000001249193588 ### Testing Loss: 0.000000838629830
2023-04-04 14:54:52,506 ===> Epoch[292/1000]
2023-04-04 14:55:37,427 >>> Training Loss: 0.000001099747124 ### Testing Loss: 0.000000858499902
2023-04-04 14:55:37,427 ===> Epoch[293/1000]
2023-04-04 14:56:22,457 >>> Training Loss: 0.000001087948021 ### Testing Loss: 0.000000852702271
2023-04-04 14:56:22,457 ===> Epoch[294/1000]
2023-04-04 14:57:08,469 >>> Training Loss: 0.000001089092393 ### Testing Loss: 0.000000841282940
2023-04-04 14:57:08,469 ===> Epoch[295/1000]
2023-04-04 14:57:54,126 >>> Training Loss: 0.000001169209781 ### Testing Loss: 0.000000834371519
2023-04-04 14:57:54,126 ===> Epoch[296/1000]
2023-04-04 14:58:39,849 >>> Training Loss: 0.000001090943897 ### Testing Loss: 0.000000839360553
2023-04-04 14:58:39,849 ===> Epoch[297/1000]
2023-04-04 14:59:25,437 >>> Training Loss: 0.000001092006983 ### Testing Loss: 0.000000835525100
2023-04-04 14:59:25,437 ===> Epoch[298/1000]
2023-04-04 15:00:11,025 >>> Training Loss: 0.000001287887585 ### Testing Loss: 0.000000832939747
2023-04-04 15:00:11,025 ===> Epoch[299/1000]
2023-04-04 15:00:56,697 >>> Training Loss: 0.000001084939981 ### Testing Loss: 0.000000844778071
2023-04-04 15:00:56,697 ===> Epoch[300/1000]
2023-04-04 15:01:42,347 >>> Training Loss: 0.000001078284981 ### Testing Loss: 0.000000836728475
2023-04-04 15:01:42,371 ===> Epoch[301/1000]
2023-04-04 15:02:28,021 >>> Training Loss: 0.000001077390948 ### Testing Loss: 0.000000834302796
2023-04-04 15:02:28,021 ===> Epoch[302/1000]
2023-04-04 15:03:13,704 >>> Training Loss: 0.000001085489657 ### Testing Loss: 0.000000852689368
2023-04-04 15:03:13,704 ===> Epoch[303/1000]
2023-04-04 15:04:00,187 >>> Training Loss: 0.000001122981189 ### Testing Loss: 0.000000829944895
2023-04-04 15:04:00,187 ===> Epoch[304/1000]
2023-04-04 15:04:45,779 >>> Training Loss: 0.000001077419029 ### Testing Loss: 0.000000830721433
2023-04-04 15:04:45,780 ===> Epoch[305/1000]
2023-04-04 15:05:31,522 >>> Training Loss: 0.000001099575911 ### Testing Loss: 0.000000843602663
2023-04-04 15:05:31,523 ===> Epoch[306/1000]
2023-04-04 15:06:17,092 >>> Training Loss: 0.000001082589620 ### Testing Loss: 0.000000836280606
2023-04-04 15:06:17,092 ===> Epoch[307/1000]
2023-04-04 15:07:02,744 >>> Training Loss: 0.000001067825224 ### Testing Loss: 0.000000821607500
2023-04-04 15:07:02,744 ===> Epoch[308/1000]
2023-04-04 15:07:48,368 >>> Training Loss: 0.000001105923161 ### Testing Loss: 0.000000885748932
2023-04-04 15:07:48,368 ===> Epoch[309/1000]
2023-04-04 15:08:34,048 >>> Training Loss: 0.000001060783802 ### Testing Loss: 0.000000826127120
2023-04-04 15:08:34,048 ===> Epoch[310/1000]
2023-04-04 15:09:19,707 >>> Training Loss: 0.000001073875978 ### Testing Loss: 0.000000830092631
2023-04-04 15:09:19,707 ===> Epoch[311/1000]
2023-04-04 15:10:05,391 >>> Training Loss: 0.000001107419848 ### Testing Loss: 0.000000842256782
2023-04-04 15:10:05,392 ===> Epoch[312/1000]
2023-04-04 15:10:50,996 >>> Training Loss: 0.000001149711920 ### Testing Loss: 0.000000881252959
2023-04-04 15:10:50,996 ===> Epoch[313/1000]
2023-04-04 15:11:36,653 >>> Training Loss: 0.000001069936047 ### Testing Loss: 0.000000823284893
2023-04-04 15:11:36,654 ===> Epoch[314/1000]
2023-04-04 15:12:22,289 >>> Training Loss: 0.000001062964657 ### Testing Loss: 0.000000833577701
2023-04-04 15:12:22,289 ===> Epoch[315/1000]
2023-04-04 15:13:07,912 >>> Training Loss: 0.000001140274208 ### Testing Loss: 0.000000826140933
2023-04-04 15:13:07,912 ===> Epoch[316/1000]
2023-04-04 15:13:53,395 >>> Training Loss: 0.000001077487013 ### Testing Loss: 0.000000829083490
2023-04-04 15:13:53,395 ===> Epoch[317/1000]
2023-04-04 15:14:39,002 >>> Training Loss: 0.000001049989578 ### Testing Loss: 0.000000826976645
2023-04-04 15:14:39,002 ===> Epoch[318/1000]
2023-04-04 15:15:24,611 >>> Training Loss: 0.000001048526201 ### Testing Loss: 0.000000840990026
2023-04-04 15:15:24,612 ===> Epoch[319/1000]
2023-04-04 15:16:10,392 >>> Training Loss: 0.000001338906600 ### Testing Loss: 0.000000828739985
2023-04-04 15:16:10,392 ===> Epoch[320/1000]
2023-04-04 15:16:55,951 >>> Training Loss: 0.000001058849421 ### Testing Loss: 0.000000836546405
2023-04-04 15:16:55,951 ===> Epoch[321/1000]
2023-04-04 15:17:41,646 >>> Training Loss: 0.000001072825853 ### Testing Loss: 0.000000834755724
2023-04-04 15:17:41,646 ===> Epoch[322/1000]
2023-04-04 15:18:29,100 >>> Training Loss: 0.000001055105940 ### Testing Loss: 0.000000826347957
2023-04-04 15:18:29,100 ===> Epoch[323/1000]
2023-04-04 15:19:14,786 >>> Training Loss: 0.000001115303803 ### Testing Loss: 0.000000824262941
2023-04-04 15:19:14,786 ===> Epoch[324/1000]
2023-04-04 15:20:00,437 >>> Training Loss: 0.000001057876034 ### Testing Loss: 0.000000824682331
2023-04-04 15:20:00,437 ===> Epoch[325/1000]
2023-04-04 15:20:46,157 >>> Training Loss: 0.000001068365350 ### Testing Loss: 0.000000824990309
2023-04-04 15:20:46,158 ===> Epoch[326/1000]
2023-04-04 15:21:31,775 >>> Training Loss: 0.000001031963734 ### Testing Loss: 0.000000845239356
2023-04-04 15:21:31,775 ===> Epoch[327/1000]
2023-04-04 15:22:17,358 >>> Training Loss: 0.000001059378974 ### Testing Loss: 0.000000824748497
2023-04-04 15:22:17,358 ===> Epoch[328/1000]
2023-04-04 15:23:02,955 >>> Training Loss: 0.000001047795763 ### Testing Loss: 0.000000905613319
2023-04-04 15:23:02,955 ===> Epoch[329/1000]
2023-04-04 15:23:48,626 >>> Training Loss: 0.000001137496497 ### Testing Loss: 0.000000830927831
2023-04-04 15:23:48,627 ===> Epoch[330/1000]
2023-04-04 15:24:34,237 >>> Training Loss: 0.000001069474934 ### Testing Loss: 0.000000822468394
2023-04-04 15:24:34,237 ===> Epoch[331/1000]
2023-04-04 15:25:19,984 >>> Training Loss: 0.000001044063765 ### Testing Loss: 0.000000829857299
2023-04-04 15:25:19,984 ===> Epoch[332/1000]
2023-04-04 15:26:05,615 >>> Training Loss: 0.000001065416882 ### Testing Loss: 0.000000823802566
2023-04-04 15:26:05,615 ===> Epoch[333/1000]
2023-04-04 15:26:51,242 >>> Training Loss: 0.000001038392270 ### Testing Loss: 0.000000829917099
2023-04-04 15:26:51,243 ===> Epoch[334/1000]
2023-04-04 15:27:36,883 >>> Training Loss: 0.000001072506393 ### Testing Loss: 0.000000822157972
2023-04-04 15:27:36,883 ===> Epoch[335/1000]
2023-04-04 15:28:22,673 >>> Training Loss: 0.000001028619749 ### Testing Loss: 0.000000819950856
2023-04-04 15:28:22,673 ===> Epoch[336/1000]
2023-04-04 15:29:08,409 >>> Training Loss: 0.000001052265475 ### Testing Loss: 0.000000826253597
2023-04-04 15:29:08,409 ===> Epoch[337/1000]
2023-04-04 15:29:54,184 >>> Training Loss: 0.000001020643822 ### Testing Loss: 0.000000843646376
2023-04-04 15:29:54,184 ===> Epoch[338/1000]
2023-04-04 15:30:39,899 >>> Training Loss: 0.000001137262871 ### Testing Loss: 0.000000827581346
2023-04-04 15:30:39,899 ===> Epoch[339/1000]
2023-04-04 15:31:25,653 >>> Training Loss: 0.000001037146490 ### Testing Loss: 0.000000820043056
2023-04-04 15:31:25,654 ===> Epoch[340/1000]
2023-04-04 15:32:11,303 >>> Training Loss: 0.000001013125143 ### Testing Loss: 0.000000812659152
2023-04-04 15:32:11,303 ===> Epoch[341/1000]
2023-04-04 15:32:56,993 >>> Training Loss: 0.000001024436074 ### Testing Loss: 0.000000821176059
2023-04-04 15:32:56,994 ===> Epoch[342/1000]
2023-04-04 15:33:43,592 >>> Training Loss: 0.000001037068728 ### Testing Loss: 0.000001100557256
2023-04-04 15:33:43,592 ===> Epoch[343/1000]
2023-04-04 15:34:29,207 >>> Training Loss: 0.000001037495963 ### Testing Loss: 0.000000849576452
2023-04-04 15:34:29,207 ===> Epoch[344/1000]
2023-04-04 15:35:14,918 >>> Training Loss: 0.000001061326657 ### Testing Loss: 0.000000838603512
2023-04-04 15:35:14,919 ===> Epoch[345/1000]
2023-04-04 15:36:00,634 >>> Training Loss: 0.000001031166676 ### Testing Loss: 0.000000818220485
2023-04-04 15:36:00,635 ===> Epoch[346/1000]
2023-04-04 15:36:46,352 >>> Training Loss: 0.000001018524586 ### Testing Loss: 0.000000822962647
2023-04-04 15:36:46,352 ===> Epoch[347/1000]
2023-04-04 15:37:32,040 >>> Training Loss: 0.000001089388093 ### Testing Loss: 0.000000815389740
2023-04-04 15:37:32,041 ===> Epoch[348/1000]
2023-04-04 15:38:17,735 >>> Training Loss: 0.000001017926934 ### Testing Loss: 0.000000825139523
2023-04-04 15:38:17,736 ===> Epoch[349/1000]
2023-04-04 15:39:03,312 >>> Training Loss: 0.000001016959231 ### Testing Loss: 0.000000819044487
2023-04-04 15:39:03,312 ===> Epoch[350/1000]
2023-04-04 15:39:48,944 >>> Training Loss: 0.000001020274908 ### Testing Loss: 0.000000822504376
2023-04-04 15:39:48,944 ===> Epoch[351/1000]
2023-04-04 15:40:34,691 >>> Training Loss: 0.000001190235935 ### Testing Loss: 0.000000837519337
2023-04-04 15:40:34,691 ===> Epoch[352/1000]
2023-04-04 15:41:20,359 >>> Training Loss: 0.000001023875939 ### Testing Loss: 0.000000834944672
2023-04-04 15:41:20,359 ===> Epoch[353/1000]
2023-04-04 15:42:05,943 >>> Training Loss: 0.000001021032631 ### Testing Loss: 0.000000816840213
2023-04-04 15:42:05,943 ===> Epoch[354/1000]
2023-04-04 15:42:51,669 >>> Training Loss: 0.000001014654345 ### Testing Loss: 0.000000956631425
2023-04-04 15:42:51,669 ===> Epoch[355/1000]
2023-04-04 15:43:37,185 >>> Training Loss: 0.000001271809651 ### Testing Loss: 0.000000823986682
2023-04-04 15:43:37,185 ===> Epoch[356/1000]
2023-04-04 15:44:22,837 >>> Training Loss: 0.000001017561999 ### Testing Loss: 0.000000818746230
2023-04-04 15:44:22,837 ===> Epoch[357/1000]
2023-04-04 15:45:08,500 >>> Training Loss: 0.000001010752044 ### Testing Loss: 0.000000826343921
2023-04-04 15:45:08,500 ===> Epoch[358/1000]
2023-04-04 15:45:54,078 >>> Training Loss: 0.000001305298838 ### Testing Loss: 0.000000852933681
2023-04-04 15:45:54,078 ===> Epoch[359/1000]
2023-04-04 15:46:39,683 >>> Training Loss: 0.000001032969408 ### Testing Loss: 0.000000821204196
2023-04-04 15:46:39,683 ===> Epoch[360/1000]
2023-04-04 15:47:25,327 >>> Training Loss: 0.000001002197564 ### Testing Loss: 0.000000814483997
2023-04-04 15:47:25,327 ===> Epoch[361/1000]
2023-04-04 15:48:10,885 >>> Training Loss: 0.000000998931455 ### Testing Loss: 0.000000807466961
2023-04-04 15:48:10,885 ===> Epoch[362/1000]
2023-04-04 15:48:57,432 >>> Training Loss: 0.000001002122872 ### Testing Loss: 0.000000810652068
2023-04-04 15:48:57,432 ===> Epoch[363/1000]
2023-04-04 15:49:43,097 >>> Training Loss: 0.000001027945245 ### Testing Loss: 0.000000811593509
2023-04-04 15:49:43,097 ===> Epoch[364/1000]
2023-04-04 15:50:28,757 >>> Training Loss: 0.000001003379452 ### Testing Loss: 0.000000810201016
2023-04-04 15:50:28,757 ===> Epoch[365/1000]
2023-04-04 15:51:14,323 >>> Training Loss: 0.000001076223839 ### Testing Loss: 0.000000821042590
2023-04-04 15:51:14,323 ===> Epoch[366/1000]
2023-04-04 15:51:59,964 >>> Training Loss: 0.000001025734150 ### Testing Loss: 0.000000820111268
2023-04-04 15:51:59,964 ===> Epoch[367/1000]
2023-04-04 15:52:45,709 >>> Training Loss: 0.000001110643211 ### Testing Loss: 0.000000828231521
2023-04-04 15:52:45,709 ===> Epoch[368/1000]
2023-04-04 15:53:31,305 >>> Training Loss: 0.000000998139285 ### Testing Loss: 0.000000812218047
2023-04-04 15:53:31,305 ===> Epoch[369/1000]
2023-04-04 15:54:16,911 >>> Training Loss: 0.000001013357405 ### Testing Loss: 0.000000815724491
2023-04-04 15:54:16,911 ===> Epoch[370/1000]
2023-04-04 15:55:02,574 >>> Training Loss: 0.000000993472327 ### Testing Loss: 0.000000814903387
2023-04-04 15:55:02,574 ===> Epoch[371/1000]
2023-04-04 15:55:48,125 >>> Training Loss: 0.000001135859293 ### Testing Loss: 0.000000814365876
2023-04-04 15:55:48,126 ===> Epoch[372/1000]
2023-04-04 15:56:33,709 >>> Training Loss: 0.000000999764097 ### Testing Loss: 0.000000810987046
2023-04-04 15:56:33,709 ===> Epoch[373/1000]
2023-04-04 15:57:18,234 >>> Training Loss: 0.000000991170623 ### Testing Loss: 0.000000811393932
2023-04-04 15:57:18,234 ===> Epoch[374/1000]
2023-04-04 15:58:02,534 >>> Training Loss: 0.000001002481440 ### Testing Loss: 0.000000807548645
2023-04-04 15:58:02,534 ===> Epoch[375/1000]
2023-04-04 15:58:46,894 >>> Training Loss: 0.000000992223022 ### Testing Loss: 0.000000824088772
2023-04-04 15:58:46,904 ===> Epoch[376/1000]
2023-04-04 15:59:31,228 >>> Training Loss: 0.000001020614718 ### Testing Loss: 0.000000823926086
2023-04-04 15:59:31,228 ===> Epoch[377/1000]
2023-04-04 16:00:15,588 >>> Training Loss: 0.000001015411044 ### Testing Loss: 0.000000847854892
2023-04-04 16:00:15,588 ===> Epoch[378/1000]
2023-04-04 16:00:59,878 >>> Training Loss: 0.000001024861717 ### Testing Loss: 0.000000840782036
2023-04-04 16:00:59,878 ===> Epoch[379/1000]
2023-04-04 16:01:44,228 >>> Training Loss: 0.000001049342586 ### Testing Loss: 0.000000811729990
2023-04-04 16:01:44,228 ===> Epoch[380/1000]
2023-04-04 16:02:28,658 >>> Training Loss: 0.000000985309725 ### Testing Loss: 0.000000811620566
2023-04-04 16:02:28,658 ===> Epoch[381/1000]
2023-04-04 16:03:12,973 >>> Training Loss: 0.000000976572551 ### Testing Loss: 0.000000806465323
2023-04-04 16:03:12,983 ===> Epoch[382/1000]
2023-04-04 16:03:58,044 >>> Training Loss: 0.000000998435098 ### Testing Loss: 0.000000815128431
2023-04-04 16:03:58,044 ===> Epoch[383/1000]
2023-04-04 16:04:42,414 >>> Training Loss: 0.000001010751475 ### Testing Loss: 0.000000816705779
2023-04-04 16:04:42,414 ===> Epoch[384/1000]
2023-04-04 16:05:26,854 >>> Training Loss: 0.000001089487455 ### Testing Loss: 0.000000826233020
2023-04-04 16:05:26,854 ===> Epoch[385/1000]
2023-04-04 16:06:11,278 >>> Training Loss: 0.000000988656552 ### Testing Loss: 0.000000847442891
2023-04-04 16:06:11,278 ===> Epoch[386/1000]
2023-04-04 16:06:55,738 >>> Training Loss: 0.000000985470592 ### Testing Loss: 0.000000811439406
2023-04-04 16:06:55,738 ===> Epoch[387/1000]
2023-04-04 16:07:40,168 >>> Training Loss: 0.000001110469952 ### Testing Loss: 0.000000832002911
2023-04-04 16:07:40,168 ===> Epoch[388/1000]
2023-04-04 16:08:24,598 >>> Training Loss: 0.000000997738994 ### Testing Loss: 0.000000809561072
2023-04-04 16:08:24,598 ===> Epoch[389/1000]
2023-04-04 16:09:09,078 >>> Training Loss: 0.000000971816121 ### Testing Loss: 0.000000804571584
2023-04-04 16:09:09,078 ===> Epoch[390/1000]
2023-04-04 16:09:53,519 >>> Training Loss: 0.000000969587404 ### Testing Loss: 0.000000820036860
2023-04-04 16:09:53,519 ===> Epoch[391/1000]
2023-04-04 16:10:37,889 >>> Training Loss: 0.000000999707368 ### Testing Loss: 0.000000806481580
2023-04-04 16:10:37,889 ===> Epoch[392/1000]
2023-04-04 16:11:22,279 >>> Training Loss: 0.000000985347015 ### Testing Loss: 0.000000814404927
2023-04-04 16:11:22,279 ===> Epoch[393/1000]
2023-04-04 16:12:06,892 >>> Training Loss: 0.000000983314635 ### Testing Loss: 0.000000816088686
2023-04-04 16:12:06,892 ===> Epoch[394/1000]
2023-04-04 16:12:51,212 >>> Training Loss: 0.000001445897624 ### Testing Loss: 0.000000838401206
2023-04-04 16:12:51,212 ===> Epoch[395/1000]
2023-04-04 16:13:35,586 >>> Training Loss: 0.000001018755370 ### Testing Loss: 0.000000814322959
2023-04-04 16:13:35,586 ===> Epoch[396/1000]
2023-04-04 16:14:19,815 >>> Training Loss: 0.000000975274702 ### Testing Loss: 0.000000806182186
2023-04-04 16:14:19,815 ===> Epoch[397/1000]
2023-04-04 16:15:04,135 >>> Training Loss: 0.000000967830147 ### Testing Loss: 0.000000879720290
2023-04-04 16:15:04,135 ===> Epoch[398/1000]
2023-04-04 16:15:48,526 >>> Training Loss: 0.000000976709430 ### Testing Loss: 0.000000810815095
2023-04-04 16:15:48,526 ===> Epoch[399/1000]
2023-04-04 16:16:32,922 >>> Training Loss: 0.000000981644689 ### Testing Loss: 0.000000812186670
2023-04-04 16:16:32,922 ===> Epoch[400/1000]
2023-04-04 16:17:17,242 >>> Training Loss: 0.000000989410978 ### Testing Loss: 0.000000809903838
2023-04-04 16:17:17,262 ===> Epoch[401/1000]
2023-04-04 16:18:01,592 >>> Training Loss: 0.000000988014222 ### Testing Loss: 0.000000815708745
2023-04-04 16:18:01,592 ===> Epoch[402/1000]
2023-04-04 16:18:46,911 >>> Training Loss: 0.000000982186179 ### Testing Loss: 0.000000837592609
2023-04-04 16:18:46,911 ===> Epoch[403/1000]
2023-04-04 16:19:31,346 >>> Training Loss: 0.000000993146841 ### Testing Loss: 0.000000815317321
2023-04-04 16:19:31,346 ===> Epoch[404/1000]
2023-04-04 16:20:15,788 >>> Training Loss: 0.000000980893333 ### Testing Loss: 0.000000811216694
2023-04-04 16:20:15,788 ===> Epoch[405/1000]
2023-04-04 16:21:00,158 >>> Training Loss: 0.000001043410634 ### Testing Loss: 0.000000815138435
2023-04-04 16:21:00,158 ===> Epoch[406/1000]
2023-04-04 16:21:44,568 >>> Training Loss: 0.000000968447921 ### Testing Loss: 0.000000805055151
2023-04-04 16:21:44,568 ===> Epoch[407/1000]
2023-04-04 16:22:29,028 >>> Training Loss: 0.000000969819894 ### Testing Loss: 0.000000830583588
2023-04-04 16:22:29,028 ===> Epoch[408/1000]
2023-04-04 16:23:13,468 >>> Training Loss: 0.000000979215315 ### Testing Loss: 0.000000812476173
2023-04-04 16:23:13,468 ===> Epoch[409/1000]
2023-04-04 16:23:57,921 >>> Training Loss: 0.000001044947567 ### Testing Loss: 0.000000822753918
2023-04-04 16:23:57,921 ===> Epoch[410/1000]
2023-04-04 16:24:42,341 >>> Training Loss: 0.000000974486056 ### Testing Loss: 0.000000813055749
2023-04-04 16:24:42,341 ===> Epoch[411/1000]
2023-04-04 16:25:26,780 >>> Training Loss: 0.000000972015528 ### Testing Loss: 0.000000811693837
2023-04-04 16:25:26,780 ===> Epoch[412/1000]
2023-04-04 16:26:11,310 >>> Training Loss: 0.000000979852075 ### Testing Loss: 0.000000832051910
2023-04-04 16:26:11,310 ===> Epoch[413/1000]
2023-04-04 16:26:55,734 >>> Training Loss: 0.000001214839131 ### Testing Loss: 0.000000808207176
2023-04-04 16:26:55,734 ===> Epoch[414/1000]
2023-04-04 16:27:40,155 >>> Training Loss: 0.000000977944296 ### Testing Loss: 0.000000809730352
2023-04-04 16:27:40,155 ===> Epoch[415/1000]
2023-04-04 16:28:24,594 >>> Training Loss: 0.000000956379608 ### Testing Loss: 0.000000801222541
2023-04-04 16:28:24,594 ===> Epoch[416/1000]
2023-04-04 16:29:09,105 >>> Training Loss: 0.000001217771228 ### Testing Loss: 0.000000818420460
2023-04-04 16:29:09,105 ===> Epoch[417/1000]
2023-04-04 16:29:53,595 >>> Training Loss: 0.000001029659870 ### Testing Loss: 0.000000816481815
2023-04-04 16:29:53,595 ===> Epoch[418/1000]
2023-04-04 16:30:38,120 >>> Training Loss: 0.000000983579412 ### Testing Loss: 0.000000802010163
2023-04-04 16:30:38,120 ===> Epoch[419/1000]
2023-04-04 16:31:22,599 >>> Training Loss: 0.000000991480988 ### Testing Loss: 0.000000802458203
2023-04-04 16:31:22,599 ===> Epoch[420/1000]
2023-04-04 16:32:07,050 >>> Training Loss: 0.000001104047556 ### Testing Loss: 0.000001009616312
2023-04-04 16:32:07,050 ===> Epoch[421/1000]
2023-04-04 16:32:51,550 >>> Training Loss: 0.000001007331093 ### Testing Loss: 0.000000808212860
2023-04-04 16:32:51,550 ===> Epoch[422/1000]
2023-04-04 16:33:37,284 >>> Training Loss: 0.000000990554099 ### Testing Loss: 0.000000813793235
2023-04-04 16:33:37,284 ===> Epoch[423/1000]
2023-04-04 16:34:21,930 >>> Training Loss: 0.000000981195512 ### Testing Loss: 0.000000806190371
2023-04-04 16:34:21,930 ===> Epoch[424/1000]
2023-04-04 16:35:06,560 >>> Training Loss: 0.000000957841849 ### Testing Loss: 0.000000803736100
2023-04-04 16:35:06,560 ===> Epoch[425/1000]
2023-04-04 16:35:51,211 >>> Training Loss: 0.000001015318048 ### Testing Loss: 0.000000811045709
2023-04-04 16:35:51,211 ===> Epoch[426/1000]
2023-04-04 16:36:35,911 >>> Training Loss: 0.000000960485181 ### Testing Loss: 0.000000808401751
2023-04-04 16:36:35,911 ===> Epoch[427/1000]
2023-04-04 16:37:20,613 >>> Training Loss: 0.000000946245393 ### Testing Loss: 0.000000818154206
2023-04-04 16:37:20,613 ===> Epoch[428/1000]
2023-04-04 16:38:05,413 >>> Training Loss: 0.000000966109837 ### Testing Loss: 0.000000819078650
2023-04-04 16:38:05,413 ===> Epoch[429/1000]
2023-04-04 16:38:49,993 >>> Training Loss: 0.000000962616582 ### Testing Loss: 0.000000803729108
2023-04-04 16:38:49,993 ===> Epoch[430/1000]
2023-04-04 16:39:34,603 >>> Training Loss: 0.000001057729492 ### Testing Loss: 0.000000802539660
2023-04-04 16:39:34,603 ===> Epoch[431/1000]
2023-04-04 16:40:19,263 >>> Training Loss: 0.000000954062898 ### Testing Loss: 0.000000799172597
2023-04-04 16:40:19,263 ===> Epoch[432/1000]
2023-04-04 16:41:04,026 >>> Training Loss: 0.000000942884640 ### Testing Loss: 0.000000814136001
2023-04-04 16:41:04,026 ===> Epoch[433/1000]
2023-04-04 16:41:48,695 >>> Training Loss: 0.000000953833137 ### Testing Loss: 0.000000849780804
2023-04-04 16:41:48,695 ===> Epoch[434/1000]
2023-04-04 16:42:33,366 >>> Training Loss: 0.000000980197001 ### Testing Loss: 0.000000807240781
2023-04-04 16:42:33,366 ===> Epoch[435/1000]
2023-04-04 16:43:17,976 >>> Training Loss: 0.000000952858898 ### Testing Loss: 0.000000803229682
2023-04-04 16:43:17,976 ===> Epoch[436/1000]
2023-04-04 16:44:02,709 >>> Training Loss: 0.000000958253850 ### Testing Loss: 0.000000807205083
2023-04-04 16:44:02,709 ===> Epoch[437/1000]
2023-04-04 16:44:47,359 >>> Training Loss: 0.000000987121098 ### Testing Loss: 0.000000826064991
2023-04-04 16:44:47,359 ===> Epoch[438/1000]
2023-04-04 16:45:31,979 >>> Training Loss: 0.000000960093075 ### Testing Loss: 0.000000812318262
2023-04-04 16:45:31,979 ===> Epoch[439/1000]
2023-04-04 16:46:16,609 >>> Training Loss: 0.000001001140959 ### Testing Loss: 0.000000810566519
2023-04-04 16:46:16,619 ===> Epoch[440/1000]
2023-04-04 16:47:01,230 >>> Training Loss: 0.000000973071224 ### Testing Loss: 0.000000819687330
2023-04-04 16:47:01,230 ===> Epoch[441/1000]
2023-04-04 16:47:45,978 >>> Training Loss: 0.000000972600787 ### Testing Loss: 0.000000808854111
2023-04-04 16:47:45,978 ===> Epoch[442/1000]
2023-04-04 16:48:32,109 >>> Training Loss: 0.000000995628056 ### Testing Loss: 0.000000808051936
2023-04-04 16:48:32,109 ===> Epoch[443/1000]
2023-04-04 16:49:16,899 >>> Training Loss: 0.000000954652592 ### Testing Loss: 0.000000812461110
2023-04-04 16:49:16,899 ===> Epoch[444/1000]
2023-04-04 16:50:01,920 >>> Training Loss: 0.000000972932412 ### Testing Loss: 0.000000804187380
2023-04-04 16:50:01,920 ===> Epoch[445/1000]
2023-04-04 16:50:46,889 >>> Training Loss: 0.000017179534552 ### Testing Loss: 0.000010314060091
2023-04-04 16:50:46,889 ===> Epoch[446/1000]
2023-04-04 16:51:31,873 >>> Training Loss: 0.000009237915947 ### Testing Loss: 0.000002952319619
2023-04-04 16:51:31,873 ===> Epoch[447/1000]
2023-04-04 16:52:16,723 >>> Training Loss: 0.000005510984920 ### Testing Loss: 0.000002262685712
2023-04-04 16:52:16,723 ===> Epoch[448/1000]
2023-04-04 16:53:01,713 >>> Training Loss: 0.000004368341706 ### Testing Loss: 0.000001924121079
2023-04-04 16:53:01,713 ===> Epoch[449/1000]
2023-04-04 16:53:46,533 >>> Training Loss: 0.000003648890925 ### Testing Loss: 0.000001618116016
2023-04-04 16:53:46,533 ===> Epoch[450/1000]
2023-04-04 16:54:31,366 >>> Training Loss: 0.000002922860858 ### Testing Loss: 0.000001294233243
2023-04-04 16:54:31,366 ===> Epoch[451/1000]
2023-04-04 16:55:16,286 >>> Training Loss: 0.000002224365744 ### Testing Loss: 0.000001113317580
2023-04-04 16:55:16,286 ===> Epoch[452/1000]
2023-04-04 16:56:01,156 >>> Training Loss: 0.000001757217660 ### Testing Loss: 0.000001032193722
2023-04-04 16:56:01,156 ===> Epoch[453/1000]
2023-04-04 16:56:46,046 >>> Training Loss: 0.000001514593123 ### Testing Loss: 0.000000942985650
2023-04-04 16:56:46,046 ===> Epoch[454/1000]
2023-04-04 16:57:30,966 >>> Training Loss: 0.000001396706807 ### Testing Loss: 0.000000906195680
2023-04-04 16:57:30,976 ===> Epoch[455/1000]
2023-04-04 16:58:15,949 >>> Training Loss: 0.000001275366344 ### Testing Loss: 0.000000877873617
2023-04-04 16:58:15,949 ===> Epoch[456/1000]
2023-04-04 16:59:00,949 >>> Training Loss: 0.000001208400249 ### Testing Loss: 0.000000875507510
2023-04-04 16:59:00,949 ===> Epoch[457/1000]
2023-04-04 16:59:45,840 >>> Training Loss: 0.000001252307015 ### Testing Loss: 0.000000893290576
2023-04-04 16:59:45,840 ===> Epoch[458/1000]
2023-04-04 17:00:30,060 >>> Training Loss: 0.000001155241080 ### Testing Loss: 0.000000852940218
2023-04-04 17:00:30,060 ===> Epoch[459/1000]
2023-04-04 17:01:14,403 >>> Training Loss: 0.000001142295901 ### Testing Loss: 0.000000842171858
2023-04-04 17:01:14,403 ===> Epoch[460/1000]
2023-04-04 17:01:58,647 >>> Training Loss: 0.000001110775884 ### Testing Loss: 0.000000856587349
2023-04-04 17:01:58,647 ===> Epoch[461/1000]
2023-04-04 17:02:42,867 >>> Training Loss: 0.000001318403633 ### Testing Loss: 0.000000889999626
2023-04-04 17:02:42,867 ===> Epoch[462/1000]
2023-04-04 17:03:27,087 >>> Training Loss: 0.000001173970759 ### Testing Loss: 0.000000834258913
2023-04-04 17:03:27,087 ===> Epoch[463/1000]
2023-04-04 17:04:12,089 >>> Training Loss: 0.000001088391969 ### Testing Loss: 0.000000825022312
2023-04-04 17:04:12,089 ===> Epoch[464/1000]
2023-04-04 17:04:56,372 >>> Training Loss: 0.000001057105010 ### Testing Loss: 0.000000833820422
2023-04-04 17:04:56,372 ===> Epoch[465/1000]
2023-04-04 17:05:40,613 >>> Training Loss: 0.000001047299634 ### Testing Loss: 0.000000818715364
2023-04-04 17:05:40,613 ===> Epoch[466/1000]
2023-04-04 17:06:24,743 >>> Training Loss: 0.000001044722012 ### Testing Loss: 0.000000818350713
2023-04-04 17:06:24,743 ===> Epoch[467/1000]
2023-04-04 17:07:08,873 >>> Training Loss: 0.000001021130743 ### Testing Loss: 0.000000828844918
2023-04-04 17:07:08,873 ===> Epoch[468/1000]
2023-04-04 17:07:53,184 >>> Training Loss: 0.000001369004735 ### Testing Loss: 0.000000830795102
2023-04-04 17:07:53,184 ===> Epoch[469/1000]
2023-04-04 17:08:37,477 >>> Training Loss: 0.000001019909405 ### Testing Loss: 0.000000814385089
2023-04-04 17:08:37,477 ===> Epoch[470/1000]
2023-04-04 17:09:21,717 >>> Training Loss: 0.000000996218205 ### Testing Loss: 0.000000809456424
2023-04-04 17:09:21,727 ===> Epoch[471/1000]
2023-04-04 17:10:06,037 >>> Training Loss: 0.000001041762175 ### Testing Loss: 0.000000806976175
2023-04-04 17:10:06,037 ===> Epoch[472/1000]
2023-04-04 17:10:50,196 >>> Training Loss: 0.000001004966066 ### Testing Loss: 0.000000929727605
2023-04-04 17:10:50,196 ===> Epoch[473/1000]
2023-04-04 17:11:34,387 >>> Training Loss: 0.000001014895474 ### Testing Loss: 0.000000814390432
2023-04-04 17:11:34,387 ===> Epoch[474/1000]
2023-04-04 17:12:18,682 >>> Training Loss: 0.000001007251058 ### Testing Loss: 0.000000819189211
2023-04-04 17:12:18,682 ===> Epoch[475/1000]
2023-04-04 17:13:03,002 >>> Training Loss: 0.000001157121915 ### Testing Loss: 0.000000807215940
2023-04-04 17:13:03,002 ===> Epoch[476/1000]
2023-04-04 17:13:47,182 >>> Training Loss: 0.000000984233111 ### Testing Loss: 0.000000800518421
2023-04-04 17:13:47,182 ===> Epoch[477/1000]
2023-04-04 17:14:31,412 >>> Training Loss: 0.000000978330604 ### Testing Loss: 0.000000803133162
2023-04-04 17:14:31,412 ===> Epoch[478/1000]
2023-04-04 17:15:15,680 >>> Training Loss: 0.000000979340712 ### Testing Loss: 0.000000829125497
2023-04-04 17:15:15,680 ===> Epoch[479/1000]
2023-04-04 17:15:59,830 >>> Training Loss: 0.000001159873705 ### Testing Loss: 0.000000836661911
2023-04-04 17:15:59,830 ===> Epoch[480/1000]
2023-04-04 17:16:44,171 >>> Training Loss: 0.000001001432452 ### Testing Loss: 0.000000805897741
2023-04-04 17:16:44,171 ===> Epoch[481/1000]
2023-04-04 17:17:28,501 >>> Training Loss: 0.000000984508461 ### Testing Loss: 0.000000811671498
2023-04-04 17:17:28,501 ===> Epoch[482/1000]
2023-04-04 17:18:12,851 >>> Training Loss: 0.000000968226459 ### Testing Loss: 0.000000798424367
2023-04-04 17:18:12,851 ===> Epoch[483/1000]
2023-04-04 17:18:57,917 >>> Training Loss: 0.000000968205200 ### Testing Loss: 0.000000805008483
2023-04-04 17:18:57,917 ===> Epoch[484/1000]
2023-04-04 17:19:42,197 >>> Training Loss: 0.000000984040867 ### Testing Loss: 0.000000803678972
2023-04-04 17:19:42,197 ===> Epoch[485/1000]
2023-04-04 17:20:26,537 >>> Training Loss: 0.000001059651709 ### Testing Loss: 0.000001996857463
2023-04-04 17:20:26,537 ===> Epoch[486/1000]
2023-04-04 17:21:10,737 >>> Training Loss: 0.000001054239306 ### Testing Loss: 0.000000807548020
2023-04-04 17:21:10,737 ===> Epoch[487/1000]
2023-04-04 17:21:55,034 >>> Training Loss: 0.000000954584721 ### Testing Loss: 0.000000799433451
2023-04-04 17:21:55,034 ===> Epoch[488/1000]
2023-04-04 17:22:39,334 >>> Training Loss: 0.000000940886821 ### Testing Loss: 0.000000801953377
2023-04-04 17:22:39,334 ===> Epoch[489/1000]
2023-04-04 17:23:23,624 >>> Training Loss: 0.000000954260599 ### Testing Loss: 0.000000809138214
2023-04-04 17:23:23,624 ===> Epoch[490/1000]
2023-04-04 17:24:07,924 >>> Training Loss: 0.000000960345801 ### Testing Loss: 0.000000801199349
2023-04-04 17:24:07,924 ===> Epoch[491/1000]
2023-04-04 17:24:52,284 >>> Training Loss: 0.000000967798201 ### Testing Loss: 0.000000796925178
2023-04-04 17:24:52,284 ===> Epoch[492/1000]
2023-04-04 17:25:36,569 >>> Training Loss: 0.000000971649797 ### Testing Loss: 0.000000822744482
2023-04-04 17:25:36,569 ===> Epoch[493/1000]
2023-04-04 17:26:20,969 >>> Training Loss: 0.000001031418378 ### Testing Loss: 0.000000991268621
2023-04-04 17:26:20,969 ===> Epoch[494/1000]
2023-04-04 17:27:05,279 >>> Training Loss: 0.000000954450684 ### Testing Loss: 0.000000809357232
2023-04-04 17:27:05,279 ===> Epoch[495/1000]
2023-04-04 17:27:49,639 >>> Training Loss: 0.000000949608875 ### Testing Loss: 0.000000799456359
2023-04-04 17:27:49,639 ===> Epoch[496/1000]
2023-04-04 17:28:33,925 >>> Training Loss: 0.000000955387577 ### Testing Loss: 0.000000824129927
2023-04-04 17:28:33,925 ===> Epoch[497/1000]
2023-04-04 17:29:18,248 >>> Training Loss: 0.000000943759460 ### Testing Loss: 0.000001199220833
2023-04-04 17:29:18,248 ===> Epoch[498/1000]
2023-04-04 17:30:02,518 >>> Training Loss: 0.000001061180228 ### Testing Loss: 0.000000796316499
2023-04-04 17:30:02,518 ===> Epoch[499/1000]
2023-04-04 17:30:46,808 >>> Training Loss: 0.000000937607581 ### Testing Loss: 0.000000793051356
2023-04-04 17:30:46,808 ===> Epoch[500/1000]
2023-04-04 17:31:31,088 >>> Training Loss: 0.000000928850852 ### Testing Loss: 0.000000796449683
2023-04-04 17:31:31,128 ===> Epoch[501/1000]
2023-04-04 17:32:15,460 >>> Training Loss: 0.000000943592738 ### Testing Loss: 0.000000795995049
2023-04-04 17:32:15,460 ===> Epoch[502/1000]
2023-04-04 17:32:59,781 >>> Training Loss: 0.000000947970193 ### Testing Loss: 0.000000797012603
2023-04-04 17:32:59,781 ===> Epoch[503/1000]
2023-04-04 17:33:45,012 >>> Training Loss: 0.000000943455689 ### Testing Loss: 0.000000811825771
2023-04-04 17:33:45,012 ===> Epoch[504/1000]
2023-04-04 17:34:29,402 >>> Training Loss: 0.000001014608642 ### Testing Loss: 0.000000794554865
2023-04-04 17:34:29,402 ===> Epoch[505/1000]
2023-04-04 17:35:13,832 >>> Training Loss: 0.000000967214874 ### Testing Loss: 0.000000812682401
2023-04-04 17:35:13,832 ===> Epoch[506/1000]
2023-04-04 17:35:58,387 >>> Training Loss: 0.000000935583046 ### Testing Loss: 0.000000796067297
2023-04-04 17:35:58,387 ===> Epoch[507/1000]
2023-04-04 17:36:42,777 >>> Training Loss: 0.000001063735453 ### Testing Loss: 0.000000794939979
2023-04-04 17:36:42,777 ===> Epoch[508/1000]
2023-04-04 17:37:27,357 >>> Training Loss: 0.000000933776846 ### Testing Loss: 0.000000808855020
2023-04-04 17:37:27,357 ===> Epoch[509/1000]
2023-04-04 17:38:11,787 >>> Training Loss: 0.000000956863460 ### Testing Loss: 0.000000793809136
2023-04-04 17:38:11,797 ===> Epoch[510/1000]
2023-04-04 17:38:56,352 >>> Training Loss: 0.000000920890159 ### Testing Loss: 0.000000811204302
2023-04-04 17:38:56,352 ===> Epoch[511/1000]
2023-04-04 17:39:40,832 >>> Training Loss: 0.000000914999248 ### Testing Loss: 0.000000804648948
2023-04-04 17:39:40,832 ===> Epoch[512/1000]
2023-04-04 17:40:25,232 >>> Training Loss: 0.000001010532969 ### Testing Loss: 0.000000799437316
2023-04-04 17:40:25,232 ===> Epoch[513/1000]
2023-04-04 17:41:09,702 >>> Training Loss: 0.000000924205267 ### Testing Loss: 0.000000792751052
2023-04-04 17:41:09,702 ===> Epoch[514/1000]
2023-04-04 17:41:54,152 >>> Training Loss: 0.000000964045626 ### Testing Loss: 0.000000793098877
2023-04-04 17:41:54,152 ===> Epoch[515/1000]
2023-04-04 17:42:38,585 >>> Training Loss: 0.000000922062043 ### Testing Loss: 0.000000788747855
2023-04-04 17:42:38,585 ===> Epoch[516/1000]
2023-04-04 17:43:23,046 >>> Training Loss: 0.000000912352789 ### Testing Loss: 0.000000794918492
2023-04-04 17:43:23,046 ===> Epoch[517/1000]
2023-04-04 17:44:07,586 >>> Training Loss: 0.000000955484893 ### Testing Loss: 0.000000899900499
2023-04-04 17:44:07,586 ===> Epoch[518/1000]
2023-04-04 17:44:52,026 >>> Training Loss: 0.000000921283117 ### Testing Loss: 0.000000790687011
2023-04-04 17:44:52,026 ===> Epoch[519/1000]
2023-04-04 17:45:36,506 >>> Training Loss: 0.000000933818171 ### Testing Loss: 0.000000791209970
2023-04-04 17:45:36,506 ===> Epoch[520/1000]
2023-04-04 17:46:21,068 >>> Training Loss: 0.000000921971491 ### Testing Loss: 0.000000803292778
2023-04-04 17:46:21,068 ===> Epoch[521/1000]
2023-04-04 17:47:05,438 >>> Training Loss: 0.000000924752953 ### Testing Loss: 0.000000826385076
2023-04-04 17:47:05,438 ===> Epoch[522/1000]
2023-04-04 17:47:50,008 >>> Training Loss: 0.000001654165885 ### Testing Loss: 0.000000829109524
2023-04-04 17:47:50,008 ===> Epoch[523/1000]
2023-04-04 17:48:36,008 >>> Training Loss: 0.000000951735615 ### Testing Loss: 0.000000807172341
2023-04-04 17:48:36,008 ===> Epoch[524/1000]
2023-04-04 17:49:20,732 >>> Training Loss: 0.000000903748912 ### Testing Loss: 0.000000789942419
2023-04-04 17:49:20,732 ===> Epoch[525/1000]
2023-04-04 17:50:05,613 >>> Training Loss: 0.000000905299089 ### Testing Loss: 0.000000802743386
2023-04-04 17:50:05,613 ===> Epoch[526/1000]
2023-04-04 17:50:50,413 >>> Training Loss: 0.000000912607391 ### Testing Loss: 0.000000788451700
2023-04-04 17:50:50,413 ===> Epoch[527/1000]
2023-04-04 17:51:35,303 >>> Training Loss: 0.000001013500537 ### Testing Loss: 0.000000805417301
2023-04-04 17:51:35,303 ===> Epoch[528/1000]
2023-04-04 17:52:20,103 >>> Training Loss: 0.000000935705543 ### Testing Loss: 0.000000801898295
2023-04-04 17:52:20,103 ===> Epoch[529/1000]
2023-04-04 17:53:04,920 >>> Training Loss: 0.000000956726694 ### Testing Loss: 0.000000791369644
2023-04-04 17:53:04,920 ===> Epoch[530/1000]
2023-04-04 17:53:49,720 >>> Training Loss: 0.000001080938318 ### Testing Loss: 0.000000788188629
2023-04-04 17:53:49,720 ===> Epoch[531/1000]
2023-04-04 17:54:34,469 >>> Training Loss: 0.000000899175177 ### Testing Loss: 0.000000789295029
2023-04-04 17:54:34,469 ===> Epoch[532/1000]
2023-04-04 17:55:19,189 >>> Training Loss: 0.000000906273840 ### Testing Loss: 0.000000794301059
2023-04-04 17:55:19,199 ===> Epoch[533/1000]
2023-04-04 17:56:03,925 >>> Training Loss: 0.000000945544059 ### Testing Loss: 0.000000827025133
2023-04-04 17:56:03,925 ===> Epoch[534/1000]
2023-04-04 17:56:48,618 >>> Training Loss: 0.000000930258693 ### Testing Loss: 0.000000794920879
2023-04-04 17:56:48,628 ===> Epoch[535/1000]
2023-04-04 17:57:33,518 >>> Training Loss: 0.000000922460458 ### Testing Loss: 0.000000795742210
2023-04-04 17:57:33,518 ===> Epoch[536/1000]
2023-04-04 17:58:18,258 >>> Training Loss: 0.000000926830126 ### Testing Loss: 0.000000793242805
2023-04-04 17:58:18,258 ===> Epoch[537/1000]
2023-04-04 17:59:03,018 >>> Training Loss: 0.000000930226179 ### Testing Loss: 0.000000794712719
2023-04-04 17:59:03,018 ===> Epoch[538/1000]
2023-04-04 17:59:47,871 >>> Training Loss: 0.000000965343020 ### Testing Loss: 0.000000797757366
2023-04-04 17:59:47,871 ===> Epoch[539/1000]
2023-04-04 18:00:32,660 >>> Training Loss: 0.000000936263859 ### Testing Loss: 0.000000816358181
2023-04-04 18:00:32,660 ===> Epoch[540/1000]
2023-04-04 18:01:17,571 >>> Training Loss: 0.000000921056710 ### Testing Loss: 0.000000787475074
2023-04-04 18:01:17,571 ===> Epoch[541/1000]
2023-04-04 18:02:03,491 >>> Training Loss: 0.000000922293680 ### Testing Loss: 0.000000789534340
2023-04-04 18:02:03,491 ===> Epoch[542/1000]
2023-04-04 18:02:49,180 >>> Training Loss: 0.000000939544861 ### Testing Loss: 0.000000801547117
2023-04-04 18:02:49,180 ===> Epoch[543/1000]
2023-04-04 18:03:36,253 >>> Training Loss: 0.000000918455441 ### Testing Loss: 0.000000797843995
2023-04-04 18:03:36,253 ===> Epoch[544/1000]
2023-04-04 18:04:22,033 >>> Training Loss: 0.000000917537818 ### Testing Loss: 0.000000788204488
2023-04-04 18:04:22,033 ===> Epoch[545/1000]
2023-04-04 18:05:07,973 >>> Training Loss: 0.000000915376290 ### Testing Loss: 0.000000791472303
2023-04-04 18:05:07,973 ===> Epoch[546/1000]
2023-04-04 18:05:53,793 >>> Training Loss: 0.000000930492661 ### Testing Loss: 0.000000805397065
2023-04-04 18:05:53,793 ===> Epoch[547/1000]
2023-04-04 18:06:39,689 >>> Training Loss: 0.000000912803046 ### Testing Loss: 0.000000792753326
2023-04-04 18:06:39,689 ===> Epoch[548/1000]
2023-04-04 18:07:25,479 >>> Training Loss: 0.000001063592435 ### Testing Loss: 0.000000829561202
2023-04-04 18:07:25,479 ===> Epoch[549/1000]
2023-04-04 18:08:09,989 >>> Training Loss: 0.000000915031023 ### Testing Loss: 0.000000804740182
2023-04-04 18:08:09,989 ===> Epoch[550/1000]
2023-04-04 18:08:54,369 >>> Training Loss: 0.000000929451346 ### Testing Loss: 0.000000793121671
2023-04-04 18:08:54,369 ===> Epoch[551/1000]
2023-04-04 18:09:38,709 >>> Training Loss: 0.000000911928680 ### Testing Loss: 0.000000803432783
2023-04-04 18:09:38,709 ===> Epoch[552/1000]
2023-04-04 18:10:22,927 >>> Training Loss: 0.000000971792815 ### Testing Loss: 0.000001061956937
2023-04-04 18:10:22,927 ===> Epoch[553/1000]
2023-04-04 18:11:07,297 >>> Training Loss: 0.000001160087550 ### Testing Loss: 0.000000797409825
2023-04-04 18:11:07,297 ===> Epoch[554/1000]
2023-04-04 18:11:51,607 >>> Training Loss: 0.000000892058949 ### Testing Loss: 0.000000787991041
2023-04-04 18:11:51,607 ===> Epoch[555/1000]
2023-04-04 18:12:36,057 >>> Training Loss: 0.000000875502451 ### Testing Loss: 0.000000783607334
2023-04-04 18:12:36,057 ===> Epoch[556/1000]
2023-04-04 18:13:20,447 >>> Training Loss: 0.000000904661363 ### Testing Loss: 0.000000782558971
2023-04-04 18:13:20,447 ===> Epoch[557/1000]
2023-04-04 18:14:04,992 >>> Training Loss: 0.000000897016491 ### Testing Loss: 0.000000785771647
2023-04-04 18:14:04,992 ===> Epoch[558/1000]
2023-04-04 18:14:49,632 >>> Training Loss: 0.000000949533671 ### Testing Loss: 0.000000797132770
2023-04-04 18:14:49,632 ===> Epoch[559/1000]
2023-04-04 18:15:34,292 >>> Training Loss: 0.000000894633956 ### Testing Loss: 0.000000793948857
2023-04-04 18:15:34,292 ===> Epoch[560/1000]
2023-04-04 18:16:19,043 >>> Training Loss: 0.000000890545380 ### Testing Loss: 0.000000786267890
2023-04-04 18:16:19,043 ===> Epoch[561/1000]
2023-04-04 18:17:03,808 >>> Training Loss: 0.000000899146755 ### Testing Loss: 0.000000828927341
2023-04-04 18:17:03,818 ===> Epoch[562/1000]
2023-04-04 18:17:48,508 >>> Training Loss: 0.000000914536884 ### Testing Loss: 0.000000805595676
2023-04-04 18:17:48,508 ===> Epoch[563/1000]
2023-04-04 18:18:34,928 >>> Training Loss: 0.000000896833740 ### Testing Loss: 0.000000785535178
2023-04-04 18:18:34,928 ===> Epoch[564/1000]
2023-04-04 18:19:19,588 >>> Training Loss: 0.000001185002247 ### Testing Loss: 0.000000798710175
2023-04-04 18:19:19,588 ===> Epoch[565/1000]
2023-04-04 18:20:04,378 >>> Training Loss: 0.000000899848544 ### Testing Loss: 0.000000796239704
2023-04-04 18:20:04,378 ===> Epoch[566/1000]
2023-04-04 18:20:49,153 >>> Training Loss: 0.000000883047903 ### Testing Loss: 0.000000816734428
2023-04-04 18:20:49,153 ===> Epoch[567/1000]
2023-04-04 18:21:33,993 >>> Training Loss: 0.000000916472629 ### Testing Loss: 0.000000788670945
2023-04-04 18:21:33,993 ===> Epoch[568/1000]
2023-04-04 18:22:18,933 >>> Training Loss: 0.000000892002163 ### Testing Loss: 0.000000801656313
2023-04-04 18:22:18,933 ===> Epoch[569/1000]
2023-04-04 18:23:03,773 >>> Training Loss: 0.000000991383217 ### Testing Loss: 0.000000792061201
2023-04-04 18:23:03,773 ===> Epoch[570/1000]
2023-04-04 18:23:48,688 >>> Training Loss: 0.000000897483687 ### Testing Loss: 0.000000791389937
2023-04-04 18:23:48,688 ===> Epoch[571/1000]
2023-04-04 18:24:33,498 >>> Training Loss: 0.000000904531930 ### Testing Loss: 0.000000799681516
2023-04-04 18:24:33,498 ===> Epoch[572/1000]
2023-04-04 18:25:18,278 >>> Training Loss: 0.000000932008390 ### Testing Loss: 0.000000787018223
2023-04-04 18:25:18,278 ===> Epoch[573/1000]
2023-04-04 18:26:03,109 >>> Training Loss: 0.000000886384612 ### Testing Loss: 0.000000790445654
2023-04-04 18:26:03,109 ===> Epoch[574/1000]
2023-04-04 18:26:47,849 >>> Training Loss: 0.000001029527198 ### Testing Loss: 0.000000789220678
2023-04-04 18:26:47,849 ===> Epoch[575/1000]
2023-04-04 18:27:32,603 >>> Training Loss: 0.000000893384936 ### Testing Loss: 0.000000789316744
2023-04-04 18:27:32,603 ===> Epoch[576/1000]
2023-04-04 18:28:17,503 >>> Training Loss: 0.000000888924660 ### Testing Loss: 0.000000790425247
2023-04-04 18:28:17,503 ===> Epoch[577/1000]
2023-04-04 18:29:02,283 >>> Training Loss: 0.000000887296096 ### Testing Loss: 0.000000801441104
2023-04-04 18:29:02,283 ===> Epoch[578/1000]
2023-04-04 18:29:47,103 >>> Training Loss: 0.000000896576353 ### Testing Loss: 0.000000784289682
2023-04-04 18:29:47,103 ===> Epoch[579/1000]
2023-04-04 18:30:31,886 >>> Training Loss: 0.000000883379414 ### Testing Loss: 0.000000792023684
2023-04-04 18:30:31,896 ===> Epoch[580/1000]
2023-04-04 18:31:16,646 >>> Training Loss: 0.000000896832262 ### Testing Loss: 0.000000784508472
2023-04-04 18:31:16,646 ===> Epoch[581/1000]
2023-04-04 18:32:01,546 >>> Training Loss: 0.000000878871106 ### Testing Loss: 0.000000795037295
2023-04-04 18:32:01,546 ===> Epoch[582/1000]
2023-04-04 18:32:46,395 >>> Training Loss: 0.000000904539263 ### Testing Loss: 0.000000804981994
2023-04-04 18:32:46,395 ===> Epoch[583/1000]
2023-04-04 18:33:32,608 >>> Training Loss: 0.000000892814228 ### Testing Loss: 0.000000803794705
2023-04-04 18:33:32,608 ===> Epoch[584/1000]
2023-04-04 18:34:17,343 >>> Training Loss: 0.000000950023264 ### Testing Loss: 0.000000903460375
2023-04-04 18:34:17,343 ===> Epoch[585/1000]
2023-04-04 18:35:02,143 >>> Training Loss: 0.000000895363542 ### Testing Loss: 0.000000785080147
2023-04-04 18:35:02,143 ===> Epoch[586/1000]
2023-04-04 18:35:46,993 >>> Training Loss: 0.000000908004495 ### Testing Loss: 0.000000789899275
2023-04-04 18:35:46,993 ===> Epoch[587/1000]
2023-04-04 18:36:31,743 >>> Training Loss: 0.000000901823626 ### Testing Loss: 0.000000788940497
2023-04-04 18:36:31,743 ===> Epoch[588/1000]
2023-04-04 18:37:16,527 >>> Training Loss: 0.000001082554832 ### Testing Loss: 0.000000784274221
2023-04-04 18:37:16,527 ===> Epoch[589/1000]
2023-04-04 18:38:01,277 >>> Training Loss: 0.000000868888321 ### Testing Loss: 0.000000786542500
2023-04-04 18:38:01,277 ===> Epoch[590/1000]
2023-04-04 18:38:45,987 >>> Training Loss: 0.000000872533406 ### Testing Loss: 0.000000783731195
2023-04-04 18:38:45,997 ===> Epoch[591/1000]
2023-04-04 18:39:30,777 >>> Training Loss: 0.000000893781134 ### Testing Loss: 0.000000795036556
2023-04-04 18:39:30,777 ===> Epoch[592/1000]
2023-04-04 18:40:15,537 >>> Training Loss: 0.000000886929627 ### Testing Loss: 0.000000792378671
2023-04-04 18:40:15,537 ===> Epoch[593/1000]
2023-04-04 18:41:00,373 >>> Training Loss: 0.000000887854014 ### Testing Loss: 0.000000799255361
2023-04-04 18:41:00,373 ===> Epoch[594/1000]
2023-04-04 18:41:45,083 >>> Training Loss: 0.000000886683722 ### Testing Loss: 0.000000793159529
2023-04-04 18:41:45,083 ===> Epoch[595/1000]
2023-04-04 18:42:29,893 >>> Training Loss: 0.000000917306750 ### Testing Loss: 0.000000789375235
2023-04-04 18:42:29,893 ===> Epoch[596/1000]
2023-04-04 18:43:14,743 >>> Training Loss: 0.000000874742341 ### Testing Loss: 0.000000797450411
2023-04-04 18:43:14,743 ===> Epoch[597/1000]
2023-04-04 18:43:59,477 >>> Training Loss: 0.000000886880002 ### Testing Loss: 0.000000806050537
2023-04-04 18:43:59,477 ===> Epoch[598/1000]
2023-04-04 18:44:44,217 >>> Training Loss: 0.000000922924073 ### Testing Loss: 0.000000782577104
2023-04-04 18:44:44,227 ===> Epoch[599/1000]
2023-04-04 18:45:29,027 >>> Training Loss: 0.000000880385585 ### Testing Loss: 0.000000790024671
2023-04-04 18:45:29,027 ===> Epoch[600/1000]
2023-04-04 18:46:13,797 >>> Training Loss: 0.000000882895790 ### Testing Loss: 0.000000789918943
2023-04-04 18:46:13,817 ===> Epoch[601/1000]
2023-04-04 18:46:58,537 >>> Training Loss: 0.000001017492878 ### Testing Loss: 0.000000803434034
2023-04-04 18:46:58,537 ===> Epoch[602/1000]
2023-04-04 18:47:43,395 >>> Training Loss: 0.000000874654404 ### Testing Loss: 0.000000786561316
2023-04-04 18:47:43,395 ===> Epoch[603/1000]
2023-04-04 18:48:29,357 >>> Training Loss: 0.000000873735928 ### Testing Loss: 0.000000780224752
2023-04-04 18:48:29,357 ===> Epoch[604/1000]
2023-04-04 18:49:14,007 >>> Training Loss: 0.000000907189644 ### Testing Loss: 0.000000793644858
2023-04-04 18:49:14,007 ===> Epoch[605/1000]
2023-04-04 18:49:58,867 >>> Training Loss: 0.000000920452294 ### Testing Loss: 0.000000794298103
2023-04-04 18:49:58,867 ===> Epoch[606/1000]
2023-04-04 18:50:43,730 >>> Training Loss: 0.000000897429629 ### Testing Loss: 0.000000941884423
2023-04-04 18:50:43,740 ===> Epoch[607/1000]
2023-04-04 18:51:28,524 >>> Training Loss: 0.000000878219794 ### Testing Loss: 0.000000780841674
2023-04-04 18:51:28,524 ===> Epoch[608/1000]
2023-04-04 18:52:13,334 >>> Training Loss: 0.000000868656286 ### Testing Loss: 0.000000788353930
2023-04-04 18:52:13,334 ===> Epoch[609/1000]
2023-04-04 18:52:58,165 >>> Training Loss: 0.000000937359573 ### Testing Loss: 0.000000797596385
2023-04-04 18:52:58,165 ===> Epoch[610/1000]
2023-04-04 18:53:42,895 >>> Training Loss: 0.000000946871012 ### Testing Loss: 0.000000796663244
2023-04-04 18:53:42,895 ===> Epoch[611/1000]
2023-04-04 18:54:27,668 >>> Training Loss: 0.000000894624520 ### Testing Loss: 0.000000806719072
2023-04-04 18:54:27,668 ===> Epoch[612/1000]
2023-04-04 18:55:12,578 >>> Training Loss: 0.000000896421682 ### Testing Loss: 0.000000787865190
2023-04-04 18:55:12,578 ===> Epoch[613/1000]
2023-04-04 18:55:57,328 >>> Training Loss: 0.000000871205657 ### Testing Loss: 0.000000783833570
2023-04-04 18:55:57,328 ===> Epoch[614/1000]
2023-04-04 18:56:42,148 >>> Training Loss: 0.000000866643234 ### Testing Loss: 0.000000787444833
2023-04-04 18:56:42,148 ===> Epoch[615/1000]
2023-04-04 18:57:27,021 >>> Training Loss: 0.000000886411556 ### Testing Loss: 0.000000784600388
2023-04-04 18:57:27,021 ===> Epoch[616/1000]
2023-04-04 18:58:11,794 >>> Training Loss: 0.000000879347851 ### Testing Loss: 0.000000794577431
2023-04-04 18:58:11,794 ===> Epoch[617/1000]
2023-04-04 18:58:56,604 >>> Training Loss: 0.000000934104719 ### Testing Loss: 0.000000780518008
2023-04-04 18:58:56,604 ===> Epoch[618/1000]
2023-04-04 18:59:41,374 >>> Training Loss: 0.000000866360608 ### Testing Loss: 0.000000803080638
2023-04-04 18:59:41,374 ===> Epoch[619/1000]
2023-04-04 19:00:26,134 >>> Training Loss: 0.000000872815690 ### Testing Loss: 0.000000792312562
2023-04-04 19:00:26,134 ===> Epoch[620/1000]
2023-04-04 19:01:10,917 >>> Training Loss: 0.000001007927267 ### Testing Loss: 0.000000803980697
2023-04-04 19:01:10,917 ===> Epoch[621/1000]
2023-04-04 19:01:55,668 >>> Training Loss: 0.000000873370709 ### Testing Loss: 0.000000792464959
2023-04-04 19:01:55,668 ===> Epoch[622/1000]
2023-04-04 19:02:40,448 >>> Training Loss: 0.000000861900617 ### Testing Loss: 0.000000790050478
2023-04-04 19:02:40,448 ===> Epoch[623/1000]
2023-04-04 19:03:25,328 >>> Training Loss: 0.000000863961361 ### Testing Loss: 0.000000798479448
2023-04-04 19:03:25,328 ===> Epoch[624/1000]
2023-04-04 19:04:11,219 >>> Training Loss: 0.000000962774607 ### Testing Loss: 0.000000791482989
2023-04-04 19:04:11,219 ===> Epoch[625/1000]
2023-04-04 19:04:56,044 >>> Training Loss: 0.000000863594892 ### Testing Loss: 0.000000784148199
2023-04-04 19:04:56,044 ===> Epoch[626/1000]
2023-04-04 19:05:40,824 >>> Training Loss: 0.000000858361261 ### Testing Loss: 0.000000779612321
2023-04-04 19:05:40,824 ===> Epoch[627/1000]
2023-04-04 19:06:25,604 >>> Training Loss: 0.000001033115041 ### Testing Loss: 0.000000802031821
2023-04-04 19:06:25,604 ===> Epoch[628/1000]
2023-04-04 19:07:10,394 >>> Training Loss: 0.000000888170860 ### Testing Loss: 0.000000789165767
2023-04-04 19:07:10,394 ===> Epoch[629/1000]
2023-04-04 19:07:55,107 >>> Training Loss: 0.000000871798136 ### Testing Loss: 0.000000787076544
2023-04-04 19:07:55,107 ===> Epoch[630/1000]
2023-04-04 19:08:39,827 >>> Training Loss: 0.000000866412961 ### Testing Loss: 0.000000777887465
2023-04-04 19:08:39,827 ===> Epoch[631/1000]
2023-04-04 19:09:24,607 >>> Training Loss: 0.000000868061761 ### Testing Loss: 0.000000814752411
2023-04-04 19:09:24,607 ===> Epoch[632/1000]
2023-04-04 19:10:09,407 >>> Training Loss: 0.000000871460941 ### Testing Loss: 0.000000786758164
2023-04-04 19:10:09,407 ===> Epoch[633/1000]
2023-04-04 19:10:54,157 >>> Training Loss: 0.000000855431267 ### Testing Loss: 0.000000803903333
2023-04-04 19:10:54,157 ===> Epoch[634/1000]
2023-04-04 19:11:38,940 >>> Training Loss: 0.000000868502468 ### Testing Loss: 0.000000781125493
2023-04-04 19:11:38,940 ===> Epoch[635/1000]
2023-04-04 19:12:23,749 >>> Training Loss: 0.000000872731448 ### Testing Loss: 0.000000947074454
2023-04-04 19:12:23,749 ===> Epoch[636/1000]
2023-04-04 19:13:08,509 >>> Training Loss: 0.000000885302597 ### Testing Loss: 0.000000782321820
2023-04-04 19:13:08,509 ===> Epoch[637/1000]
2023-04-04 19:13:53,280 >>> Training Loss: 0.000000867113499 ### Testing Loss: 0.000000783654400
2023-04-04 19:13:53,280 ===> Epoch[638/1000]
2023-04-04 19:14:38,173 >>> Training Loss: 0.000000865534219 ### Testing Loss: 0.000000791471393
2023-04-04 19:14:38,173 ===> Epoch[639/1000]
2023-04-04 19:15:22,953 >>> Training Loss: 0.000000877782668 ### Testing Loss: 0.000000793312950
2023-04-04 19:15:22,953 ===> Epoch[640/1000]
2023-04-04 19:16:07,673 >>> Training Loss: 0.000000855339351 ### Testing Loss: 0.000000784439067
2023-04-04 19:16:07,673 ===> Epoch[641/1000]
2023-04-04 19:16:52,278 >>> Training Loss: 0.000000957223278 ### Testing Loss: 0.000000788438456
2023-04-04 19:16:52,278 ===> Epoch[642/1000]
2023-04-04 19:17:37,008 >>> Training Loss: 0.000000870313499 ### Testing Loss: 0.000000786268401
2023-04-04 19:17:37,008 ===> Epoch[643/1000]
2023-04-04 19:18:21,946 >>> Training Loss: 0.000000861880892 ### Testing Loss: 0.000000785626582
2023-04-04 19:18:21,946 ===> Epoch[644/1000]
2023-04-04 19:19:07,598 >>> Training Loss: 0.000000859003933 ### Testing Loss: 0.000000786941996
2023-04-04 19:19:07,598 ===> Epoch[645/1000]
2023-04-04 19:19:52,298 >>> Training Loss: 0.000000885420150 ### Testing Loss: 0.000000784839131
2023-04-04 19:19:52,298 ===> Epoch[646/1000]
2023-04-04 19:20:37,039 >>> Training Loss: 0.000000978594358 ### Testing Loss: 0.000000795207370
2023-04-04 19:20:37,039 ===> Epoch[647/1000]
2023-04-04 19:21:21,689 >>> Training Loss: 0.000000869353073 ### Testing Loss: 0.000000784598626
2023-04-04 19:21:21,689 ===> Epoch[648/1000]
2023-04-04 19:22:06,592 >>> Training Loss: 0.000000856307281 ### Testing Loss: 0.000000783848748
2023-04-04 19:22:06,592 ===> Epoch[649/1000]
2023-04-04 19:22:51,373 >>> Training Loss: 0.000000858668500 ### Testing Loss: 0.000000789760747
2023-04-04 19:22:51,373 ===> Epoch[650/1000]
2023-04-04 19:23:36,153 >>> Training Loss: 0.000000864864660 ### Testing Loss: 0.000000782986604
2023-04-04 19:23:36,153 ===> Epoch[651/1000]
2023-04-04 19:24:20,873 >>> Training Loss: 0.000000864965500 ### Testing Loss: 0.000000796380846
2023-04-04 19:24:20,873 ===> Epoch[652/1000]
2023-04-04 19:25:05,646 >>> Training Loss: 0.000001492039019 ### Testing Loss: 0.000000815594262
2023-04-04 19:25:05,646 ===> Epoch[653/1000]
2023-04-04 19:25:50,455 >>> Training Loss: 0.000000909798587 ### Testing Loss: 0.000000793494053
2023-04-04 19:25:50,455 ===> Epoch[654/1000]
2023-04-04 19:26:35,165 >>> Training Loss: 0.000000854200380 ### Testing Loss: 0.000000777640480
2023-04-04 19:26:35,165 ===> Epoch[655/1000]
2023-04-04 19:27:19,846 >>> Training Loss: 0.000000842355405 ### Testing Loss: 0.000000774920636
2023-04-04 19:27:19,846 ===> Epoch[656/1000]
2023-04-04 19:28:04,741 >>> Training Loss: 0.000000867691313 ### Testing Loss: 0.000000818904368
2023-04-04 19:28:04,741 ===> Epoch[657/1000]
2023-04-04 19:28:49,521 >>> Training Loss: 0.000000868963696 ### Testing Loss: 0.000000783061978
2023-04-04 19:28:49,521 ===> Epoch[658/1000]
2023-04-04 19:29:34,272 >>> Training Loss: 0.000000855223107 ### Testing Loss: 0.000000792777257
2023-04-04 19:29:34,272 ===> Epoch[659/1000]
2023-04-04 19:30:18,993 >>> Training Loss: 0.000000967705319 ### Testing Loss: 0.000000858967269
2023-04-04 19:30:18,993 ===> Epoch[660/1000]
2023-04-04 19:31:03,753 >>> Training Loss: 0.000000890559818 ### Testing Loss: 0.000000791612365
2023-04-04 19:31:03,753 ===> Epoch[661/1000]
2023-04-04 19:31:48,539 >>> Training Loss: 0.000000862159993 ### Testing Loss: 0.000000785408304
2023-04-04 19:31:48,539 ===> Epoch[662/1000]
2023-04-04 19:32:33,259 >>> Training Loss: 0.000000864362960 ### Testing Loss: 0.000000776421416
2023-04-04 19:32:33,259 ===> Epoch[663/1000]
2023-04-04 19:33:18,099 >>> Training Loss: 0.000000872618102 ### Testing Loss: 0.000000779964978
2023-04-04 19:33:18,099 ===> Epoch[664/1000]
2023-04-04 19:34:03,939 >>> Training Loss: 0.000000866773064 ### Testing Loss: 0.000000788503428
2023-04-04 19:34:03,939 ===> Epoch[665/1000]
2023-04-04 19:34:48,709 >>> Training Loss: 0.000000858619387 ### Testing Loss: 0.000000785305986
2023-04-04 19:34:48,709 ===> Epoch[666/1000]
2023-04-04 19:35:33,433 >>> Training Loss: 0.000000857191594 ### Testing Loss: 0.000000796554218
2023-04-04 19:35:33,433 ===> Epoch[667/1000]
2023-04-04 19:36:18,223 >>> Training Loss: 0.000001362543685 ### Testing Loss: 0.000000817519606
2023-04-04 19:36:18,223 ===> Epoch[668/1000]
2023-04-04 19:37:03,083 >>> Training Loss: 0.000000913837539 ### Testing Loss: 0.000000787181364
2023-04-04 19:37:03,083 ===> Epoch[669/1000]
2023-04-04 19:37:47,853 >>> Training Loss: 0.000000851794141 ### Testing Loss: 0.000000817204523
2023-04-04 19:37:47,853 ===> Epoch[670/1000]
2023-04-04 19:38:32,647 >>> Training Loss: 0.000000862541185 ### Testing Loss: 0.000000778914625
2023-04-04 19:38:32,647 ===> Epoch[671/1000]
2023-04-04 19:39:17,437 >>> Training Loss: 0.000000854434745 ### Testing Loss: 0.000000800907515
2023-04-04 19:39:17,437 ===> Epoch[672/1000]
2023-04-04 19:40:02,197 >>> Training Loss: 0.000000882978838 ### Testing Loss: 0.000000807467018
2023-04-04 19:40:02,197 ===> Epoch[673/1000]
2023-04-04 19:40:47,007 >>> Training Loss: 0.000000956288432 ### Testing Loss: 0.000000792768788
2023-04-04 19:40:47,007 ===> Epoch[674/1000]
2023-04-04 19:41:31,707 >>> Training Loss: 0.000000848059642 ### Testing Loss: 0.000000803391345
2023-04-04 19:41:31,707 ===> Epoch[675/1000]
2023-04-04 19:42:16,542 >>> Training Loss: 0.000000889720127 ### Testing Loss: 0.000000787573072
2023-04-04 19:42:16,542 ===> Epoch[676/1000]
2023-04-04 19:43:01,354 >>> Training Loss: 0.000000850104129 ### Testing Loss: 0.000000788481543
2023-04-04 19:43:01,354 ===> Epoch[677/1000]
2023-04-04 19:43:46,164 >>> Training Loss: 0.000000839084919 ### Testing Loss: 0.000000786087639
2023-04-04 19:43:46,164 ===> Epoch[678/1000]
2023-04-04 19:44:30,974 >>> Training Loss: 0.000001013335009 ### Testing Loss: 0.000000796630275
2023-04-04 19:44:30,974 ===> Epoch[679/1000]
2023-04-04 19:45:15,788 >>> Training Loss: 0.000000855612427 ### Testing Loss: 0.000000786280793
2023-04-04 19:45:15,788 ===> Epoch[680/1000]
2023-04-04 19:46:00,570 >>> Training Loss: 0.000000842528493 ### Testing Loss: 0.000000778980962
2023-04-04 19:46:00,570 ===> Epoch[681/1000]
2023-04-04 19:46:45,381 >>> Training Loss: 0.000000868436359 ### Testing Loss: 0.000000791317234
2023-04-04 19:46:45,381 ===> Epoch[682/1000]
2023-04-04 19:47:30,181 >>> Training Loss: 0.000000860850321 ### Testing Loss: 0.000000780660969
2023-04-04 19:47:30,181 ===> Epoch[683/1000]
2023-04-04 19:48:14,982 >>> Training Loss: 0.000001206824891 ### Testing Loss: 0.000000796589916
2023-04-04 19:48:14,982 ===> Epoch[684/1000]
2023-04-04 19:49:00,636 >>> Training Loss: 0.000000863911112 ### Testing Loss: 0.000000780932851
2023-04-04 19:49:00,636 ===> Epoch[685/1000]
2023-04-04 19:49:45,376 >>> Training Loss: 0.000000840205246 ### Testing Loss: 0.000000782569259
2023-04-04 19:49:45,376 ===> Epoch[686/1000]
2023-04-04 19:50:30,086 >>> Training Loss: 0.000000842597103 ### Testing Loss: 0.000000776644754
2023-04-04 19:50:30,086 ===> Epoch[687/1000]
2023-04-04 19:51:14,855 >>> Training Loss: 0.000000846523108 ### Testing Loss: 0.000000781840413
2023-04-04 19:51:14,855 ===> Epoch[688/1000]
2023-04-04 19:51:59,632 >>> Training Loss: 0.000000868651000 ### Testing Loss: 0.000000823496180
2023-04-04 19:51:59,632 ===> Epoch[689/1000]
2023-04-04 19:52:44,303 >>> Training Loss: 0.000000875007913 ### Testing Loss: 0.000000781927895
2023-04-04 19:52:44,303 ===> Epoch[690/1000]
2023-04-04 19:53:29,063 >>> Training Loss: 0.000000880076982 ### Testing Loss: 0.000004524432825
2023-04-04 19:53:29,063 ===> Epoch[691/1000]
2023-04-04 19:54:13,803 >>> Training Loss: 0.000001058361931 ### Testing Loss: 0.000000783548387
2023-04-04 19:54:13,803 ===> Epoch[692/1000]
2023-04-04 19:54:58,543 >>> Training Loss: 0.000000838612664 ### Testing Loss: 0.000000786401529
2023-04-04 19:54:58,543 ===> Epoch[693/1000]
2023-04-04 19:55:43,297 >>> Training Loss: 0.000000842581301 ### Testing Loss: 0.000000778986930
2023-04-04 19:55:43,297 ===> Epoch[694/1000]
2023-04-04 19:56:28,087 >>> Training Loss: 0.000000936673644 ### Testing Loss: 0.000000816274564
2023-04-04 19:56:28,087 ===> Epoch[695/1000]
2023-04-04 19:57:12,817 >>> Training Loss: 0.000000849821902 ### Testing Loss: 0.000000782326310
2023-04-04 19:57:12,817 ===> Epoch[696/1000]
2023-04-04 19:57:57,667 >>> Training Loss: 0.000000844894657 ### Testing Loss: 0.000000784028941
2023-04-04 19:57:57,667 ===> Epoch[697/1000]
2023-04-04 19:58:42,380 >>> Training Loss: 0.000000841530607 ### Testing Loss: 0.000000780778294
2023-04-04 19:58:42,380 ===> Epoch[698/1000]
2023-04-04 19:59:27,160 >>> Training Loss: 0.000000903393186 ### Testing Loss: 0.000000782504458
2023-04-04 19:59:27,160 ===> Epoch[699/1000]
2023-04-04 20:00:11,951 >>> Training Loss: 0.000000951171955 ### Testing Loss: 0.000001202796739
2023-04-04 20:00:11,951 ===> Epoch[700/1000]
2023-04-04 20:00:56,711 >>> Training Loss: 0.000000901066812 ### Testing Loss: 0.000000784166616
2023-04-04 20:00:56,741 ===> Epoch[701/1000]
2023-04-04 20:01:41,501 >>> Training Loss: 0.000000846989110 ### Testing Loss: 0.000000777964829
2023-04-04 20:01:41,501 ===> Epoch[702/1000]
2023-04-04 20:02:26,225 >>> Training Loss: 0.000000864353808 ### Testing Loss: 0.000000783007351
2023-04-04 20:02:26,225 ===> Epoch[703/1000]
2023-04-04 20:03:10,936 >>> Training Loss: 0.000000860122270 ### Testing Loss: 0.000000783800203
2023-04-04 20:03:10,936 ===> Epoch[704/1000]
2023-04-04 20:03:56,558 >>> Training Loss: 0.000000977833679 ### Testing Loss: 0.000004658637863
2023-04-04 20:03:56,558 ===> Epoch[705/1000]
2023-04-04 20:04:41,278 >>> Training Loss: 0.000001021525463 ### Testing Loss: 0.000000777936236
2023-04-04 20:04:41,278 ===> Epoch[706/1000]
2023-04-04 20:05:25,978 >>> Training Loss: 0.000000842941233 ### Testing Loss: 0.000000778812137
2023-04-04 20:05:25,978 ===> Epoch[707/1000]
2023-04-04 20:06:10,723 >>> Training Loss: 0.000000831353418 ### Testing Loss: 0.000000783787982
2023-04-04 20:06:10,723 ===> Epoch[708/1000]
2023-04-04 20:06:55,483 >>> Training Loss: 0.000000846981550 ### Testing Loss: 0.000000777608250
2023-04-04 20:06:55,483 ===> Epoch[709/1000]
2023-04-04 20:07:40,203 >>> Training Loss: 0.000000847433682 ### Testing Loss: 0.000000796821951
2023-04-04 20:07:40,203 ===> Epoch[710/1000]
2023-04-04 20:08:25,003 >>> Training Loss: 0.000000883324958 ### Testing Loss: 0.000000804951299
2023-04-04 20:08:25,003 ===> Epoch[711/1000]
2023-04-04 20:09:09,786 >>> Training Loss: 0.000000854631253 ### Testing Loss: 0.000000793738138
2023-04-04 20:09:09,796 ===> Epoch[712/1000]
2023-04-04 20:09:54,585 >>> Training Loss: 0.000000854554798 ### Testing Loss: 0.000000810062261
2023-04-04 20:09:54,585 ===> Epoch[713/1000]
2023-04-04 20:10:39,316 >>> Training Loss: 0.000000952001983 ### Testing Loss: 0.000000797343944
2023-04-04 20:10:39,316 ===> Epoch[714/1000]
2023-04-04 20:11:24,126 >>> Training Loss: 0.000000845541706 ### Testing Loss: 0.000000782467623
2023-04-04 20:11:24,126 ===> Epoch[715/1000]
2023-04-04 20:12:08,945 >>> Training Loss: 0.000000852887240 ### Testing Loss: 0.000000790938259
2023-04-04 20:12:08,945 ===> Epoch[716/1000]
2023-04-04 20:12:53,738 >>> Training Loss: 0.000000873523902 ### Testing Loss: 0.000000793052436
2023-04-04 20:12:53,738 ===> Epoch[717/1000]
2023-04-04 20:13:38,478 >>> Training Loss: 0.000000837717835 ### Testing Loss: 0.000000791406251
2023-04-04 20:13:38,478 ===> Epoch[718/1000]
2023-04-04 20:14:23,239 >>> Training Loss: 0.000000854860502 ### Testing Loss: 0.000000801705880
2023-04-04 20:14:23,239 ===> Epoch[719/1000]
2023-04-04 20:15:07,899 >>> Training Loss: 0.000000850089691 ### Testing Loss: 0.000000804861997
2023-04-04 20:15:07,899 ===> Epoch[720/1000]
2023-04-04 20:15:52,702 >>> Training Loss: 0.000000861098158 ### Testing Loss: 0.000000783028895
2023-04-04 20:15:52,702 ===> Epoch[721/1000]
2023-04-04 20:16:37,422 >>> Training Loss: 0.000000951039453 ### Testing Loss: 0.000000790140007
2023-04-04 20:16:37,422 ===> Epoch[722/1000]
2023-04-04 20:17:22,202 >>> Training Loss: 0.000000853883535 ### Testing Loss: 0.000000786719454
2023-04-04 20:17:22,202 ===> Epoch[723/1000]
2023-04-04 20:18:06,852 >>> Training Loss: 0.000000874167370 ### Testing Loss: 0.000000780402786
2023-04-04 20:18:06,852 ===> Epoch[724/1000]
2023-04-04 20:18:52,893 >>> Training Loss: 0.000000835711830 ### Testing Loss: 0.000000778696403
2023-04-04 20:18:52,893 ===> Epoch[725/1000]
2023-04-04 20:19:37,731 >>> Training Loss: 0.000000844365161 ### Testing Loss: 0.000001221227990
2023-04-04 20:19:37,731 ===> Epoch[726/1000]
2023-04-04 20:20:22,482 >>> Training Loss: 0.000000870195549 ### Testing Loss: 0.000000778055266
2023-04-04 20:20:22,482 ===> Epoch[727/1000]
2023-04-04 20:21:07,212 >>> Training Loss: 0.000000868106440 ### Testing Loss: 0.000000781514927
2023-04-04 20:21:07,212 ===> Epoch[728/1000]
2023-04-04 20:21:52,012 >>> Training Loss: 0.000000835436083 ### Testing Loss: 0.000000776103889
2023-04-04 20:21:52,012 ===> Epoch[729/1000]
2023-04-04 20:22:36,819 >>> Training Loss: 0.000000852494054 ### Testing Loss: 0.000000791108334
2023-04-04 20:22:36,819 ===> Epoch[730/1000]
2023-04-04 20:23:21,639 >>> Training Loss: 0.000000933253602 ### Testing Loss: 0.000000782220582
2023-04-04 20:23:21,639 ===> Epoch[731/1000]
2023-04-04 20:24:06,380 >>> Training Loss: 0.000000844351007 ### Testing Loss: 0.000000784940767
2023-04-04 20:24:06,380 ===> Epoch[732/1000]
2023-04-04 20:24:51,259 >>> Training Loss: 0.000000832844592 ### Testing Loss: 0.000000778984543
2023-04-04 20:24:51,259 ===> Epoch[733/1000]
2023-04-04 20:25:35,959 >>> Training Loss: 0.000002794239435 ### Testing Loss: 0.000001325121161
2023-04-04 20:25:35,959 ===> Epoch[734/1000]
2023-04-04 20:26:20,677 >>> Training Loss: 0.000001306246531 ### Testing Loss: 0.000000830671922
2023-04-04 20:26:20,677 ===> Epoch[735/1000]
2023-04-04 20:27:05,438 >>> Training Loss: 0.000000937153686 ### Testing Loss: 0.000000802218096
2023-04-04 20:27:05,438 ===> Epoch[736/1000]
2023-04-04 20:27:50,138 >>> Training Loss: 0.000000873551699 ### Testing Loss: 0.000000791366460
2023-04-04 20:27:50,138 ===> Epoch[737/1000]
2023-04-04 20:28:34,948 >>> Training Loss: 0.000000862832906 ### Testing Loss: 0.000000782477059
2023-04-04 20:28:34,948 ===> Epoch[738/1000]
2023-04-04 20:29:19,758 >>> Training Loss: 0.000000928340739 ### Testing Loss: 0.000000795605047
2023-04-04 20:29:19,758 ===> Epoch[739/1000]
2023-04-04 20:30:04,572 >>> Training Loss: 0.000000866945811 ### Testing Loss: 0.000000787180113
2023-04-04 20:30:04,572 ===> Epoch[740/1000]
2023-04-04 20:30:49,402 >>> Training Loss: 0.000000865867491 ### Testing Loss: 0.000000783953112
2023-04-04 20:30:49,402 ===> Epoch[741/1000]
2023-04-04 20:31:34,102 >>> Training Loss: 0.000000969999178 ### Testing Loss: 0.000000826922076
2023-04-04 20:31:34,102 ===> Epoch[742/1000]
2023-04-04 20:32:18,922 >>> Training Loss: 0.000000868899974 ### Testing Loss: 0.000000784136319
2023-04-04 20:32:18,932 ===> Epoch[743/1000]
2023-04-04 20:33:03,737 >>> Training Loss: 0.000000855364533 ### Testing Loss: 0.000000803367413
2023-04-04 20:33:03,737 ===> Epoch[744/1000]
2023-04-04 20:33:49,380 >>> Training Loss: 0.000000846517935 ### Testing Loss: 0.000000796214010
2023-04-04 20:33:49,380 ===> Epoch[745/1000]
2023-04-04 20:34:34,079 >>> Training Loss: 0.000000853529230 ### Testing Loss: 0.000000780108735
2023-04-04 20:34:34,079 ===> Epoch[746/1000]
2023-04-04 20:35:18,809 >>> Training Loss: 0.000000889732348 ### Testing Loss: 0.000000792286016
2023-04-04 20:35:18,809 ===> Epoch[747/1000]
2023-04-04 20:36:03,550 >>> Training Loss: 0.000000873844328 ### Testing Loss: 0.000000787831198
2023-04-04 20:36:03,550 ===> Epoch[748/1000]
2023-04-04 20:36:48,145 >>> Training Loss: 0.000001025913662 ### Testing Loss: 0.000000784320775
2023-04-04 20:36:48,155 ===> Epoch[749/1000]
2023-04-04 20:37:32,846 >>> Training Loss: 0.000000840883388 ### Testing Loss: 0.000000778422759
2023-04-04 20:37:32,846 ===> Epoch[750/1000]
2023-04-04 20:38:17,596 >>> Training Loss: 0.000000981217568 ### Testing Loss: 0.000000790420188
2023-04-04 20:38:17,596 ===> Epoch[751/1000]
2023-04-04 20:39:02,295 >>> Training Loss: 0.000000847908098 ### Testing Loss: 0.000000788451985
2023-04-04 20:39:02,295 ===> Epoch[752/1000]
2023-04-04 20:39:46,989 >>> Training Loss: 0.000000851458651 ### Testing Loss: 0.000000784071290
2023-04-04 20:39:46,989 ===> Epoch[753/1000]
2023-04-04 20:40:31,730 >>> Training Loss: 0.000000840793405 ### Testing Loss: 0.000000783643088
2023-04-04 20:40:31,730 ===> Epoch[754/1000]
2023-04-04 20:41:16,451 >>> Training Loss: 0.000002526783874 ### Testing Loss: 0.000024729921279
2023-04-04 20:41:16,451 ===> Epoch[755/1000]
2023-04-04 20:42:01,191 >>> Training Loss: 0.000004777802133 ### Testing Loss: 0.000001149101195
2023-04-04 20:42:01,191 ===> Epoch[756/1000]
2023-04-04 20:42:45,961 >>> Training Loss: 0.000001501049042 ### Testing Loss: 0.000000880087214
2023-04-04 20:42:45,961 ===> Epoch[757/1000]
2023-04-04 20:43:30,676 >>> Training Loss: 0.000001080775746 ### Testing Loss: 0.000000819646402
2023-04-04 20:43:30,676 ===> Epoch[758/1000]
2023-04-04 20:44:15,386 >>> Training Loss: 0.000000958693249 ### Testing Loss: 0.000000806362209
2023-04-04 20:44:15,386 ===> Epoch[759/1000]
2023-04-04 20:44:59,996 >>> Training Loss: 0.000000902577881 ### Testing Loss: 0.000000793013385
2023-04-04 20:44:59,996 ===> Epoch[760/1000]
2023-04-04 20:45:44,706 >>> Training Loss: 0.000000899400050 ### Testing Loss: 0.000000801336341
2023-04-04 20:45:44,706 ===> Epoch[761/1000]
2023-04-04 20:46:29,471 >>> Training Loss: 0.000000878037042 ### Testing Loss: 0.000000789778767
2023-04-04 20:46:29,471 ===> Epoch[762/1000]
2023-04-04 20:47:14,281 >>> Training Loss: 0.000000884088934 ### Testing Loss: 0.000000800036048
2023-04-04 20:47:14,281 ===> Epoch[763/1000]
2023-04-04 20:47:59,062 >>> Training Loss: 0.000000870969188 ### Testing Loss: 0.000000783926680
2023-04-04 20:47:59,062 ===> Epoch[764/1000]
2023-04-04 20:48:44,763 >>> Training Loss: 0.000001719391889 ### Testing Loss: 0.000000812073097
2023-04-04 20:48:44,763 ===> Epoch[765/1000]
2023-04-04 20:49:29,553 >>> Training Loss: 0.000000918724822 ### Testing Loss: 0.000000792459332
2023-04-04 20:49:29,553 ===> Epoch[766/1000]
2023-04-04 20:50:14,355 >>> Training Loss: 0.000000906617117 ### Testing Loss: 0.000000794345112
2023-04-04 20:50:14,355 ===> Epoch[767/1000]
2023-04-04 20:50:59,105 >>> Training Loss: 0.000000879482741 ### Testing Loss: 0.000000783996541
2023-04-04 20:50:59,105 ===> Epoch[768/1000]
2023-04-04 20:51:45,051 >>> Training Loss: 0.000000870909275 ### Testing Loss: 0.000000785817690
2023-04-04 20:51:45,051 ===> Epoch[769/1000]
2023-04-04 20:52:30,505 >>> Training Loss: 0.000000875370063 ### Testing Loss: 0.000000783542589
2023-04-04 20:52:30,505 ===> Epoch[770/1000]
2023-04-04 20:53:16,118 >>> Training Loss: 0.000000873019701 ### Testing Loss: 0.000000776917943
2023-04-04 20:53:16,118 ===> Epoch[771/1000]
2023-04-04 20:54:01,366 >>> Training Loss: 0.000000985494580 ### Testing Loss: 0.000000789023375
2023-04-04 20:54:01,366 ===> Epoch[772/1000]
2023-04-04 20:54:46,753 >>> Training Loss: 0.000000881040819 ### Testing Loss: 0.000000781477581
2023-04-04 20:54:46,753 ===> Epoch[773/1000]
2023-04-04 20:55:32,039 >>> Training Loss: 0.000000851982691 ### Testing Loss: 0.000000777422997
2023-04-04 20:55:32,039 ===> Epoch[774/1000]
2023-04-04 20:56:17,342 >>> Training Loss: 0.000000849391199 ### Testing Loss: 0.000000777744958
2023-04-04 20:56:17,342 ===> Epoch[775/1000]
2023-04-04 20:57:01,754 >>> Training Loss: 0.000000847760361 ### Testing Loss: 0.000000781274650
2023-04-04 20:57:01,754 ===> Epoch[776/1000]
2023-04-04 20:57:45,934 >>> Training Loss: 0.000000907260699 ### Testing Loss: 0.000000785218617
2023-04-04 20:57:45,934 ===> Epoch[777/1000]
2023-04-04 20:58:30,089 >>> Training Loss: 0.000000850553135 ### Testing Loss: 0.000000774831506
2023-04-04 20:58:30,089 ===> Epoch[778/1000]
2023-04-04 20:59:14,169 >>> Training Loss: 0.000000837723235 ### Testing Loss: 0.000000774131991
2023-04-04 20:59:14,169 ===> Epoch[779/1000]
2023-04-04 20:59:58,389 >>> Training Loss: 0.000000885940494 ### Testing Loss: 0.000000783964651
2023-04-04 20:59:58,389 ===> Epoch[780/1000]
2023-04-04 21:00:42,472 >>> Training Loss: 0.000000845537954 ### Testing Loss: 0.000000794027528
2023-04-04 21:00:42,472 ===> Epoch[781/1000]
2023-04-04 21:01:26,682 >>> Training Loss: 0.000000876435649 ### Testing Loss: 0.000000780165124
2023-04-04 21:01:26,682 ===> Epoch[782/1000]
2023-04-04 21:02:10,772 >>> Training Loss: 0.000000833055594 ### Testing Loss: 0.000000882288703
2023-04-04 21:02:10,772 ===> Epoch[783/1000]
2023-04-04 21:02:54,904 >>> Training Loss: 0.000000866245841 ### Testing Loss: 0.000000797819439
2023-04-04 21:02:54,904 ===> Epoch[784/1000]
2023-04-04 21:03:39,990 >>> Training Loss: 0.000000863168736 ### Testing Loss: 0.000000782935786
2023-04-04 21:03:39,990 ===> Epoch[785/1000]
2023-04-04 21:04:24,121 >>> Training Loss: 0.000000957274096 ### Testing Loss: 0.000000849763978
2023-04-04 21:04:24,121 ===> Epoch[786/1000]
2023-04-04 21:05:08,251 >>> Training Loss: 0.000000882288248 ### Testing Loss: 0.000000774739647
2023-04-04 21:05:08,251 ===> Epoch[787/1000]
2023-04-04 21:05:52,321 >>> Training Loss: 0.000000826190501 ### Testing Loss: 0.000000776791410
2023-04-04 21:05:52,321 ===> Epoch[788/1000]
2023-04-04 21:06:36,391 >>> Training Loss: 0.000000824247991 ### Testing Loss: 0.000000773355737
2023-04-04 21:06:36,391 ===> Epoch[789/1000]
2023-04-04 21:07:20,526 >>> Training Loss: 0.000001120275897 ### Testing Loss: 0.000000789178898
2023-04-04 21:07:20,526 ===> Epoch[790/1000]
2023-04-04 21:08:04,636 >>> Training Loss: 0.000000840086273 ### Testing Loss: 0.000000782775032
2023-04-04 21:08:04,636 ===> Epoch[791/1000]
2023-04-04 21:08:48,685 >>> Training Loss: 0.000000831159923 ### Testing Loss: 0.000000786443479
2023-04-04 21:08:48,685 ===> Epoch[792/1000]
2023-04-04 21:09:32,906 >>> Training Loss: 0.000000827115059 ### Testing Loss: 0.000000775737817
2023-04-04 21:09:32,906 ===> Epoch[793/1000]
2023-04-04 21:10:17,002 >>> Training Loss: 0.000000839256245 ### Testing Loss: 0.000000771823977
2023-04-04 21:10:17,002 ===> Epoch[794/1000]
2023-04-04 21:11:01,072 >>> Training Loss: 0.000000825569259 ### Testing Loss: 0.000000794379616
2023-04-04 21:11:01,072 ===> Epoch[795/1000]
2023-04-04 21:11:45,122 >>> Training Loss: 0.000000920502941 ### Testing Loss: 0.000000778764331
2023-04-04 21:11:45,122 ===> Epoch[796/1000]
2023-04-04 21:12:29,032 >>> Training Loss: 0.000000834391244 ### Testing Loss: 0.000000773725105
2023-04-04 21:12:29,032 ===> Epoch[797/1000]
2023-04-04 21:13:12,922 >>> Training Loss: 0.000000845495492 ### Testing Loss: 0.000000801432918
2023-04-04 21:13:12,922 ===> Epoch[798/1000]
2023-04-04 21:13:56,745 >>> Training Loss: 0.000000847736317 ### Testing Loss: 0.000000778501658
2023-04-04 21:13:56,745 ===> Epoch[799/1000]
2023-04-04 21:14:40,755 >>> Training Loss: 0.000000838033543 ### Testing Loss: 0.000000782451195
2023-04-04 21:14:40,755 ===> Epoch[800/1000]
2023-04-04 21:15:24,785 >>> Training Loss: 0.000000827066174 ### Testing Loss: 0.000000779764548
2023-04-04 21:15:24,805 ===> Epoch[801/1000]
2023-04-04 21:16:08,836 >>> Training Loss: 0.000000832496312 ### Testing Loss: 0.000000778420599
2023-04-04 21:16:08,836 ===> Epoch[802/1000]
2023-04-04 21:16:52,819 >>> Training Loss: 0.000000844040642 ### Testing Loss: 0.000000778233868
2023-04-04 21:16:52,819 ===> Epoch[803/1000]
2023-04-04 21:17:36,749 >>> Training Loss: 0.000000837466132 ### Testing Loss: 0.000000771211376
2023-04-04 21:17:36,749 ===> Epoch[804/1000]
2023-04-04 21:18:20,779 >>> Training Loss: 0.000000862986610 ### Testing Loss: 0.000000774017110
2023-04-04 21:18:20,779 ===> Epoch[805/1000]
2023-04-04 21:19:05,921 >>> Training Loss: 0.000000883869632 ### Testing Loss: 0.000000781396409
2023-04-04 21:19:05,921 ===> Epoch[806/1000]
2023-04-04 21:19:49,940 >>> Training Loss: 0.000000820441471 ### Testing Loss: 0.000000780160406
2023-04-04 21:19:49,940 ===> Epoch[807/1000]
2023-04-04 21:20:33,997 >>> Training Loss: 0.000000815254737 ### Testing Loss: 0.000000782457846
2023-04-04 21:20:33,997 ===> Epoch[808/1000]
2023-04-04 21:21:18,067 >>> Training Loss: 0.000000821031904 ### Testing Loss: 0.000000781675510
2023-04-04 21:21:18,067 ===> Epoch[809/1000]
2023-04-04 21:22:02,017 >>> Training Loss: 0.000000851839843 ### Testing Loss: 0.000000783709197
2023-04-04 21:22:02,017 ===> Epoch[810/1000]
2023-04-04 21:22:46,007 >>> Training Loss: 0.000000833418994 ### Testing Loss: 0.000000777848868
2023-04-04 21:22:46,007 ===> Epoch[811/1000]
2023-04-04 21:23:29,993 >>> Training Loss: 0.000000830073134 ### Testing Loss: 0.000000781230256
2023-04-04 21:23:29,993 ===> Epoch[812/1000]
2023-04-04 21:24:14,076 >>> Training Loss: 0.000000858654460 ### Testing Loss: 0.000000781280448
2023-04-04 21:24:14,076 ===> Epoch[813/1000]
2023-04-04 21:24:57,927 >>> Training Loss: 0.000000826004396 ### Testing Loss: 0.000000775794376
2023-04-04 21:24:57,927 ===> Epoch[814/1000]
2023-04-04 21:25:41,797 >>> Training Loss: 0.000000826560495 ### Testing Loss: 0.000000790615275
2023-04-04 21:25:41,797 ===> Epoch[815/1000]
2023-04-04 21:26:25,697 >>> Training Loss: 0.000000849733681 ### Testing Loss: 0.000000782415668
2023-04-04 21:26:25,697 ===> Epoch[816/1000]
2023-04-04 21:27:09,799 >>> Training Loss: 0.000000862550507 ### Testing Loss: 0.000000802243108
2023-04-04 21:27:09,799 ===> Epoch[817/1000]
2023-04-04 21:27:53,970 >>> Training Loss: 0.000000814843361 ### Testing Loss: 0.000000787507020
2023-04-04 21:27:53,970 ===> Epoch[818/1000]
2023-04-04 21:28:38,019 >>> Training Loss: 0.000000854784787 ### Testing Loss: 0.000000778219089
2023-04-04 21:28:38,019 ===> Epoch[819/1000]
2023-04-04 21:29:22,089 >>> Training Loss: 0.000000806455432 ### Testing Loss: 0.000000776306365
2023-04-04 21:29:22,089 ===> Epoch[820/1000]
2023-04-04 21:30:06,170 >>> Training Loss: 0.000000862358831 ### Testing Loss: 0.000000941043936
2023-04-04 21:30:06,170 ===> Epoch[821/1000]
2023-04-04 21:30:50,283 >>> Training Loss: 0.000000832869944 ### Testing Loss: 0.000000770765496
2023-04-04 21:30:50,283 ===> Epoch[822/1000]
2023-04-04 21:31:34,283 >>> Training Loss: 0.000000811851805 ### Testing Loss: 0.000000778813103
2023-04-04 21:31:34,283 ===> Epoch[823/1000]
2023-04-04 21:32:18,413 >>> Training Loss: 0.000000844004319 ### Testing Loss: 0.000000778957485
2023-04-04 21:32:18,413 ===> Epoch[824/1000]
2023-04-04 21:33:02,483 >>> Training Loss: 0.000000817113630 ### Testing Loss: 0.000000778145136
2023-04-04 21:33:02,483 ===> Epoch[825/1000]
2023-04-04 21:33:47,468 >>> Training Loss: 0.000000814126281 ### Testing Loss: 0.000000790196907
2023-04-04 21:33:47,468 ===> Epoch[826/1000]
2023-04-04 21:34:31,508 >>> Training Loss: 0.000000832099204 ### Testing Loss: 0.000000795378185
2023-04-04 21:34:31,508 ===> Epoch[827/1000]
2023-04-04 21:35:15,688 >>> Training Loss: 0.000000821741082 ### Testing Loss: 0.000000784664110
2023-04-04 21:35:15,688 ===> Epoch[828/1000]
2023-04-04 21:35:59,708 >>> Training Loss: 0.000000917031002 ### Testing Loss: 0.000000777777359
2023-04-04 21:35:59,708 ===> Epoch[829/1000]
2023-04-04 21:36:43,748 >>> Training Loss: 0.000000821967205 ### Testing Loss: 0.000000773132854
2023-04-04 21:36:43,748 ===> Epoch[830/1000]
2023-04-04 21:37:27,753 >>> Training Loss: 0.000000903108003 ### Testing Loss: 0.000000887846397
2023-04-04 21:37:27,753 ===> Epoch[831/1000]
2023-04-04 21:38:11,843 >>> Training Loss: 0.000000876588956 ### Testing Loss: 0.000000777269975
2023-04-04 21:38:11,843 ===> Epoch[832/1000]
2023-04-04 21:38:55,893 >>> Training Loss: 0.000000825001052 ### Testing Loss: 0.000000777469779
2023-04-04 21:38:55,893 ===> Epoch[833/1000]
2023-04-04 21:39:39,953 >>> Training Loss: 0.000000811462542 ### Testing Loss: 0.000000781751339
2023-04-04 21:39:39,953 ===> Epoch[834/1000]
2023-04-04 21:40:24,038 >>> Training Loss: 0.000000813870145 ### Testing Loss: 0.000000821080675
2023-04-04 21:40:24,038 ===> Epoch[835/1000]
2023-04-04 21:41:08,018 >>> Training Loss: 0.000001220407853 ### Testing Loss: 0.000000780151765
2023-04-04 21:41:08,018 ===> Epoch[836/1000]
2023-04-04 21:41:52,108 >>> Training Loss: 0.000000816878128 ### Testing Loss: 0.000000775091451
2023-04-04 21:41:52,108 ===> Epoch[837/1000]
2023-04-04 21:42:36,178 >>> Training Loss: 0.000000804926117 ### Testing Loss: 0.000000773159115
2023-04-04 21:42:36,178 ===> Epoch[838/1000]
2023-04-04 21:43:20,268 >>> Training Loss: 0.000000816045883 ### Testing Loss: 0.000000782409700
2023-04-04 21:43:20,268 ===> Epoch[839/1000]
2023-04-04 21:44:04,302 >>> Training Loss: 0.000000861763283 ### Testing Loss: 0.000000782031805
2023-04-04 21:44:04,302 ===> Epoch[840/1000]
2023-04-04 21:44:48,342 >>> Training Loss: 0.000000812661824 ### Testing Loss: 0.000000773784166
2023-04-04 21:44:48,342 ===> Epoch[841/1000]
2023-04-04 21:45:32,372 >>> Training Loss: 0.000000834100490 ### Testing Loss: 0.000000775341903
2023-04-04 21:45:32,372 ===> Epoch[842/1000]
2023-04-04 21:46:16,402 >>> Training Loss: 0.000000813641861 ### Testing Loss: 0.000000792343542
2023-04-04 21:46:16,402 ===> Epoch[843/1000]
2023-04-04 21:47:00,402 >>> Training Loss: 0.000000863284981 ### Testing Loss: 0.000000787153965
2023-04-04 21:47:00,402 ===> Epoch[844/1000]
2023-04-04 21:47:44,446 >>> Training Loss: 0.000000848030822 ### Testing Loss: 0.000000776234231
2023-04-04 21:47:44,456 ===> Epoch[845/1000]
2023-04-04 21:48:29,878 >>> Training Loss: 0.000000808788627 ### Testing Loss: 0.000000784674910
2023-04-04 21:48:29,878 ===> Epoch[846/1000]
2023-04-04 21:49:13,918 >>> Training Loss: 0.000000827380745 ### Testing Loss: 0.000000772897920
2023-04-04 21:49:13,918 ===> Epoch[847/1000]
2023-04-04 21:49:58,038 >>> Training Loss: 0.000000968614927 ### Testing Loss: 0.000000783411338
2023-04-04 21:49:58,038 ===> Epoch[848/1000]
2023-04-04 21:50:42,191 >>> Training Loss: 0.000000833410184 ### Testing Loss: 0.000000778516721
2023-04-04 21:50:42,191 ===> Epoch[849/1000]
2023-04-04 21:51:26,222 >>> Training Loss: 0.000000816510749 ### Testing Loss: 0.000000788606314
2023-04-04 21:51:26,222 ===> Epoch[850/1000]
2023-04-04 21:52:10,302 >>> Training Loss: 0.000000816850843 ### Testing Loss: 0.000000787920669
2023-04-04 21:52:10,302 ===> Epoch[851/1000]
2023-04-04 21:52:54,281 >>> Training Loss: 0.000000807662047 ### Testing Loss: 0.000000777994728
2023-04-04 21:52:54,281 ===> Epoch[852/1000]
2023-04-04 21:53:38,274 >>> Training Loss: 0.000001275690124 ### Testing Loss: 0.000000783962719
2023-04-04 21:53:38,274 ===> Epoch[853/1000]
2023-04-04 21:54:22,296 >>> Training Loss: 0.000000841784527 ### Testing Loss: 0.000000774903413
2023-04-04 21:54:22,296 ===> Epoch[854/1000]
2023-04-04 21:55:06,407 >>> Training Loss: 0.000000809172604 ### Testing Loss: 0.000000782579320
2023-04-04 21:55:06,407 ===> Epoch[855/1000]
2023-04-04 21:55:50,447 >>> Training Loss: 0.000000822001596 ### Testing Loss: 0.000000771742009
2023-04-04 21:55:50,447 ===> Epoch[856/1000]
2023-04-04 21:56:34,427 >>> Training Loss: 0.000000815324540 ### Testing Loss: 0.000000787555280
2023-04-04 21:56:34,427 ===> Epoch[857/1000]
2023-04-04 21:57:18,479 >>> Training Loss: 0.000000814515658 ### Testing Loss: 0.000000772802309
2023-04-04 21:57:18,479 ===> Epoch[858/1000]
2023-04-04 21:58:02,519 >>> Training Loss: 0.000001043711677 ### Testing Loss: 0.000000780790572
2023-04-04 21:58:02,519 ===> Epoch[859/1000]
2023-04-04 21:58:46,630 >>> Training Loss: 0.000000825191592 ### Testing Loss: 0.000000773004501
2023-04-04 21:58:46,630 ===> Epoch[860/1000]
2023-04-04 21:59:30,640 >>> Training Loss: 0.000000803206376 ### Testing Loss: 0.000000774978503
2023-04-04 21:59:30,640 ===> Epoch[861/1000]
2023-04-04 22:00:14,690 >>> Training Loss: 0.000000803315118 ### Testing Loss: 0.000000781604570
2023-04-04 22:00:14,690 ===> Epoch[862/1000]
2023-04-04 22:00:58,643 >>> Training Loss: 0.000000808849393 ### Testing Loss: 0.000000785169505
2023-04-04 22:00:58,643 ===> Epoch[863/1000]
2023-04-04 22:01:42,603 >>> Training Loss: 0.000000818699334 ### Testing Loss: 0.000000778009053
2023-04-04 22:01:42,603 ===> Epoch[864/1000]
2023-04-04 22:02:26,583 >>> Training Loss: 0.000000826014457 ### Testing Loss: 0.000000773649106
2023-04-04 22:02:26,583 ===> Epoch[865/1000]
2023-04-04 22:03:10,513 >>> Training Loss: 0.000000813467238 ### Testing Loss: 0.000000780524090
2023-04-04 22:03:10,513 ===> Epoch[866/1000]
2023-04-04 22:03:55,440 >>> Training Loss: 0.000000854907341 ### Testing Loss: 0.000000781039034
2023-04-04 22:03:55,440 ===> Epoch[867/1000]
2023-04-04 22:04:39,543 >>> Training Loss: 0.000000820098080 ### Testing Loss: 0.000000808648792
2023-04-04 22:04:39,543 ===> Epoch[868/1000]
2023-04-04 22:05:23,543 >>> Training Loss: 0.000000822082200 ### Testing Loss: 0.000000789104490
2023-04-04 22:05:23,543 ===> Epoch[869/1000]
2023-04-04 22:06:07,603 >>> Training Loss: 0.000000823755386 ### Testing Loss: 0.000000772849944
2023-04-04 22:06:07,603 ===> Epoch[870/1000]
2023-04-04 22:06:51,623 >>> Training Loss: 0.000000813075701 ### Testing Loss: 0.000000800376711
2023-04-04 22:06:51,623 ===> Epoch[871/1000]
2023-04-04 22:07:35,768 >>> Training Loss: 0.000001210775508 ### Testing Loss: 0.000000789529565
2023-04-04 22:07:35,768 ===> Epoch[872/1000]
2023-04-04 22:08:19,778 >>> Training Loss: 0.000000825861719 ### Testing Loss: 0.000000788255363
2023-04-04 22:08:19,778 ===> Epoch[873/1000]
2023-04-04 22:09:03,889 >>> Training Loss: 0.000000804627120 ### Testing Loss: 0.000000770215536
2023-04-04 22:09:03,889 ===> Epoch[874/1000]
2023-04-04 22:09:47,938 >>> Training Loss: 0.000000815256783 ### Testing Loss: 0.000000777836306
2023-04-04 22:09:47,938 ===> Epoch[875/1000]
2023-04-04 22:10:32,008 >>> Training Loss: 0.000000832207320 ### Testing Loss: 0.000000777365130
2023-04-04 22:10:32,008 ===> Epoch[876/1000]
2023-04-04 22:11:16,021 >>> Training Loss: 0.000000800381997 ### Testing Loss: 0.000000769846622
2023-04-04 22:11:16,021 ===> Epoch[877/1000]
2023-04-04 22:11:59,982 >>> Training Loss: 0.000000845241061 ### Testing Loss: 0.000000784666895
2023-04-04 22:11:59,982 ===> Epoch[878/1000]
2023-04-04 22:12:44,052 >>> Training Loss: 0.000000799628367 ### Testing Loss: 0.000000770925794
2023-04-04 22:12:44,052 ===> Epoch[879/1000]
2023-04-04 22:13:28,071 >>> Training Loss: 0.000000832275873 ### Testing Loss: 0.000000793387358
2023-04-04 22:13:28,071 ===> Epoch[880/1000]
2023-04-04 22:14:12,092 >>> Training Loss: 0.000000807824904 ### Testing Loss: 0.000000823203095
2023-04-04 22:14:12,092 ===> Epoch[881/1000]
2023-04-04 22:14:56,090 >>> Training Loss: 0.000000811091581 ### Testing Loss: 0.000000779685763
2023-04-04 22:14:56,090 ===> Epoch[882/1000]
2023-04-04 22:15:40,111 >>> Training Loss: 0.000000816539739 ### Testing Loss: 0.000000779753748
2023-04-04 22:15:40,111 ===> Epoch[883/1000]
2023-04-04 22:16:24,092 >>> Training Loss: 0.000000807261586 ### Testing Loss: 0.000000790597198
2023-04-04 22:16:24,092 ===> Epoch[884/1000]
2023-04-04 22:17:08,182 >>> Training Loss: 0.000000900182101 ### Testing Loss: 0.000000775658862
2023-04-04 22:17:08,182 ===> Epoch[885/1000]
2023-04-04 22:17:52,287 >>> Training Loss: 0.000000801404724 ### Testing Loss: 0.000000777779007
2023-04-04 22:17:52,287 ===> Epoch[886/1000]
2023-04-04 22:18:37,529 >>> Training Loss: 0.000000819701654 ### Testing Loss: 0.000000801005456
2023-04-04 22:18:37,529 ===> Epoch[887/1000]
2023-04-04 22:19:21,559 >>> Training Loss: 0.000000810613415 ### Testing Loss: 0.000000777470916
2023-04-04 22:19:21,559 ===> Epoch[888/1000]
2023-04-04 22:20:05,639 >>> Training Loss: 0.000000802321438 ### Testing Loss: 0.000000768511654
2023-04-04 22:20:05,639 ===> Epoch[889/1000]
2023-04-04 22:20:49,790 >>> Training Loss: 0.000000803001058 ### Testing Loss: 0.000000770929944
2023-04-04 22:20:49,790 ===> Epoch[890/1000]
2023-04-04 22:21:33,912 >>> Training Loss: 0.000000850959793 ### Testing Loss: 0.000000810827032
2023-04-04 22:21:33,912 ===> Epoch[891/1000]
2023-04-04 22:22:18,002 >>> Training Loss: 0.000000799600912 ### Testing Loss: 0.000000780136020
2023-04-04 22:22:18,002 ===> Epoch[892/1000]
2023-04-04 22:23:02,052 >>> Training Loss: 0.000000809843471 ### Testing Loss: 0.000000791070647
2023-04-04 22:23:02,052 ===> Epoch[893/1000]
2023-04-04 22:23:46,152 >>> Training Loss: 0.000000810054928 ### Testing Loss: 0.000000866894709
2023-04-04 22:23:46,152 ===> Epoch[894/1000]
2023-04-04 22:24:30,254 >>> Training Loss: 0.000000868004747 ### Testing Loss: 0.000000781642598
2023-04-04 22:24:30,254 ===> Epoch[895/1000]
2023-04-04 22:25:14,264 >>> Training Loss: 0.000000859096701 ### Testing Loss: 0.000001738240826
2023-04-04 22:25:14,264 ===> Epoch[896/1000]
2023-04-04 22:25:58,444 >>> Training Loss: 0.000000856821941 ### Testing Loss: 0.000000780077471
2023-04-04 22:25:58,444 ===> Epoch[897/1000]
2023-04-04 22:26:42,484 >>> Training Loss: 0.000000794683046 ### Testing Loss: 0.000000778379501
2023-04-04 22:26:42,484 ===> Epoch[898/1000]
2023-04-04 22:27:26,544 >>> Training Loss: 0.000000834057573 ### Testing Loss: 0.000000776990589
2023-04-04 22:27:26,544 ===> Epoch[899/1000]
2023-04-04 22:28:10,559 >>> Training Loss: 0.000000796820871 ### Testing Loss: 0.000000780447920
2023-04-04 22:28:10,559 ===> Epoch[900/1000]
2023-04-04 22:28:54,560 >>> Training Loss: 0.000000809507299 ### Testing Loss: 0.000000780955304
2023-04-04 22:28:54,580 ===> Epoch[901/1000]
2023-04-04 22:29:38,630 >>> Training Loss: 0.000000859148031 ### Testing Loss: 0.000000792699723
2023-04-04 22:29:38,630 ===> Epoch[902/1000]
2023-04-04 22:30:22,709 >>> Training Loss: 0.000000812332019 ### Testing Loss: 0.000000806318099
2023-04-04 22:30:22,709 ===> Epoch[903/1000]
2023-04-04 22:31:06,759 >>> Training Loss: 0.000000806325318 ### Testing Loss: 0.000000783997052
2023-04-04 22:31:06,759 ===> Epoch[904/1000]
2023-04-04 22:31:50,767 >>> Training Loss: 0.000000806626019 ### Testing Loss: 0.000000780889991
2023-04-04 22:31:50,767 ===> Epoch[905/1000]
2023-04-04 22:32:34,777 >>> Training Loss: 0.000000924919561 ### Testing Loss: 0.000000775097305
2023-04-04 22:32:34,777 ===> Epoch[906/1000]
2023-04-04 22:33:18,857 >>> Training Loss: 0.000000799568113 ### Testing Loss: 0.000000776684487
2023-04-04 22:33:18,857 ===> Epoch[907/1000]
2023-04-04 22:34:03,778 >>> Training Loss: 0.000000796084237 ### Testing Loss: 0.000000774719183
2023-04-04 22:34:03,778 ===> Epoch[908/1000]
2023-04-04 22:34:47,843 >>> Training Loss: 0.000000803218825 ### Testing Loss: 0.000001137310164
2023-04-04 22:34:47,843 ===> Epoch[909/1000]
2023-04-04 22:35:31,933 >>> Training Loss: 0.000001038156142 ### Testing Loss: 0.000000769836504
2023-04-04 22:35:31,933 ===> Epoch[910/1000]
2023-04-04 22:36:15,843 >>> Training Loss: 0.000000796346626 ### Testing Loss: 0.000000775479180
2023-04-04 22:36:15,843 ===> Epoch[911/1000]
2023-04-04 22:36:59,844 >>> Training Loss: 0.000000795558833 ### Testing Loss: 0.000000771673683
2023-04-04 22:36:59,844 ===> Epoch[912/1000]
2023-04-04 22:37:43,894 >>> Training Loss: 0.000000859290822 ### Testing Loss: 0.000000796038989
2023-04-04 22:37:43,894 ===> Epoch[913/1000]
2023-04-04 22:38:27,869 >>> Training Loss: 0.000000825567383 ### Testing Loss: 0.000000778816855
2023-04-04 22:38:27,869 ===> Epoch[914/1000]
2023-04-04 22:39:11,948 >>> Training Loss: 0.000000796006418 ### Testing Loss: 0.000000780787445
2023-04-04 22:39:11,948 ===> Epoch[915/1000]
2023-04-04 22:39:55,952 >>> Training Loss: 0.000000794983066 ### Testing Loss: 0.000000781971721
2023-04-04 22:39:55,952 ===> Epoch[916/1000]
2023-04-04 22:40:39,892 >>> Training Loss: 0.000000863083983 ### Testing Loss: 0.000000777777359
2023-04-04 22:40:39,892 ===> Epoch[917/1000]
2023-04-04 22:41:23,942 >>> Training Loss: 0.000000796914151 ### Testing Loss: 0.000000771924931
2023-04-04 22:41:23,942 ===> Epoch[918/1000]
2023-04-04 22:42:07,948 >>> Training Loss: 0.000000791924776 ### Testing Loss: 0.000000775185356
2023-04-04 22:42:07,948 ===> Epoch[919/1000]
2023-04-04 22:42:51,888 >>> Training Loss: 0.000000830615534 ### Testing Loss: 0.000000781948302
2023-04-04 22:42:51,888 ===> Epoch[920/1000]
2023-04-04 22:43:35,969 >>> Training Loss: 0.000000793202958 ### Testing Loss: 0.000000770797101
2023-04-04 22:43:35,969 ===> Epoch[921/1000]
2023-04-04 22:44:19,938 >>> Training Loss: 0.000000867100766 ### Testing Loss: 0.000000775042338
2023-04-04 22:44:19,938 ===> Epoch[922/1000]
2023-04-04 22:45:03,992 >>> Training Loss: 0.000000788781449 ### Testing Loss: 0.000000784119266
2023-04-04 22:45:03,992 ===> Epoch[923/1000]
2023-04-04 22:45:48,042 >>> Training Loss: 0.000000789789681 ### Testing Loss: 0.000000800335044
2023-04-04 22:45:48,042 ===> Epoch[924/1000]
2023-04-04 22:46:32,152 >>> Training Loss: 0.000000813289034 ### Testing Loss: 0.000000797190637
2023-04-04 22:46:32,152 ===> Epoch[925/1000]
2023-04-04 22:47:16,133 >>> Training Loss: 0.000000794183222 ### Testing Loss: 0.000000786338262
2023-04-04 22:47:16,133 ===> Epoch[926/1000]
2023-04-04 22:48:00,093 >>> Training Loss: 0.000000801518638 ### Testing Loss: 0.000000773041734
2023-04-04 22:48:00,093 ===> Epoch[927/1000]
2023-04-04 22:48:44,898 >>> Training Loss: 0.000000817079297 ### Testing Loss: 0.000000773792124
2023-04-04 22:48:44,898 ===> Epoch[928/1000]
2023-04-04 22:49:28,858 >>> Training Loss: 0.000000792724393 ### Testing Loss: 0.000001054133349
2023-04-04 22:49:28,858 ===> Epoch[929/1000]
2023-04-04 22:50:12,988 >>> Training Loss: 0.000000819815398 ### Testing Loss: 0.000000768436564
2023-04-04 22:50:12,988 ===> Epoch[930/1000]
2023-04-04 22:50:57,078 >>> Training Loss: 0.000000788617513 ### Testing Loss: 0.000000766230642
2023-04-04 22:50:57,078 ===> Epoch[931/1000]
2023-04-04 22:51:40,995 >>> Training Loss: 0.000000858366150 ### Testing Loss: 0.000000786006808
2023-04-04 22:51:40,995 ===> Epoch[932/1000]
2023-04-04 22:52:24,984 >>> Training Loss: 0.000000791125274 ### Testing Loss: 0.000000775992987
2023-04-04 22:52:24,984 ===> Epoch[933/1000]
2023-04-04 22:53:09,024 >>> Training Loss: 0.000000849931894 ### Testing Loss: 0.000000778929973
2023-04-04 22:53:09,024 ===> Epoch[934/1000]
2023-04-04 22:53:53,104 >>> Training Loss: 0.000000807852246 ### Testing Loss: 0.000000771488999
2023-04-04 22:53:53,104 ===> Epoch[935/1000]
2023-04-04 22:54:37,194 >>> Training Loss: 0.000000795354822 ### Testing Loss: 0.000000780886865
2023-04-04 22:54:37,194 ===> Epoch[936/1000]
2023-04-04 22:55:21,158 >>> Training Loss: 0.000004248452569 ### Testing Loss: 0.000001148221941
2023-04-04 22:55:21,158 ===> Epoch[937/1000]
2023-04-04 22:56:05,168 >>> Training Loss: 0.000001395633603 ### Testing Loss: 0.000000844237093
2023-04-04 22:56:05,168 ===> Epoch[938/1000]
2023-04-04 22:56:49,218 >>> Training Loss: 0.000000948935963 ### Testing Loss: 0.000000793940842
2023-04-04 22:56:49,218 ===> Epoch[939/1000]
2023-04-04 22:57:33,258 >>> Training Loss: 0.000000884177268 ### Testing Loss: 0.000000786021758
2023-04-04 22:57:33,258 ===> Epoch[940/1000]
2023-04-04 22:58:17,298 >>> Training Loss: 0.000000837220739 ### Testing Loss: 0.000000776780269
2023-04-04 22:58:17,298 ===> Epoch[941/1000]
2023-04-04 22:59:01,323 >>> Training Loss: 0.000000867472068 ### Testing Loss: 0.000000776429090
2023-04-04 22:59:01,323 ===> Epoch[942/1000]
2023-04-04 22:59:45,393 >>> Training Loss: 0.000000819938521 ### Testing Loss: 0.000000778559468
2023-04-04 22:59:45,393 ===> Epoch[943/1000]
2023-04-04 23:00:29,463 >>> Training Loss: 0.000000848201353 ### Testing Loss: 0.000000774092939
2023-04-04 23:00:29,463 ===> Epoch[944/1000]
2023-04-04 23:01:13,463 >>> Training Loss: 0.000005497916845 ### Testing Loss: 0.000002341182608
2023-04-04 23:01:13,463 ===> Epoch[945/1000]
2023-04-04 23:01:57,515 >>> Training Loss: 0.000003897482657 ### Testing Loss: 0.000001431610031
2023-04-04 23:01:57,515 ===> Epoch[946/1000]
2023-04-04 23:02:41,506 >>> Training Loss: 0.000002185742005 ### Testing Loss: 0.000001052345283
2023-04-04 23:02:41,506 ===> Epoch[947/1000]
2023-04-04 23:03:25,476 >>> Training Loss: 0.000001530330223 ### Testing Loss: 0.000000942556142
2023-04-04 23:03:25,476 ===> Epoch[948/1000]
2023-04-04 23:04:10,329 >>> Training Loss: 0.000001244204896 ### Testing Loss: 0.000000865551442
2023-04-04 23:04:10,329 ===> Epoch[949/1000]
2023-04-04 23:04:54,349 >>> Training Loss: 0.000001092237994 ### Testing Loss: 0.000000823415917
2023-04-04 23:04:54,349 ===> Epoch[950/1000]
2023-04-04 23:05:38,473 >>> Training Loss: 0.000001007499918 ### Testing Loss: 0.000000824511289
2023-04-04 23:05:38,473 ===> Epoch[951/1000]
2023-04-04 23:06:22,463 >>> Training Loss: 0.000000947256581 ### Testing Loss: 0.000000807541426
2023-04-04 23:06:22,463 ===> Epoch[952/1000]
2023-04-04 23:07:06,514 >>> Training Loss: 0.000000914916313 ### Testing Loss: 0.000000795280528
2023-04-04 23:07:06,514 ===> Epoch[953/1000]
2023-04-04 23:07:50,494 >>> Training Loss: 0.000000917002637 ### Testing Loss: 0.000000792572564
2023-04-04 23:07:50,494 ===> Epoch[954/1000]
2023-04-04 23:08:34,448 >>> Training Loss: 0.000000937753953 ### Testing Loss: 0.000000807713377
2023-04-04 23:08:34,448 ===> Epoch[955/1000]
2023-04-04 23:09:18,449 >>> Training Loss: 0.000000885097791 ### Testing Loss: 0.000000801049111
2023-04-04 23:09:18,449 ===> Epoch[956/1000]
2023-04-04 23:10:02,479 >>> Training Loss: 0.000000890515651 ### Testing Loss: 0.000000785929842
2023-04-04 23:10:02,479 ===> Epoch[957/1000]
2023-04-04 23:10:46,559 >>> Training Loss: 0.000000928458064 ### Testing Loss: 0.000000786707687
2023-04-04 23:10:46,559 ===> Epoch[958/1000]
2023-04-04 23:11:30,579 >>> Training Loss: 0.000000874399518 ### Testing Loss: 0.000000775310411
2023-04-04 23:11:30,579 ===> Epoch[959/1000]
2023-04-04 23:12:14,517 >>> Training Loss: 0.000000861791705 ### Testing Loss: 0.000000776079901
2023-04-04 23:12:14,517 ===> Epoch[960/1000]
2023-04-04 23:12:58,497 >>> Training Loss: 0.000000833128524 ### Testing Loss: 0.000000790892329
2023-04-04 23:12:58,497 ===> Epoch[961/1000]
2023-04-04 23:13:42,527 >>> Training Loss: 0.000000855368626 ### Testing Loss: 0.000000771350528
2023-04-04 23:13:42,527 ===> Epoch[962/1000]
2023-04-04 23:14:26,528 >>> Training Loss: 0.000000834413868 ### Testing Loss: 0.000000780974233
2023-04-04 23:14:26,528 ===> Epoch[963/1000]
2023-04-04 23:15:10,528 >>> Training Loss: 0.000000836629340 ### Testing Loss: 0.000000773202373
2023-04-04 23:15:10,528 ===> Epoch[964/1000]
2023-04-04 23:15:54,565 >>> Training Loss: 0.000001639327024 ### Testing Loss: 0.000000985415113
2023-04-04 23:15:54,565 ===> Epoch[965/1000]
2023-04-04 23:16:38,585 >>> Training Loss: 0.000000992027708 ### Testing Loss: 0.000000780079802
2023-04-04 23:16:38,585 ===> Epoch[966/1000]
2023-04-04 23:17:22,576 >>> Training Loss: 0.000000845991963 ### Testing Loss: 0.000000767390759
2023-04-04 23:17:22,576 ===> Epoch[967/1000]
2023-04-04 23:18:06,565 >>> Training Loss: 0.000000816868351 ### Testing Loss: 0.000000766846085
2023-04-04 23:18:06,565 ===> Epoch[968/1000]
2023-04-04 23:18:51,542 >>> Training Loss: 0.000000822171955 ### Testing Loss: 0.000000769098108
2023-04-04 23:18:51,542 ===> Epoch[969/1000]
2023-04-04 23:19:35,602 >>> Training Loss: 0.000000813923805 ### Testing Loss: 0.000000771238604
2023-04-04 23:19:35,602 ===> Epoch[970/1000]
2023-04-04 23:20:19,622 >>> Training Loss: 0.000000818310752 ### Testing Loss: 0.000000776577735
2023-04-04 23:20:19,622 ===> Epoch[971/1000]
2023-04-04 23:21:03,672 >>> Training Loss: 0.000000825658674 ### Testing Loss: 0.000000784058557
2023-04-04 23:21:03,672 ===> Epoch[972/1000]
2023-04-04 23:21:47,582 >>> Training Loss: 0.000000839212021 ### Testing Loss: 0.000000778176002
2023-04-04 23:21:47,582 ===> Epoch[973/1000]
2023-04-04 23:22:31,622 >>> Training Loss: 0.000000832269166 ### Testing Loss: 0.000000818886917
2023-04-04 23:22:31,622 ===> Epoch[974/1000]
2023-04-04 23:23:15,572 >>> Training Loss: 0.000000970728593 ### Testing Loss: 0.000000787121508
2023-04-04 23:23:15,572 ===> Epoch[975/1000]
2023-04-04 23:23:59,492 >>> Training Loss: 0.000000831028046 ### Testing Loss: 0.000000765151640
2023-04-04 23:23:59,492 ===> Epoch[976/1000]
2023-04-04 23:24:43,422 >>> Training Loss: 0.000000801352485 ### Testing Loss: 0.000000770351903
2023-04-04 23:24:43,422 ===> Epoch[977/1000]
2023-04-04 23:25:27,422 >>> Training Loss: 0.000000834975992 ### Testing Loss: 0.000000771993484
2023-04-04 23:25:27,422 ===> Epoch[978/1000]
2023-04-04 23:26:11,435 >>> Training Loss: 0.000000845819557 ### Testing Loss: 0.000000767980055
2023-04-04 23:26:11,435 ===> Epoch[979/1000]
2023-04-04 23:26:55,435 >>> Training Loss: 0.000000793924642 ### Testing Loss: 0.000000862983597
2023-04-04 23:26:55,435 ===> Epoch[980/1000]
2023-04-04 23:27:39,405 >>> Training Loss: 0.000000810806227 ### Testing Loss: 0.000000772774911
2023-04-04 23:27:39,405 ===> Epoch[981/1000]
2023-04-04 23:28:23,386 >>> Training Loss: 0.000000865447134 ### Testing Loss: 0.000000773422926
2023-04-04 23:28:23,386 ===> Epoch[982/1000]
2023-04-04 23:29:07,286 >>> Training Loss: 0.000000802547049 ### Testing Loss: 0.000000775536535
2023-04-04 23:29:07,286 ===> Epoch[983/1000]
2023-04-04 23:29:51,329 >>> Training Loss: 0.000000799969030 ### Testing Loss: 0.000000776023398
2023-04-04 23:29:51,329 ===> Epoch[984/1000]
2023-04-04 23:30:35,349 >>> Training Loss: 0.000000916453303 ### Testing Loss: 0.000000772877968
2023-04-04 23:30:35,349 ===> Epoch[985/1000]
2023-04-04 23:31:19,369 >>> Training Loss: 0.000000805185266 ### Testing Loss: 0.000000766177777
2023-04-04 23:31:19,369 ===> Epoch[986/1000]
2023-04-04 23:32:03,340 >>> Training Loss: 0.000000789043838 ### Testing Loss: 0.000000771826819
2023-04-04 23:32:03,340 ===> Epoch[987/1000]
2023-04-04 23:32:47,293 >>> Training Loss: 0.000000885882912 ### Testing Loss: 0.000000776023910
2023-04-04 23:32:47,293 ===> Epoch[988/1000]
2023-04-04 23:33:32,665 >>> Training Loss: 0.000000804819535 ### Testing Loss: 0.000000765229970
2023-04-04 23:33:32,665 ===> Epoch[989/1000]
2023-04-04 23:34:16,495 >>> Training Loss: 0.000000795030417 ### Testing Loss: 0.000000782739335
2023-04-04 23:34:16,495 ===> Epoch[990/1000]
2023-04-04 23:35:00,455 >>> Training Loss: 0.000000961679348 ### Testing Loss: 0.000000770751967
2023-04-04 23:35:00,455 ===> Epoch[991/1000]
2023-04-04 23:35:44,406 >>> Training Loss: 0.000000796040979 ### Testing Loss: 0.000000770521353
2023-04-04 23:35:44,406 ===> Epoch[992/1000]
2023-04-04 23:36:28,339 >>> Training Loss: 0.000000793858646 ### Testing Loss: 0.000000770022950
2023-04-04 23:36:28,339 ===> Epoch[993/1000]
2023-04-04 23:37:12,259 >>> Training Loss: 0.000000797631003 ### Testing Loss: 0.000000779585207
2023-04-04 23:37:12,259 ===> Epoch[994/1000]
2023-04-04 23:37:56,339 >>> Training Loss: 0.000000812311896 ### Testing Loss: 0.000000786366684
2023-04-04 23:37:56,339 ===> Epoch[995/1000]
2023-04-04 23:38:40,199 >>> Training Loss: 0.000000916057616 ### Testing Loss: 0.000000776545676
2023-04-04 23:38:40,199 ===> Epoch[996/1000]
2023-04-04 23:39:24,240 >>> Training Loss: 0.000000833636705 ### Testing Loss: 0.000000769289727
2023-04-04 23:39:24,240 ===> Epoch[997/1000]
2023-04-04 23:40:08,140 >>> Training Loss: 0.000000846927719 ### Testing Loss: 0.000001536168725
2023-04-04 23:40:08,140 ===> Epoch[998/1000]
2023-04-04 23:40:52,111 >>> Training Loss: 0.000000850661991 ### Testing Loss: 0.000000769768860
2023-04-04 23:40:52,121 ===> Epoch[999/1000]
2023-04-04 23:41:36,120 >>> Training Loss: 0.000000786047963 ### Testing Loss: 0.000000767419863
2023-04-04 23:41:36,120 ===> Epoch[1000/1000]
2023-04-04 23:42:20,051 >>> Training Loss: 0.000000786955411 ### Testing Loss: 0.000000768172072
