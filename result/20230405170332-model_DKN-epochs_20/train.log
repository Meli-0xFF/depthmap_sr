2023-04-05 17:04:26,765 ===> Epoch[1/20]
2023-04-05 17:20:42,714 >>> Training Loss: 0.000007870916306 ### Testing Loss: 0.000003123646138
2023-04-05 17:20:42,714 ===> Epoch[2/20]
2023-04-05 17:37:10,100 >>> Training Loss: 0.000006300962013 ### Testing Loss: 0.000002687878577
2023-04-05 17:37:10,100 ===> Epoch[3/20]
2023-04-05 17:53:40,933 >>> Training Loss: 0.000005929963663 ### Testing Loss: 0.000002568837772
2023-04-05 17:53:40,933 ===> Epoch[4/20]
2023-04-05 18:10:13,340 >>> Training Loss: 0.000005437358141 ### Testing Loss: 0.000002435514261
2023-04-05 18:10:13,340 ===> Epoch[5/20]
2023-04-05 18:26:47,512 >>> Training Loss: 0.000005170616987 ### Testing Loss: 0.000002445438213
2023-04-05 18:26:47,512 ===> Epoch[6/20]
2023-04-05 18:43:26,972 >>> Training Loss: 0.000004955890290 ### Testing Loss: 0.000002315757683
2023-04-05 18:43:26,972 ===> Epoch[7/20]
2023-04-05 19:00:00,528 >>> Training Loss: 0.000004778094990 ### Testing Loss: 0.000002243980816
2023-04-05 19:00:00,528 ===> Epoch[8/20]
2023-04-05 19:16:33,269 >>> Training Loss: 0.000004699084911 ### Testing Loss: 0.000002188941153
2023-04-05 19:16:33,269 ===> Epoch[9/20]
2023-04-05 19:33:05,991 >>> Training Loss: 0.000004501810963 ### Testing Loss: 0.000002111120921
2023-04-05 19:33:05,991 ===> Epoch[10/20]
2023-04-05 19:49:38,139 >>> Training Loss: 0.000004406042535 ### Testing Loss: 0.000002151513399
2023-04-05 19:49:38,139 ===> Epoch[11/20]
2023-04-05 20:06:09,982 >>> Training Loss: 0.000004253206953 ### Testing Loss: 0.000002076314786
2023-04-05 20:06:09,982 ===> Epoch[12/20]
2023-04-05 20:22:42,041 >>> Training Loss: 0.000004157200692 ### Testing Loss: 0.000001986554025
2023-04-05 20:22:42,041 ===> Epoch[13/20]
2023-04-05 20:39:14,004 >>> Training Loss: 0.000004072170213 ### Testing Loss: 0.000002185087396
2023-04-05 20:39:14,004 ===> Epoch[14/20]
2023-04-05 20:55:44,889 >>> Training Loss: 0.000003881452812 ### Testing Loss: 0.000002089000191
2023-04-05 20:55:44,889 ===> Epoch[15/20]
2023-04-05 21:12:15,788 >>> Training Loss: 0.000003818256118 ### Testing Loss: 0.000002012985988
2023-04-05 21:12:15,788 ===> Epoch[16/20]
2023-04-05 21:28:47,002 >>> Training Loss: 0.000003794440090 ### Testing Loss: 0.000001961636826
2023-04-05 21:28:47,002 ===> Epoch[17/20]
2023-04-05 21:45:17,863 >>> Training Loss: 0.000003768076340 ### Testing Loss: 0.000002118909151
2023-04-05 21:45:17,873 ===> Epoch[18/20]
2023-04-05 22:01:49,057 >>> Training Loss: 0.000003746740731 ### Testing Loss: 0.000002032177463
2023-04-05 22:01:49,057 ===> Epoch[19/20]
2023-04-05 22:18:19,940 >>> Training Loss: 0.000003725405122 ### Testing Loss: 0.000002032975090
2023-04-05 22:18:19,940 ===> Epoch[20/20]
2023-04-05 22:34:51,849 >>> Training Loss: 0.000003701858077 ### Testing Loss: 0.000001907897513
