2023-03-31 17:17:31,242 ===> Epoch[1/1000]
2023-03-31 17:17:57,387 >>> Training Loss: 0.000007758281754 ### Testing Loss: 0.000002774981795
2023-03-31 17:17:57,387 ===> Epoch[2/1000]
2023-03-31 17:18:19,785 >>> Training Loss: 0.000005436988886 ### Testing Loss: 0.000002147210580
2023-03-31 17:18:19,786 ===> Epoch[3/1000]
2023-03-31 17:18:41,599 >>> Training Loss: 0.000004396911208 ### Testing Loss: 0.000001890998647
2023-03-31 17:18:41,599 ===> Epoch[4/1000]
2023-03-31 17:19:03,969 >>> Training Loss: 0.000003675681683 ### Testing Loss: 0.000001396065386
2023-03-31 17:19:03,969 ===> Epoch[5/1000]
2023-03-31 17:19:28,005 >>> Training Loss: 0.000003085226808 ### Testing Loss: 0.000001247086288
2023-03-31 17:19:28,005 ===> Epoch[6/1000]
2023-03-31 17:19:50,907 >>> Training Loss: 0.000002684578931 ### Testing Loss: 0.000001279901539
2023-03-31 17:19:50,907 ===> Epoch[7/1000]
2023-03-31 17:20:13,960 >>> Training Loss: 0.000002484138349 ### Testing Loss: 0.000001145137503
2023-03-31 17:20:13,961 ===> Epoch[8/1000]
2023-03-31 17:20:37,182 >>> Training Loss: 0.000002364143256 ### Testing Loss: 0.000000994146944
2023-03-31 17:20:37,182 ===> Epoch[9/1000]
2023-03-31 17:21:00,412 >>> Training Loss: 0.000002220075885 ### Testing Loss: 0.000000981501557
2023-03-31 17:21:00,412 ===> Epoch[10/1000]
2023-03-31 17:21:23,482 >>> Training Loss: 0.000002241938319 ### Testing Loss: 0.000000915906298
2023-03-31 17:21:23,483 ===> Epoch[11/1000]
2023-03-31 17:21:46,583 >>> Training Loss: 0.000001977026841 ### Testing Loss: 0.000000872263399
2023-03-31 17:21:46,583 ===> Epoch[12/1000]
2023-03-31 17:22:09,229 >>> Training Loss: 0.000001957190761 ### Testing Loss: 0.000000811861469
2023-03-31 17:22:09,229 ===> Epoch[13/1000]
2023-03-31 17:22:31,949 >>> Training Loss: 0.000001833350439 ### Testing Loss: 0.000000817644604
2023-03-31 17:22:31,949 ===> Epoch[14/1000]
2023-03-31 17:22:54,696 >>> Training Loss: 0.000001734824309 ### Testing Loss: 0.000000792433241
2023-03-31 17:22:54,696 ===> Epoch[15/1000]
2023-03-31 17:23:17,516 >>> Training Loss: 0.000001800983796 ### Testing Loss: 0.000000764454171
2023-03-31 17:23:17,516 ===> Epoch[16/1000]
2023-03-31 17:23:40,336 >>> Training Loss: 0.000001649617161 ### Testing Loss: 0.000000732720309
2023-03-31 17:23:40,336 ===> Epoch[17/1000]
2023-03-31 17:24:03,237 >>> Training Loss: 0.000001569428719 ### Testing Loss: 0.000000750705397
2023-03-31 17:24:03,237 ===> Epoch[18/1000]
2023-03-31 17:24:26,147 >>> Training Loss: 0.000001521144668 ### Testing Loss: 0.000000691932428
2023-03-31 17:24:26,147 ===> Epoch[19/1000]
2023-03-31 17:24:49,137 >>> Training Loss: 0.000001486055794 ### Testing Loss: 0.000000677048320
2023-03-31 17:24:49,137 ===> Epoch[20/1000]
2023-03-31 17:25:12,117 >>> Training Loss: 0.000001429729195 ### Testing Loss: 0.000000691828859
2023-03-31 17:25:12,117 ===> Epoch[21/1000]
2023-03-31 17:25:35,607 >>> Training Loss: 0.000001399520443 ### Testing Loss: 0.000000661009665
2023-03-31 17:25:35,607 ===> Epoch[22/1000]
2023-03-31 17:25:58,767 >>> Training Loss: 0.000001384272196 ### Testing Loss: 0.000000657166083
2023-03-31 17:25:58,767 ===> Epoch[23/1000]
2023-03-31 17:26:22,007 >>> Training Loss: 0.000001311896085 ### Testing Loss: 0.000000616292652
2023-03-31 17:26:22,007 ===> Epoch[24/1000]
2023-03-31 17:26:45,444 >>> Training Loss: 0.000001296032565 ### Testing Loss: 0.000000628048554
2023-03-31 17:26:45,444 ===> Epoch[25/1000]
2023-03-31 17:27:08,864 >>> Training Loss: 0.000001290679734 ### Testing Loss: 0.000000616630302
2023-03-31 17:27:08,864 ===> Epoch[26/1000]
2023-03-31 17:27:32,234 >>> Training Loss: 0.000001242131702 ### Testing Loss: 0.000000589185788
2023-03-31 17:27:32,234 ===> Epoch[27/1000]
2023-03-31 17:27:55,684 >>> Training Loss: 0.000001176105343 ### Testing Loss: 0.000000585250518
2023-03-31 17:27:55,684 ===> Epoch[28/1000]
2023-03-31 17:28:19,064 >>> Training Loss: 0.000001147304488 ### Testing Loss: 0.000000598556937
2023-03-31 17:28:19,064 ===> Epoch[29/1000]
2023-03-31 17:28:42,474 >>> Training Loss: 0.000001154772576 ### Testing Loss: 0.000000565295352
2023-03-31 17:28:42,474 ===> Epoch[30/1000]
2023-03-31 17:29:05,884 >>> Training Loss: 0.000001117847546 ### Testing Loss: 0.000000582777830
2023-03-31 17:29:05,884 ===> Epoch[31/1000]
2023-03-31 17:29:29,225 >>> Training Loss: 0.000001131194836 ### Testing Loss: 0.000000553430141
2023-03-31 17:29:29,225 ===> Epoch[32/1000]
2023-03-31 17:29:52,628 >>> Training Loss: 0.000001081265282 ### Testing Loss: 0.000000536101652
2023-03-31 17:29:52,628 ===> Epoch[33/1000]
2023-03-31 17:30:16,094 >>> Training Loss: 0.000001054980771 ### Testing Loss: 0.000000533985883
2023-03-31 17:30:16,094 ===> Epoch[34/1000]
2023-03-31 17:30:39,628 >>> Training Loss: 0.000001030237399 ### Testing Loss: 0.000000605158220
2023-03-31 17:30:39,628 ===> Epoch[35/1000]
2023-03-31 17:31:03,058 >>> Training Loss: 0.000001037861693 ### Testing Loss: 0.000000526679401
2023-03-31 17:31:03,058 ===> Epoch[36/1000]
2023-03-31 17:31:26,648 >>> Training Loss: 0.000000982463916 ### Testing Loss: 0.000000519069090
2023-03-31 17:31:26,648 ===> Epoch[37/1000]
2023-03-31 17:31:50,118 >>> Training Loss: 0.000000969431881 ### Testing Loss: 0.000000510799339
2023-03-31 17:31:50,118 ===> Epoch[38/1000]
2023-03-31 17:32:13,628 >>> Training Loss: 0.000000956138479 ### Testing Loss: 0.000000511978669
2023-03-31 17:32:13,628 ===> Epoch[39/1000]
2023-03-31 17:32:37,198 >>> Training Loss: 0.000000940515633 ### Testing Loss: 0.000000496753387
2023-03-31 17:32:37,198 ===> Epoch[40/1000]
2023-03-31 17:33:00,758 >>> Training Loss: 0.000000932875764 ### Testing Loss: 0.000000489080037
2023-03-31 17:33:00,758 ===> Epoch[41/1000]
2023-03-31 17:33:24,268 >>> Training Loss: 0.000000930781653 ### Testing Loss: 0.000000482036228
2023-03-31 17:33:24,268 ===> Epoch[42/1000]
2023-03-31 17:33:47,818 >>> Training Loss: 0.000000898939390 ### Testing Loss: 0.000000490350317
2023-03-31 17:33:47,818 ===> Epoch[43/1000]
2023-03-31 17:34:11,229 >>> Training Loss: 0.000000920012042 ### Testing Loss: 0.000000650852598
2023-03-31 17:34:11,229 ===> Epoch[44/1000]
2023-03-31 17:34:35,750 >>> Training Loss: 0.000000899933980 ### Testing Loss: 0.000000472616733
2023-03-31 17:34:35,750 ===> Epoch[45/1000]
2023-03-31 17:34:59,321 >>> Training Loss: 0.000000844670922 ### Testing Loss: 0.000000481435791
2023-03-31 17:34:59,321 ===> Epoch[46/1000]
2023-03-31 17:35:22,941 >>> Training Loss: 0.000000831358363 ### Testing Loss: 0.000000467334388
2023-03-31 17:35:22,941 ===> Epoch[47/1000]
2023-03-31 17:35:46,551 >>> Training Loss: 0.000000830689658 ### Testing Loss: 0.000000464220989
2023-03-31 17:35:46,551 ===> Epoch[48/1000]
2023-03-31 17:36:10,261 >>> Training Loss: 0.000000816042984 ### Testing Loss: 0.000000473915946
2023-03-31 17:36:10,261 ===> Epoch[49/1000]
2023-03-31 17:36:33,820 >>> Training Loss: 0.000000810147526 ### Testing Loss: 0.000000469309981
2023-03-31 17:36:33,820 ===> Epoch[50/1000]
2023-03-31 17:36:57,561 >>> Training Loss: 0.000000803706712 ### Testing Loss: 0.000000461997558
2023-03-31 17:36:57,561 ===> Epoch[51/1000]
2023-03-31 17:37:21,041 >>> Training Loss: 0.000000786665396 ### Testing Loss: 0.000000455735830
2023-03-31 17:37:21,041 ===> Epoch[52/1000]
2023-03-31 17:37:44,698 >>> Training Loss: 0.000000774909552 ### Testing Loss: 0.000000446344757
2023-03-31 17:37:44,698 ===> Epoch[53/1000]
2023-03-31 17:38:08,198 >>> Training Loss: 0.000000778914114 ### Testing Loss: 0.000000440639013
2023-03-31 17:38:08,198 ===> Epoch[54/1000]
2023-03-31 17:38:31,508 >>> Training Loss: 0.000000746774333 ### Testing Loss: 0.000000448250887
2023-03-31 17:38:31,508 ===> Epoch[55/1000]
2023-03-31 17:38:54,799 >>> Training Loss: 0.000000740770759 ### Testing Loss: 0.000000440788966
2023-03-31 17:38:54,799 ===> Epoch[56/1000]
2023-03-31 17:39:18,219 >>> Training Loss: 0.000000733856325 ### Testing Loss: 0.000000434342297
2023-03-31 17:39:18,219 ===> Epoch[57/1000]
2023-03-31 17:39:41,659 >>> Training Loss: 0.000000731360672 ### Testing Loss: 0.000000446648755
2023-03-31 17:39:41,659 ===> Epoch[58/1000]
2023-03-31 17:40:05,179 >>> Training Loss: 0.000000724768938 ### Testing Loss: 0.000000434141839
2023-03-31 17:40:05,179 ===> Epoch[59/1000]
2023-03-31 17:40:28,549 >>> Training Loss: 0.000000722516972 ### Testing Loss: 0.000000433362942
2023-03-31 17:40:28,549 ===> Epoch[60/1000]
2023-03-31 17:40:51,999 >>> Training Loss: 0.000000693148422 ### Testing Loss: 0.000000429601471
2023-03-31 17:40:51,999 ===> Epoch[61/1000]
2023-03-31 17:41:15,510 >>> Training Loss: 0.000000696539985 ### Testing Loss: 0.000000434138144
2023-03-31 17:41:15,510 ===> Epoch[62/1000]
2023-03-31 17:41:39,014 >>> Training Loss: 0.000000685347402 ### Testing Loss: 0.000000419878830
2023-03-31 17:41:39,014 ===> Epoch[63/1000]
2023-03-31 17:42:02,544 >>> Training Loss: 0.000000673485147 ### Testing Loss: 0.000000414857425
2023-03-31 17:42:02,544 ===> Epoch[64/1000]
2023-03-31 17:42:25,984 >>> Training Loss: 0.000000662440414 ### Testing Loss: 0.000000413338284
2023-03-31 17:42:25,984 ===> Epoch[65/1000]
2023-03-31 17:42:49,304 >>> Training Loss: 0.000000652306824 ### Testing Loss: 0.000000419332224
2023-03-31 17:42:49,304 ===> Epoch[66/1000]
2023-03-31 17:43:12,725 >>> Training Loss: 0.000000652811707 ### Testing Loss: 0.000000417937940
2023-03-31 17:43:12,725 ===> Epoch[67/1000]
2023-03-31 17:43:36,185 >>> Training Loss: 0.000000638953168 ### Testing Loss: 0.000000420532814
2023-03-31 17:43:36,185 ===> Epoch[68/1000]
2023-03-31 17:43:59,674 >>> Training Loss: 0.000000627123256 ### Testing Loss: 0.000000414273586
2023-03-31 17:43:59,674 ===> Epoch[69/1000]
2023-03-31 17:44:23,154 >>> Training Loss: 0.000000635739582 ### Testing Loss: 0.000000412698199
2023-03-31 17:44:23,154 ===> Epoch[70/1000]
2023-03-31 17:44:46,634 >>> Training Loss: 0.000000619998559 ### Testing Loss: 0.000000409502348
2023-03-31 17:44:46,634 ===> Epoch[71/1000]
2023-03-31 17:45:10,159 >>> Training Loss: 0.000000628981070 ### Testing Loss: 0.000000404129480
2023-03-31 17:45:10,159 ===> Epoch[72/1000]
2023-03-31 17:45:33,686 >>> Training Loss: 0.000000615801468 ### Testing Loss: 0.000000410438531
2023-03-31 17:45:33,686 ===> Epoch[73/1000]
2023-03-31 17:45:57,537 >>> Training Loss: 0.000000603249248 ### Testing Loss: 0.000000400037294
2023-03-31 17:45:57,537 ===> Epoch[74/1000]
2023-03-31 17:46:21,417 >>> Training Loss: 0.000000588728881 ### Testing Loss: 0.000000396506152
2023-03-31 17:46:21,417 ===> Epoch[75/1000]
2023-03-31 17:46:45,077 >>> Training Loss: 0.000000584553163 ### Testing Loss: 0.000000399372880
2023-03-31 17:46:45,077 ===> Epoch[76/1000]
2023-03-31 17:47:08,837 >>> Training Loss: 0.000000587740431 ### Testing Loss: 0.000000399723206
2023-03-31 17:47:08,837 ===> Epoch[77/1000]
2023-03-31 17:47:32,567 >>> Training Loss: 0.000000578661002 ### Testing Loss: 0.000000396104213
2023-03-31 17:47:32,567 ===> Epoch[78/1000]
2023-03-31 17:47:56,297 >>> Training Loss: 0.000000579072093 ### Testing Loss: 0.000000398859953
2023-03-31 17:47:56,297 ===> Epoch[79/1000]
2023-03-31 17:48:20,057 >>> Training Loss: 0.000000572578529 ### Testing Loss: 0.000000394191716
2023-03-31 17:48:20,057 ===> Epoch[80/1000]
2023-03-31 17:48:43,867 >>> Training Loss: 0.000000561297497 ### Testing Loss: 0.000000383522888
2023-03-31 17:48:43,867 ===> Epoch[81/1000]
2023-03-31 17:49:07,643 >>> Training Loss: 0.000000557427484 ### Testing Loss: 0.000000402226988
2023-03-31 17:49:07,643 ===> Epoch[82/1000]
2023-03-31 17:49:32,256 >>> Training Loss: 0.000000545431305 ### Testing Loss: 0.000000396845621
2023-03-31 17:49:32,256 ===> Epoch[83/1000]
2023-03-31 17:49:56,026 >>> Training Loss: 0.000000535518325 ### Testing Loss: 0.000000386666414
2023-03-31 17:49:56,026 ===> Epoch[84/1000]
2023-03-31 17:50:19,736 >>> Training Loss: 0.000000532390573 ### Testing Loss: 0.000000382973411
2023-03-31 17:50:19,736 ===> Epoch[85/1000]
2023-03-31 17:50:43,466 >>> Training Loss: 0.000000538450138 ### Testing Loss: 0.000000394888957
2023-03-31 17:50:43,466 ===> Epoch[86/1000]
2023-03-31 17:51:07,226 >>> Training Loss: 0.000000522838377 ### Testing Loss: 0.000000401888514
2023-03-31 17:51:07,226 ===> Epoch[87/1000]
2023-03-31 17:51:30,946 >>> Training Loss: 0.000000614369355 ### Testing Loss: 0.000000386501966
2023-03-31 17:51:30,946 ===> Epoch[88/1000]
2023-03-31 17:51:54,776 >>> Training Loss: 0.000000518717627 ### Testing Loss: 0.000000380968714
2023-03-31 17:51:54,786 ===> Epoch[89/1000]
2023-03-31 17:52:18,466 >>> Training Loss: 0.000000514121552 ### Testing Loss: 0.000000384119517
2023-03-31 17:52:18,466 ===> Epoch[90/1000]
2023-03-31 17:52:42,237 >>> Training Loss: 0.000000500516364 ### Testing Loss: 0.000000378790503
2023-03-31 17:52:42,237 ===> Epoch[91/1000]
2023-03-31 17:53:05,985 >>> Training Loss: 0.000000494830203 ### Testing Loss: 0.000000376366302
2023-03-31 17:53:05,985 ===> Epoch[92/1000]
2023-03-31 17:53:29,725 >>> Training Loss: 0.000000495535915 ### Testing Loss: 0.000000376165218
2023-03-31 17:53:29,725 ===> Epoch[93/1000]
2023-03-31 17:53:53,435 >>> Training Loss: 0.000000489572983 ### Testing Loss: 0.000000369580135
2023-03-31 17:53:53,435 ===> Epoch[94/1000]
2023-03-31 17:54:17,274 >>> Training Loss: 0.000000488651324 ### Testing Loss: 0.000000374932569
2023-03-31 17:54:17,274 ===> Epoch[95/1000]
2023-03-31 17:54:41,044 >>> Training Loss: 0.000000487225577 ### Testing Loss: 0.000000371168454
2023-03-31 17:54:41,044 ===> Epoch[96/1000]
2023-03-31 17:55:04,774 >>> Training Loss: 0.000000475094595 ### Testing Loss: 0.000000373776658
2023-03-31 17:55:04,774 ===> Epoch[97/1000]
2023-03-31 17:55:28,484 >>> Training Loss: 0.000000476327330 ### Testing Loss: 0.000000375083346
2023-03-31 17:55:28,484 ===> Epoch[98/1000]
2023-03-31 17:55:52,214 >>> Training Loss: 0.000000476995069 ### Testing Loss: 0.000000374733673
2023-03-31 17:55:52,214 ===> Epoch[99/1000]
2023-03-31 17:56:15,814 >>> Training Loss: 0.000000488682929 ### Testing Loss: 0.000000367814096
2023-03-31 17:56:15,814 ===> Epoch[100/1000]
2023-03-31 17:56:39,220 >>> Training Loss: 0.000000460386474 ### Testing Loss: 0.000000399258880
2023-03-31 17:56:39,250 ===> Epoch[101/1000]
2023-03-31 17:57:03,256 >>> Training Loss: 0.000000459938747 ### Testing Loss: 0.000000370410561
2023-03-31 17:57:03,256 ===> Epoch[102/1000]
2023-03-31 17:57:26,966 >>> Training Loss: 0.000000453995995 ### Testing Loss: 0.000000367963906
2023-03-31 17:57:26,966 ===> Epoch[103/1000]
2023-03-31 17:57:50,712 >>> Training Loss: 0.000000461455102 ### Testing Loss: 0.000000367220252
2023-03-31 17:57:50,712 ===> Epoch[104/1000]
2023-03-31 17:58:14,602 >>> Training Loss: 0.000000453120634 ### Testing Loss: 0.000000359545339
2023-03-31 17:58:14,602 ===> Epoch[105/1000]
2023-03-31 17:58:38,442 >>> Training Loss: 0.000000443568752 ### Testing Loss: 0.000000361703144
2023-03-31 17:58:38,442 ===> Epoch[106/1000]
2023-03-31 17:59:02,212 >>> Training Loss: 0.000000449508207 ### Testing Loss: 0.000000358547226
2023-03-31 17:59:02,212 ===> Epoch[107/1000]
2023-03-31 17:59:26,052 >>> Training Loss: 0.000000433204860 ### Testing Loss: 0.000000358485295
2023-03-31 17:59:26,052 ===> Epoch[108/1000]
2023-03-31 17:59:49,973 >>> Training Loss: 0.000000443265918 ### Testing Loss: 0.000000355886357
2023-03-31 17:59:49,973 ===> Epoch[109/1000]
2023-03-31 18:00:13,753 >>> Training Loss: 0.000000454997377 ### Testing Loss: 0.000000363222966
2023-03-31 18:00:13,753 ===> Epoch[110/1000]
2023-03-31 18:00:37,493 >>> Training Loss: 0.000000435761450 ### Testing Loss: 0.000000359484829
2023-03-31 18:00:37,493 ===> Epoch[111/1000]
2023-03-31 18:01:01,233 >>> Training Loss: 0.000000430985324 ### Testing Loss: 0.000000357638186
2023-03-31 18:01:01,233 ===> Epoch[112/1000]
2023-03-31 18:01:25,033 >>> Training Loss: 0.000000431185867 ### Testing Loss: 0.000000353629304
2023-03-31 18:01:25,033 ===> Epoch[113/1000]
2023-03-31 18:01:48,887 >>> Training Loss: 0.000000440252506 ### Testing Loss: 0.000000355105414
2023-03-31 18:01:48,887 ===> Epoch[114/1000]
2023-03-31 18:02:12,637 >>> Training Loss: 0.000000427924789 ### Testing Loss: 0.000000357480644
2023-03-31 18:02:12,637 ===> Epoch[115/1000]
2023-03-31 18:02:36,317 >>> Training Loss: 0.000000423870034 ### Testing Loss: 0.000000359947478
2023-03-31 18:02:36,317 ===> Epoch[116/1000]
2023-03-31 18:03:00,077 >>> Training Loss: 0.000000406436442 ### Testing Loss: 0.000000350553620
2023-03-31 18:03:00,077 ===> Epoch[117/1000]
2023-03-31 18:03:23,837 >>> Training Loss: 0.000000407590818 ### Testing Loss: 0.000000354848112
2023-03-31 18:03:23,837 ===> Epoch[118/1000]
2023-03-31 18:03:47,677 >>> Training Loss: 0.000000401206336 ### Testing Loss: 0.000000350131927
2023-03-31 18:03:47,677 ===> Epoch[119/1000]
2023-03-31 18:04:11,527 >>> Training Loss: 0.000000420177770 ### Testing Loss: 0.000000354577054
2023-03-31 18:04:11,527 ===> Epoch[120/1000]
2023-03-31 18:04:36,221 >>> Training Loss: 0.000000396991254 ### Testing Loss: 0.000000345582947
2023-03-31 18:04:36,221 ===> Epoch[121/1000]
2023-03-31 18:05:00,107 >>> Training Loss: 0.000000397917177 ### Testing Loss: 0.000000349701850
2023-03-31 18:05:00,107 ===> Epoch[122/1000]
2023-03-31 18:05:23,857 >>> Training Loss: 0.000000393646246 ### Testing Loss: 0.000000357870306
2023-03-31 18:05:23,857 ===> Epoch[123/1000]
2023-03-31 18:05:47,757 >>> Training Loss: 0.000000403451679 ### Testing Loss: 0.000000351714050
2023-03-31 18:05:47,757 ===> Epoch[124/1000]
2023-03-31 18:06:11,617 >>> Training Loss: 0.000000387709946 ### Testing Loss: 0.000000345914572
2023-03-31 18:06:11,617 ===> Epoch[125/1000]
2023-03-31 18:06:35,467 >>> Training Loss: 0.000000388417305 ### Testing Loss: 0.000000344438348
2023-03-31 18:06:35,467 ===> Epoch[126/1000]
2023-03-31 18:06:59,257 >>> Training Loss: 0.000000381473029 ### Testing Loss: 0.000000345846018
2023-03-31 18:06:59,257 ===> Epoch[127/1000]
2023-03-31 18:07:23,037 >>> Training Loss: 0.000000388550802 ### Testing Loss: 0.000000347254456
2023-03-31 18:07:23,037 ===> Epoch[128/1000]
2023-03-31 18:07:46,957 >>> Training Loss: 0.000000383746965 ### Testing Loss: 0.000000346699466
2023-03-31 18:07:46,957 ===> Epoch[129/1000]
2023-03-31 18:08:10,737 >>> Training Loss: 0.000000381682440 ### Testing Loss: 0.000000350183171
2023-03-31 18:08:10,737 ===> Epoch[130/1000]
2023-03-31 18:08:34,527 >>> Training Loss: 0.000000377384964 ### Testing Loss: 0.000000352781313
2023-03-31 18:08:34,527 ===> Epoch[131/1000]
2023-03-31 18:08:58,341 >>> Training Loss: 0.000000384105675 ### Testing Loss: 0.000000345318796
2023-03-31 18:08:58,341 ===> Epoch[132/1000]
2023-03-31 18:09:22,161 >>> Training Loss: 0.000000370547070 ### Testing Loss: 0.000000338110766
2023-03-31 18:09:22,161 ===> Epoch[133/1000]
2023-03-31 18:09:45,971 >>> Training Loss: 0.000000372657325 ### Testing Loss: 0.000000337646526
2023-03-31 18:09:45,971 ===> Epoch[134/1000]
2023-03-31 18:10:09,841 >>> Training Loss: 0.000000356576436 ### Testing Loss: 0.000000344100869
2023-03-31 18:10:09,841 ===> Epoch[135/1000]
2023-03-31 18:10:33,641 >>> Training Loss: 0.000000356354946 ### Testing Loss: 0.000000347428994
2023-03-31 18:10:33,641 ===> Epoch[136/1000]
2023-03-31 18:10:57,441 >>> Training Loss: 0.000000376753945 ### Testing Loss: 0.000000337607759
2023-03-31 18:10:57,441 ===> Epoch[137/1000]
2023-03-31 18:11:21,231 >>> Training Loss: 0.000000377832777 ### Testing Loss: 0.000000351380947
2023-03-31 18:11:21,231 ===> Epoch[138/1000]
2023-03-31 18:11:45,031 >>> Training Loss: 0.000000363694880 ### Testing Loss: 0.000000341186450
2023-03-31 18:11:45,031 ===> Epoch[139/1000]
2023-03-31 18:12:08,821 >>> Training Loss: 0.000000351026642 ### Testing Loss: 0.000000338861554
2023-03-31 18:12:08,821 ===> Epoch[140/1000]
2023-03-31 18:12:32,616 >>> Training Loss: 0.000000355008552 ### Testing Loss: 0.000000338261998
2023-03-31 18:12:32,616 ===> Epoch[141/1000]
2023-03-31 18:12:56,506 >>> Training Loss: 0.000000364120325 ### Testing Loss: 0.000000336491411
2023-03-31 18:12:56,506 ===> Epoch[142/1000]
2023-03-31 18:13:20,346 >>> Training Loss: 0.000000348324704 ### Testing Loss: 0.000000346717172
2023-03-31 18:13:20,346 ===> Epoch[143/1000]
2023-03-31 18:13:44,186 >>> Training Loss: 0.000000341857969 ### Testing Loss: 0.000000339618254
2023-03-31 18:13:44,186 ===> Epoch[144/1000]
2023-03-31 18:14:08,026 >>> Training Loss: 0.000000346171362 ### Testing Loss: 0.000000340315864
2023-03-31 18:14:08,026 ===> Epoch[145/1000]
2023-03-31 18:14:31,915 >>> Training Loss: 0.000000349235364 ### Testing Loss: 0.000000332295457
2023-03-31 18:14:31,925 ===> Epoch[146/1000]
2023-03-31 18:14:55,846 >>> Training Loss: 0.000000346048864 ### Testing Loss: 0.000000339568771
2023-03-31 18:14:55,846 ===> Epoch[147/1000]
2023-03-31 18:15:19,766 >>> Training Loss: 0.000000358805266 ### Testing Loss: 0.000000338668997
2023-03-31 18:15:19,766 ===> Epoch[148/1000]
2023-03-31 18:15:43,606 >>> Training Loss: 0.000000338419824 ### Testing Loss: 0.000000333446224
2023-03-31 18:15:43,606 ===> Epoch[149/1000]
2023-03-31 18:16:07,476 >>> Training Loss: 0.000000342219892 ### Testing Loss: 0.000000336991320
2023-03-31 18:16:07,476 ===> Epoch[150/1000]
2023-03-31 18:16:31,232 >>> Training Loss: 0.000000337102136 ### Testing Loss: 0.000000338936928
2023-03-31 18:16:31,232 ===> Epoch[151/1000]
2023-03-31 18:16:55,082 >>> Training Loss: 0.000000340054157 ### Testing Loss: 0.000000333568892
2023-03-31 18:16:55,082 ===> Epoch[152/1000]
2023-03-31 18:17:18,862 >>> Training Loss: 0.000000333958468 ### Testing Loss: 0.000000334098786
2023-03-31 18:17:18,862 ===> Epoch[153/1000]
2023-03-31 18:17:42,762 >>> Training Loss: 0.000000325621329 ### Testing Loss: 0.000000340974623
2023-03-31 18:17:42,762 ===> Epoch[154/1000]
2023-03-31 18:18:06,672 >>> Training Loss: 0.000000336472453 ### Testing Loss: 0.000000340844963
2023-03-31 18:18:06,672 ===> Epoch[155/1000]
2023-03-31 18:18:30,892 >>> Training Loss: 0.000000331740949 ### Testing Loss: 0.000000332904676
2023-03-31 18:18:30,892 ===> Epoch[156/1000]
2023-03-31 18:18:54,912 >>> Training Loss: 0.000000336815901 ### Testing Loss: 0.000000338458449
2023-03-31 18:18:54,912 ===> Epoch[157/1000]
2023-03-31 18:19:18,872 >>> Training Loss: 0.000000333412402 ### Testing Loss: 0.000000330792346
2023-03-31 18:19:18,872 ===> Epoch[158/1000]
2023-03-31 18:19:43,558 >>> Training Loss: 0.000000324269706 ### Testing Loss: 0.000000332567538
2023-03-31 18:19:43,558 ===> Epoch[159/1000]
2023-03-31 18:20:07,344 >>> Training Loss: 0.000000320211512 ### Testing Loss: 0.000000331332046
2023-03-31 18:20:07,344 ===> Epoch[160/1000]
2023-03-31 18:20:31,125 >>> Training Loss: 0.000000318199341 ### Testing Loss: 0.000000329405395
2023-03-31 18:20:31,125 ===> Epoch[161/1000]
2023-03-31 18:20:54,975 >>> Training Loss: 0.000000334289609 ### Testing Loss: 0.000000358907812
2023-03-31 18:20:54,975 ===> Epoch[162/1000]
2023-03-31 18:21:19,145 >>> Training Loss: 0.000000324715018 ### Testing Loss: 0.000000331685328
2023-03-31 18:21:19,145 ===> Epoch[163/1000]
2023-03-31 18:21:42,985 >>> Training Loss: 0.000000315713066 ### Testing Loss: 0.000000328843129
2023-03-31 18:21:42,985 ===> Epoch[164/1000]
2023-03-31 18:22:06,985 >>> Training Loss: 0.000000315082815 ### Testing Loss: 0.000000331938878
2023-03-31 18:22:06,985 ===> Epoch[165/1000]
2023-03-31 18:22:30,925 >>> Training Loss: 0.000000308196462 ### Testing Loss: 0.000000328056984
2023-03-31 18:22:30,925 ===> Epoch[166/1000]
2023-03-31 18:22:54,805 >>> Training Loss: 0.000000307500642 ### Testing Loss: 0.000000325454124
2023-03-31 18:22:54,805 ===> Epoch[167/1000]
2023-03-31 18:23:18,686 >>> Training Loss: 0.000000302807507 ### Testing Loss: 0.000000326817712
2023-03-31 18:23:18,686 ===> Epoch[168/1000]
2023-03-31 18:23:42,542 >>> Training Loss: 0.000000307001642 ### Testing Loss: 0.000000331107032
2023-03-31 18:23:42,542 ===> Epoch[169/1000]
2023-03-31 18:24:06,452 >>> Training Loss: 0.000000327117192 ### Testing Loss: 0.000000331791256
2023-03-31 18:24:06,452 ===> Epoch[170/1000]
2023-03-31 18:24:30,212 >>> Training Loss: 0.000000303367727 ### Testing Loss: 0.000000326503795
2023-03-31 18:24:30,212 ===> Epoch[171/1000]
2023-03-31 18:24:54,082 >>> Training Loss: 0.000000301831903 ### Testing Loss: 0.000000326497059
2023-03-31 18:24:54,082 ===> Epoch[172/1000]
2023-03-31 18:25:18,082 >>> Training Loss: 0.000000299263519 ### Testing Loss: 0.000000326172511
2023-03-31 18:25:18,082 ===> Epoch[173/1000]
2023-03-31 18:25:41,992 >>> Training Loss: 0.000000299321954 ### Testing Loss: 0.000000330411723
2023-03-31 18:25:41,992 ===> Epoch[174/1000]
2023-03-31 18:26:05,872 >>> Training Loss: 0.000000301581963 ### Testing Loss: 0.000000326960560
2023-03-31 18:26:05,872 ===> Epoch[175/1000]
2023-03-31 18:26:29,782 >>> Training Loss: 0.000000294208490 ### Testing Loss: 0.000000327190691
2023-03-31 18:26:29,792 ===> Epoch[176/1000]
2023-03-31 18:26:53,732 >>> Training Loss: 0.000000296696811 ### Testing Loss: 0.000000325937833
2023-03-31 18:26:53,732 ===> Epoch[177/1000]
2023-03-31 18:27:17,737 >>> Training Loss: 0.000000298508240 ### Testing Loss: 0.000000327425795
2023-03-31 18:27:17,737 ===> Epoch[178/1000]
2023-03-31 18:27:41,587 >>> Training Loss: 0.000000299312632 ### Testing Loss: 0.000000324563274
2023-03-31 18:27:41,587 ===> Epoch[179/1000]
2023-03-31 18:28:05,437 >>> Training Loss: 0.000000290921975 ### Testing Loss: 0.000000339872543
2023-03-31 18:28:05,437 ===> Epoch[180/1000]
2023-03-31 18:28:29,267 >>> Training Loss: 0.000000324088518 ### Testing Loss: 0.000000323800720
2023-03-31 18:28:29,267 ===> Epoch[181/1000]
2023-03-31 18:28:53,137 >>> Training Loss: 0.000000287788453 ### Testing Loss: 0.000000326236773
2023-03-31 18:28:53,137 ===> Epoch[182/1000]
2023-03-31 18:29:17,007 >>> Training Loss: 0.000000288520198 ### Testing Loss: 0.000000325304342
2023-03-31 18:29:17,007 ===> Epoch[183/1000]
2023-03-31 18:29:40,827 >>> Training Loss: 0.000000289344314 ### Testing Loss: 0.000000325171271
2023-03-31 18:29:40,827 ===> Epoch[184/1000]
2023-03-31 18:30:04,737 >>> Training Loss: 0.000000283760102 ### Testing Loss: 0.000000327498924
2023-03-31 18:30:04,747 ===> Epoch[185/1000]
2023-03-31 18:30:28,618 >>> Training Loss: 0.000000281034232 ### Testing Loss: 0.000000323981681
2023-03-31 18:30:28,618 ===> Epoch[186/1000]
2023-03-31 18:30:52,398 >>> Training Loss: 0.000000293405151 ### Testing Loss: 0.000000327640521
2023-03-31 18:30:52,398 ===> Epoch[187/1000]
2023-03-31 18:31:16,283 >>> Training Loss: 0.000000288012217 ### Testing Loss: 0.000000334468439
2023-03-31 18:31:16,283 ===> Epoch[188/1000]
2023-03-31 18:31:40,183 >>> Training Loss: 0.000000294104041 ### Testing Loss: 0.000000325652820
2023-03-31 18:31:40,183 ===> Epoch[189/1000]
2023-03-31 18:32:04,043 >>> Training Loss: 0.000000276659563 ### Testing Loss: 0.000000331549501
2023-03-31 18:32:04,043 ===> Epoch[190/1000]
2023-03-31 18:32:27,963 >>> Training Loss: 0.000000276287722 ### Testing Loss: 0.000000327798972
2023-03-31 18:32:27,963 ===> Epoch[191/1000]
2023-03-31 18:32:51,793 >>> Training Loss: 0.000000289234976 ### Testing Loss: 0.000000325218735
2023-03-31 18:32:51,793 ===> Epoch[192/1000]
2023-03-31 18:33:15,683 >>> Training Loss: 0.000000285811041 ### Testing Loss: 0.000000322543286
2023-03-31 18:33:15,683 ===> Epoch[193/1000]
2023-03-31 18:33:39,603 >>> Training Loss: 0.000000294467526 ### Testing Loss: 0.000000336637754
2023-03-31 18:33:39,603 ===> Epoch[194/1000]
2023-03-31 18:34:03,413 >>> Training Loss: 0.000000293373120 ### Testing Loss: 0.000000324798577
2023-03-31 18:34:03,413 ===> Epoch[195/1000]
2023-03-31 18:34:28,451 >>> Training Loss: 0.000000278427137 ### Testing Loss: 0.000000324090649
2023-03-31 18:34:28,451 ===> Epoch[196/1000]
2023-03-31 18:34:52,245 >>> Training Loss: 0.000000278480172 ### Testing Loss: 0.000000321414802
2023-03-31 18:34:52,245 ===> Epoch[197/1000]
2023-03-31 18:35:16,115 >>> Training Loss: 0.000000275122773 ### Testing Loss: 0.000000323571612
2023-03-31 18:35:16,115 ===> Epoch[198/1000]
2023-03-31 18:35:39,935 >>> Training Loss: 0.000000271396033 ### Testing Loss: 0.000000326470342
2023-03-31 18:35:39,935 ===> Epoch[199/1000]
2023-03-31 18:36:03,805 >>> Training Loss: 0.000000278497907 ### Testing Loss: 0.000000325487747
2023-03-31 18:36:03,805 ===> Epoch[200/1000]
2023-03-31 18:36:27,725 >>> Training Loss: 0.000000277053260 ### Testing Loss: 0.000000324726784
2023-03-31 18:36:27,745 ===> Epoch[201/1000]
2023-03-31 18:36:51,695 >>> Training Loss: 0.000000275527128 ### Testing Loss: 0.000000319875852
2023-03-31 18:36:51,695 ===> Epoch[202/1000]
2023-03-31 18:37:15,596 >>> Training Loss: 0.000000263443809 ### Testing Loss: 0.000000319591692
2023-03-31 18:37:15,596 ===> Epoch[203/1000]
2023-03-31 18:37:39,546 >>> Training Loss: 0.000000261606743 ### Testing Loss: 0.000000320043000
2023-03-31 18:37:39,546 ===> Epoch[204/1000]
2023-03-31 18:38:03,346 >>> Training Loss: 0.000000261349982 ### Testing Loss: 0.000000316732127
2023-03-31 18:38:03,346 ===> Epoch[205/1000]
2023-03-31 18:38:27,221 >>> Training Loss: 0.000000270187940 ### Testing Loss: 0.000000321112708
2023-03-31 18:38:27,221 ===> Epoch[206/1000]
2023-03-31 18:38:51,171 >>> Training Loss: 0.000000270400108 ### Testing Loss: 0.000000324318989
2023-03-31 18:38:51,171 ===> Epoch[207/1000]
2023-03-31 18:39:15,031 >>> Training Loss: 0.000000269112718 ### Testing Loss: 0.000000332488668
2023-03-31 18:39:15,031 ===> Epoch[208/1000]
2023-03-31 18:39:38,891 >>> Training Loss: 0.000000270191606 ### Testing Loss: 0.000000319839984
2023-03-31 18:39:38,891 ===> Epoch[209/1000]
2023-03-31 18:40:02,701 >>> Training Loss: 0.000000270554040 ### Testing Loss: 0.000000325249744
2023-03-31 18:40:02,701 ===> Epoch[210/1000]
2023-03-31 18:40:26,570 >>> Training Loss: 0.000000271069212 ### Testing Loss: 0.000000316707400
2023-03-31 18:40:26,570 ===> Epoch[211/1000]
2023-03-31 18:40:50,731 >>> Training Loss: 0.000000271326400 ### Testing Loss: 0.000000319224853
2023-03-31 18:40:50,731 ===> Epoch[212/1000]
2023-03-31 18:41:14,651 >>> Training Loss: 0.000000263479393 ### Testing Loss: 0.000000320360243
2023-03-31 18:41:14,651 ===> Epoch[213/1000]
2023-03-31 18:41:38,531 >>> Training Loss: 0.000000271877411 ### Testing Loss: 0.000000322239060
2023-03-31 18:41:38,531 ===> Epoch[214/1000]
2023-03-31 18:42:02,346 >>> Training Loss: 0.000000263482320 ### Testing Loss: 0.000000328462448
2023-03-31 18:42:02,346 ===> Epoch[215/1000]
2023-03-31 18:42:26,175 >>> Training Loss: 0.000000258980464 ### Testing Loss: 0.000000321244272
2023-03-31 18:42:26,175 ===> Epoch[216/1000]
2023-03-31 18:42:50,025 >>> Training Loss: 0.000000262379245 ### Testing Loss: 0.000000318339147
2023-03-31 18:42:50,025 ===> Epoch[217/1000]
2023-03-31 18:43:13,825 >>> Training Loss: 0.000000263237979 ### Testing Loss: 0.000000319479369
2023-03-31 18:43:13,825 ===> Epoch[218/1000]
2023-03-31 18:43:37,704 >>> Training Loss: 0.000000256928331 ### Testing Loss: 0.000000319250006
2023-03-31 18:43:37,704 ===> Epoch[219/1000]
2023-03-31 18:44:01,604 >>> Training Loss: 0.000000254653344 ### Testing Loss: 0.000000316857864
2023-03-31 18:44:01,604 ===> Epoch[220/1000]
2023-03-31 18:44:25,414 >>> Training Loss: 0.000000267227620 ### Testing Loss: 0.000000316060834
2023-03-31 18:44:25,414 ===> Epoch[221/1000]
2023-03-31 18:44:49,264 >>> Training Loss: 0.000000257622474 ### Testing Loss: 0.000000331796457
2023-03-31 18:44:49,264 ===> Epoch[222/1000]
2023-03-31 18:45:13,075 >>> Training Loss: 0.000000261790888 ### Testing Loss: 0.000000321294806
2023-03-31 18:45:13,075 ===> Epoch[223/1000]
2023-03-31 18:45:36,865 >>> Training Loss: 0.000000270010133 ### Testing Loss: 0.000000316225481
2023-03-31 18:45:36,865 ===> Epoch[224/1000]
2023-03-31 18:46:00,849 >>> Training Loss: 0.000000253216655 ### Testing Loss: 0.000000316886457
2023-03-31 18:46:00,849 ===> Epoch[225/1000]
2023-03-31 18:46:24,759 >>> Training Loss: 0.000000251169723 ### Testing Loss: 0.000000317611722
2023-03-31 18:46:24,759 ===> Epoch[226/1000]
2023-03-31 18:46:48,589 >>> Training Loss: 0.000000246228609 ### Testing Loss: 0.000000319375175
2023-03-31 18:46:48,589 ===> Epoch[227/1000]
2023-03-31 18:47:12,519 >>> Training Loss: 0.000000245352425 ### Testing Loss: 0.000000315728670
2023-03-31 18:47:12,519 ===> Epoch[228/1000]
2023-03-31 18:47:36,369 >>> Training Loss: 0.000000242010771 ### Testing Loss: 0.000000317081401
2023-03-31 18:47:36,369 ===> Epoch[229/1000]
2023-03-31 18:48:00,279 >>> Training Loss: 0.000000244785213 ### Testing Loss: 0.000000317181701
2023-03-31 18:48:00,279 ===> Epoch[230/1000]
2023-03-31 18:48:24,219 >>> Training Loss: 0.000000253454232 ### Testing Loss: 0.000000315711986
2023-03-31 18:48:24,219 ===> Epoch[231/1000]
2023-03-31 18:48:48,070 >>> Training Loss: 0.000000245987337 ### Testing Loss: 0.000000314676498
2023-03-31 18:48:48,070 ===> Epoch[232/1000]
2023-03-31 18:49:11,876 >>> Training Loss: 0.000000252184293 ### Testing Loss: 0.000000321976870
2023-03-31 18:49:11,876 ===> Epoch[233/1000]
2023-03-31 18:49:36,796 >>> Training Loss: 0.000000254886316 ### Testing Loss: 0.000000322926411
2023-03-31 18:49:36,796 ===> Epoch[234/1000]
2023-03-31 18:50:00,646 >>> Training Loss: 0.000000252800845 ### Testing Loss: 0.000000328949199
2023-03-31 18:50:00,646 ===> Epoch[235/1000]
2023-03-31 18:50:24,526 >>> Training Loss: 0.000000252340129 ### Testing Loss: 0.000000316082918
2023-03-31 18:50:24,526 ===> Epoch[236/1000]
2023-03-31 18:50:48,306 >>> Training Loss: 0.000000245411997 ### Testing Loss: 0.000000316180319
2023-03-31 18:50:48,306 ===> Epoch[237/1000]
2023-03-31 18:51:12,166 >>> Training Loss: 0.000000243403775 ### Testing Loss: 0.000000313918775
2023-03-31 18:51:12,166 ===> Epoch[238/1000]
2023-03-31 18:51:36,026 >>> Training Loss: 0.000000247462964 ### Testing Loss: 0.000000318241973
2023-03-31 18:51:36,026 ===> Epoch[239/1000]
2023-03-31 18:51:59,856 >>> Training Loss: 0.000000244077881 ### Testing Loss: 0.000000315697179
2023-03-31 18:51:59,856 ===> Epoch[240/1000]
2023-03-31 18:52:23,866 >>> Training Loss: 0.000000240652383 ### Testing Loss: 0.000000322132678
2023-03-31 18:52:23,866 ===> Epoch[241/1000]
2023-03-31 18:52:47,726 >>> Training Loss: 0.000000236587539 ### Testing Loss: 0.000000319090645
2023-03-31 18:52:47,726 ===> Epoch[242/1000]
2023-03-31 18:53:11,571 >>> Training Loss: 0.000000261172744 ### Testing Loss: 0.000000317083021
2023-03-31 18:53:11,571 ===> Epoch[243/1000]
2023-03-31 18:53:35,431 >>> Training Loss: 0.000000250964973 ### Testing Loss: 0.000000316660902
2023-03-31 18:53:35,431 ===> Epoch[244/1000]
2023-03-31 18:53:59,371 >>> Training Loss: 0.000000233207672 ### Testing Loss: 0.000000314581769
2023-03-31 18:53:59,371 ===> Epoch[245/1000]
2023-03-31 18:54:23,251 >>> Training Loss: 0.000000233838534 ### Testing Loss: 0.000000312418194
2023-03-31 18:54:23,251 ===> Epoch[246/1000]
2023-03-31 18:54:47,071 >>> Training Loss: 0.000000236843604 ### Testing Loss: 0.000000318054759
2023-03-31 18:54:47,071 ===> Epoch[247/1000]
2023-03-31 18:55:10,961 >>> Training Loss: 0.000000234214397 ### Testing Loss: 0.000000316577825
2023-03-31 18:55:10,961 ===> Epoch[248/1000]
2023-03-31 18:55:34,821 >>> Training Loss: 0.000000238984597 ### Testing Loss: 0.000000315363479
2023-03-31 18:55:34,821 ===> Epoch[249/1000]
2023-03-31 18:55:58,640 >>> Training Loss: 0.000000237886269 ### Testing Loss: 0.000000327676304
2023-03-31 18:55:58,640 ===> Epoch[250/1000]
2023-03-31 18:56:22,441 >>> Training Loss: 0.000000246039889 ### Testing Loss: 0.000000315400058
2023-03-31 18:56:22,441 ===> Epoch[251/1000]
2023-03-31 18:56:46,244 >>> Training Loss: 0.000000234230782 ### Testing Loss: 0.000000314783023
2023-03-31 18:56:46,244 ===> Epoch[252/1000]
2023-03-31 18:57:10,054 >>> Training Loss: 0.000000238747219 ### Testing Loss: 0.000000313380156
2023-03-31 18:57:10,054 ===> Epoch[253/1000]
2023-03-31 18:57:33,863 >>> Training Loss: 0.000000232011260 ### Testing Loss: 0.000000313965131
2023-03-31 18:57:33,863 ===> Epoch[254/1000]
2023-03-31 18:57:57,693 >>> Training Loss: 0.000000234084510 ### Testing Loss: 0.000000314480758
2023-03-31 18:57:57,693 ===> Epoch[255/1000]
2023-03-31 18:58:21,573 >>> Training Loss: 0.000000233237870 ### Testing Loss: 0.000000315514058
2023-03-31 18:58:21,573 ===> Epoch[256/1000]
2023-03-31 18:58:45,393 >>> Training Loss: 0.000000234578707 ### Testing Loss: 0.000000318367000
2023-03-31 18:58:45,393 ===> Epoch[257/1000]
2023-03-31 18:59:09,163 >>> Training Loss: 0.000000232590494 ### Testing Loss: 0.000000313183108
2023-03-31 18:59:09,163 ===> Epoch[258/1000]
2023-03-31 18:59:33,034 >>> Training Loss: 0.000000234154129 ### Testing Loss: 0.000000316892169
2023-03-31 18:59:33,034 ===> Epoch[259/1000]
2023-03-31 18:59:56,873 >>> Training Loss: 0.000000233991557 ### Testing Loss: 0.000000324882620
2023-03-31 18:59:56,873 ===> Epoch[260/1000]
2023-03-31 19:00:20,758 >>> Training Loss: 0.000000236676598 ### Testing Loss: 0.000000319291047
2023-03-31 19:00:20,758 ===> Epoch[261/1000]
2023-03-31 19:00:45,870 >>> Training Loss: 0.000000247745874 ### Testing Loss: 0.000000320070342
2023-03-31 19:00:45,870 ===> Epoch[262/1000]
2023-03-31 19:01:10,433 >>> Training Loss: 0.000000226445366 ### Testing Loss: 0.000000311392824
2023-03-31 19:01:10,433 ===> Epoch[263/1000]
2023-03-31 19:01:35,163 >>> Training Loss: 0.000000224139768 ### Testing Loss: 0.000000314924563
2023-03-31 19:01:35,163 ===> Epoch[264/1000]
2023-03-31 19:01:59,797 >>> Training Loss: 0.000000221849717 ### Testing Loss: 0.000000312955081
2023-03-31 19:01:59,797 ===> Epoch[265/1000]
2023-03-31 19:02:24,165 >>> Training Loss: 0.000000224382589 ### Testing Loss: 0.000000314482321
2023-03-31 19:02:24,165 ===> Epoch[266/1000]
2023-03-31 19:02:48,482 >>> Training Loss: 0.000000233351955 ### Testing Loss: 0.000000317191279
2023-03-31 19:02:48,482 ===> Epoch[267/1000]
2023-03-31 19:03:12,843 >>> Training Loss: 0.000000235857570 ### Testing Loss: 0.000000320758744
2023-03-31 19:03:12,843 ===> Epoch[268/1000]
2023-03-31 19:03:37,168 >>> Training Loss: 0.000000232157930 ### Testing Loss: 0.000000317320399
2023-03-31 19:03:37,168 ===> Epoch[269/1000]
2023-03-31 19:04:01,497 >>> Training Loss: 0.000000237206379 ### Testing Loss: 0.000000318499389
2023-03-31 19:04:01,497 ===> Epoch[270/1000]
2023-03-31 19:04:26,954 >>> Training Loss: 0.000000227169110 ### Testing Loss: 0.000000312538475
2023-03-31 19:04:26,954 ===> Epoch[271/1000]
2023-03-31 19:04:51,209 >>> Training Loss: 0.000000225663911 ### Testing Loss: 0.000000312308828
2023-03-31 19:04:51,209 ===> Epoch[272/1000]
2023-03-31 19:05:15,581 >>> Training Loss: 0.000000221989325 ### Testing Loss: 0.000000313171483
2023-03-31 19:05:15,581 ===> Epoch[273/1000]
2023-03-31 19:05:39,818 >>> Training Loss: 0.000000224517521 ### Testing Loss: 0.000000312101889
2023-03-31 19:05:39,818 ===> Epoch[274/1000]
2023-03-31 19:06:04,180 >>> Training Loss: 0.000000228763071 ### Testing Loss: 0.000000313409657
2023-03-31 19:06:04,180 ===> Epoch[275/1000]
2023-03-31 19:06:28,505 >>> Training Loss: 0.000000226143101 ### Testing Loss: 0.000000312852023
2023-03-31 19:06:28,506 ===> Epoch[276/1000]
2023-03-31 19:06:52,880 >>> Training Loss: 0.000000225686051 ### Testing Loss: 0.000000310689728
2023-03-31 19:06:52,881 ===> Epoch[277/1000]
2023-03-31 19:07:17,239 >>> Training Loss: 0.000000218369692 ### Testing Loss: 0.000000310286111
2023-03-31 19:07:17,239 ===> Epoch[278/1000]
2023-03-31 19:07:41,576 >>> Training Loss: 0.000000216264851 ### Testing Loss: 0.000000313808044
2023-03-31 19:07:41,577 ===> Epoch[279/1000]
2023-03-31 19:08:05,907 >>> Training Loss: 0.000000229013168 ### Testing Loss: 0.000000314063954
2023-03-31 19:08:05,908 ===> Epoch[280/1000]
2023-03-31 19:08:30,308 >>> Training Loss: 0.000000222275929 ### Testing Loss: 0.000000314527540
2023-03-31 19:08:30,309 ===> Epoch[281/1000]
2023-03-31 19:08:54,648 >>> Training Loss: 0.000000221605205 ### Testing Loss: 0.000000312916512
2023-03-31 19:08:54,648 ===> Epoch[282/1000]
2023-03-31 19:09:18,994 >>> Training Loss: 0.000000221812456 ### Testing Loss: 0.000000319501964
2023-03-31 19:09:18,994 ===> Epoch[283/1000]
2023-03-31 19:09:43,370 >>> Training Loss: 0.000000230990352 ### Testing Loss: 0.000000312944792
2023-03-31 19:09:43,370 ===> Epoch[284/1000]
2023-03-31 19:10:07,686 >>> Training Loss: 0.000000222930538 ### Testing Loss: 0.000000312432491
2023-03-31 19:10:07,686 ===> Epoch[285/1000]
2023-03-31 19:10:32,064 >>> Training Loss: 0.000000230034672 ### Testing Loss: 0.000000313733210
2023-03-31 19:10:32,064 ===> Epoch[286/1000]
2023-03-31 19:10:56,379 >>> Training Loss: 0.000000221170836 ### Testing Loss: 0.000000317510967
2023-03-31 19:10:56,380 ===> Epoch[287/1000]
2023-03-31 19:11:20,720 >>> Training Loss: 0.000000219161265 ### Testing Loss: 0.000000315261275
2023-03-31 19:11:20,720 ===> Epoch[288/1000]
2023-03-31 19:11:45,071 >>> Training Loss: 0.000000216711825 ### Testing Loss: 0.000000313598548
2023-03-31 19:11:45,071 ===> Epoch[289/1000]
2023-03-31 19:12:09,405 >>> Training Loss: 0.000000220733199 ### Testing Loss: 0.000000313573366
2023-03-31 19:12:09,405 ===> Epoch[290/1000]
2023-03-31 19:12:33,775 >>> Training Loss: 0.000000216828653 ### Testing Loss: 0.000000326014600
2023-03-31 19:12:33,775 ===> Epoch[291/1000]
2023-03-31 19:12:58,162 >>> Training Loss: 0.000000224650307 ### Testing Loss: 0.000000309853249
2023-03-31 19:12:58,162 ===> Epoch[292/1000]
2023-03-31 19:13:22,522 >>> Training Loss: 0.000000214514174 ### Testing Loss: 0.000000312757066
2023-03-31 19:13:22,522 ===> Epoch[293/1000]
2023-03-31 19:13:46,829 >>> Training Loss: 0.000000215640128 ### Testing Loss: 0.000000313667158
2023-03-31 19:13:46,829 ===> Epoch[294/1000]
2023-03-31 19:14:11,213 >>> Training Loss: 0.000000217930378 ### Testing Loss: 0.000000324107134
2023-03-31 19:14:11,213 ===> Epoch[295/1000]
2023-03-31 19:14:35,535 >>> Training Loss: 0.000000216323542 ### Testing Loss: 0.000000314533679
2023-03-31 19:14:35,535 ===> Epoch[296/1000]
2023-03-31 19:14:59,144 >>> Training Loss: 0.000000214985775 ### Testing Loss: 0.000000312164730
2023-03-31 19:14:59,144 ===> Epoch[297/1000]
2023-03-31 19:15:22,674 >>> Training Loss: 0.000000214610225 ### Testing Loss: 0.000000311996899
2023-03-31 19:15:22,674 ===> Epoch[298/1000]
2023-03-31 19:15:46,214 >>> Training Loss: 0.000000215480270 ### Testing Loss: 0.000000313584792
2023-03-31 19:15:46,224 ===> Epoch[299/1000]
2023-03-31 19:16:09,678 >>> Training Loss: 0.000000211358085 ### Testing Loss: 0.000000311477862
2023-03-31 19:16:09,678 ===> Epoch[300/1000]
2023-03-31 19:16:33,252 >>> Training Loss: 0.000000219059018 ### Testing Loss: 0.000000312998452
2023-03-31 19:16:33,272 ===> Epoch[301/1000]
2023-03-31 19:16:56,892 >>> Training Loss: 0.000000210967769 ### Testing Loss: 0.000000315925348
2023-03-31 19:16:56,892 ===> Epoch[302/1000]
2023-03-31 19:17:20,512 >>> Training Loss: 0.000000212287389 ### Testing Loss: 0.000000316762680
2023-03-31 19:17:20,512 ===> Epoch[303/1000]
2023-03-31 19:17:44,103 >>> Training Loss: 0.000000210779518 ### Testing Loss: 0.000000313107762
2023-03-31 19:17:44,103 ===> Epoch[304/1000]
2023-03-31 19:18:07,703 >>> Training Loss: 0.000000213079517 ### Testing Loss: 0.000000311742696
2023-03-31 19:18:07,703 ===> Epoch[305/1000]
2023-03-31 19:18:31,773 >>> Training Loss: 0.000000209818126 ### Testing Loss: 0.000000310403806
2023-03-31 19:18:31,773 ===> Epoch[306/1000]
2023-03-31 19:18:55,883 >>> Training Loss: 0.000000220126793 ### Testing Loss: 0.000000316979566
2023-03-31 19:18:55,883 ===> Epoch[307/1000]
2023-03-31 19:19:20,074 >>> Training Loss: 0.000000213139998 ### Testing Loss: 0.000000313421253
2023-03-31 19:19:20,074 ===> Epoch[308/1000]
2023-03-31 19:19:44,850 >>> Training Loss: 0.000000209888228 ### Testing Loss: 0.000000310312629
2023-03-31 19:19:44,850 ===> Epoch[309/1000]
2023-03-31 19:20:08,840 >>> Training Loss: 0.000000229274349 ### Testing Loss: 0.000000310797617
2023-03-31 19:20:08,840 ===> Epoch[310/1000]
2023-03-31 19:20:32,720 >>> Training Loss: 0.000000211833907 ### Testing Loss: 0.000000313404939
2023-03-31 19:20:32,720 ===> Epoch[311/1000]
2023-03-31 19:20:56,311 >>> Training Loss: 0.000000205779799 ### Testing Loss: 0.000000312270686
2023-03-31 19:20:56,311 ===> Epoch[312/1000]
2023-03-31 19:21:19,891 >>> Training Loss: 0.000000203848103 ### Testing Loss: 0.000000309007589
2023-03-31 19:21:19,891 ===> Epoch[313/1000]
2023-03-31 19:21:43,431 >>> Training Loss: 0.000000208173986 ### Testing Loss: 0.000000309703154
2023-03-31 19:21:43,431 ===> Epoch[314/1000]
2023-03-31 19:22:06,991 >>> Training Loss: 0.000000209873406 ### Testing Loss: 0.000000310848151
2023-03-31 19:22:06,991 ===> Epoch[315/1000]
2023-03-31 19:22:30,581 >>> Training Loss: 0.000000210276681 ### Testing Loss: 0.000000313256862
2023-03-31 19:22:30,581 ===> Epoch[316/1000]
2023-03-31 19:22:54,051 >>> Training Loss: 0.000000203633604 ### Testing Loss: 0.000000312695704
2023-03-31 19:22:54,051 ===> Epoch[317/1000]
2023-03-31 19:23:17,608 >>> Training Loss: 0.000000199919057 ### Testing Loss: 0.000000312099701
2023-03-31 19:23:17,608 ===> Epoch[318/1000]
2023-03-31 19:23:41,038 >>> Training Loss: 0.000000199359860 ### Testing Loss: 0.000000312660205
2023-03-31 19:23:41,038 ===> Epoch[319/1000]
2023-03-31 19:24:04,688 >>> Training Loss: 0.000000219999976 ### Testing Loss: 0.000000318083238
2023-03-31 19:24:04,688 ===> Epoch[320/1000]
2023-03-31 19:24:28,278 >>> Training Loss: 0.000000201405953 ### Testing Loss: 0.000000311017885
2023-03-31 19:24:28,288 ===> Epoch[321/1000]
2023-03-31 19:24:51,828 >>> Training Loss: 0.000000201799082 ### Testing Loss: 0.000000312519944
2023-03-31 19:24:51,828 ===> Epoch[322/1000]
2023-03-31 19:25:15,288 >>> Training Loss: 0.000000217575860 ### Testing Loss: 0.000000383379330
2023-03-31 19:25:15,288 ===> Epoch[323/1000]
2023-03-31 19:25:38,778 >>> Training Loss: 0.000000217623153 ### Testing Loss: 0.000000310149687
2023-03-31 19:25:38,778 ===> Epoch[324/1000]
2023-03-31 19:26:02,278 >>> Training Loss: 0.000000205944929 ### Testing Loss: 0.000000312791542
2023-03-31 19:26:02,278 ===> Epoch[325/1000]
2023-03-31 19:26:25,778 >>> Training Loss: 0.000000203092966 ### Testing Loss: 0.000000312451334
2023-03-31 19:26:25,778 ===> Epoch[326/1000]
2023-03-31 19:26:49,326 >>> Training Loss: 0.000000200493929 ### Testing Loss: 0.000000315523408
2023-03-31 19:26:49,326 ===> Epoch[327/1000]
2023-03-31 19:27:12,816 >>> Training Loss: 0.000000202166333 ### Testing Loss: 0.000000324552104
2023-03-31 19:27:12,816 ===> Epoch[328/1000]
2023-03-31 19:27:36,345 >>> Training Loss: 0.000000209480802 ### Testing Loss: 0.000000312842815
2023-03-31 19:27:36,345 ===> Epoch[329/1000]
2023-03-31 19:27:59,806 >>> Training Loss: 0.000000200910833 ### Testing Loss: 0.000000313902774
2023-03-31 19:27:59,806 ===> Epoch[330/1000]
2023-03-31 19:28:23,286 >>> Training Loss: 0.000000219750831 ### Testing Loss: 0.000000325954431
2023-03-31 19:28:23,286 ===> Epoch[331/1000]
2023-03-31 19:28:46,726 >>> Training Loss: 0.000000260349310 ### Testing Loss: 0.000000317368290
2023-03-31 19:28:46,726 ===> Epoch[332/1000]
2023-03-31 19:29:10,256 >>> Training Loss: 0.000000202471043 ### Testing Loss: 0.000000310125159
2023-03-31 19:29:10,256 ===> Epoch[333/1000]
2023-03-31 19:29:33,796 >>> Training Loss: 0.000000198170412 ### Testing Loss: 0.000000308923489
2023-03-31 19:29:33,796 ===> Epoch[334/1000]
2023-03-31 19:29:57,376 >>> Training Loss: 0.000000195093122 ### Testing Loss: 0.000000308976183
2023-03-31 19:29:57,376 ===> Epoch[335/1000]
2023-03-31 19:30:20,722 >>> Training Loss: 0.000000193070164 ### Testing Loss: 0.000000310605969
2023-03-31 19:30:20,722 ===> Epoch[336/1000]
2023-03-31 19:30:44,142 >>> Training Loss: 0.000000195084851 ### Testing Loss: 0.000000307183683
2023-03-31 19:30:44,142 ===> Epoch[337/1000]
2023-03-31 19:31:07,582 >>> Training Loss: 0.000000195610298 ### Testing Loss: 0.000000312881156
2023-03-31 19:31:07,582 ===> Epoch[338/1000]
2023-03-31 19:31:31,032 >>> Training Loss: 0.000000202236620 ### Testing Loss: 0.000000312899743
2023-03-31 19:31:31,032 ===> Epoch[339/1000]
2023-03-31 19:31:54,482 >>> Training Loss: 0.000000198700363 ### Testing Loss: 0.000000309781512
2023-03-31 19:31:54,482 ===> Epoch[340/1000]
2023-03-31 19:32:17,842 >>> Training Loss: 0.000000207497621 ### Testing Loss: 0.000000312435503
2023-03-31 19:32:17,852 ===> Epoch[341/1000]
2023-03-31 19:32:41,272 >>> Training Loss: 0.000000207761587 ### Testing Loss: 0.000000309277112
2023-03-31 19:32:41,272 ===> Epoch[342/1000]
2023-03-31 19:33:04,662 >>> Training Loss: 0.000000198845228 ### Testing Loss: 0.000000311010268
2023-03-31 19:33:04,662 ===> Epoch[343/1000]
2023-03-31 19:33:28,132 >>> Training Loss: 0.000000204166597 ### Testing Loss: 0.000000307935096
2023-03-31 19:33:28,132 ===> Epoch[344/1000]
2023-03-31 19:33:51,622 >>> Training Loss: 0.000000199631330 ### Testing Loss: 0.000000307446641
2023-03-31 19:33:51,622 ===> Epoch[345/1000]
2023-03-31 19:34:15,135 >>> Training Loss: 0.000000198159427 ### Testing Loss: 0.000000317629087
2023-03-31 19:34:15,135 ===> Epoch[346/1000]
2023-03-31 19:34:39,457 >>> Training Loss: 0.000000208136100 ### Testing Loss: 0.000000316311258
2023-03-31 19:34:39,457 ===> Epoch[347/1000]
2023-03-31 19:35:02,947 >>> Training Loss: 0.000000202101702 ### Testing Loss: 0.000000309313975
2023-03-31 19:35:02,947 ===> Epoch[348/1000]
2023-03-31 19:35:26,427 >>> Training Loss: 0.000000199308047 ### Testing Loss: 0.000000311815256
2023-03-31 19:35:26,427 ===> Epoch[349/1000]
2023-03-31 19:35:49,947 >>> Training Loss: 0.000000196879071 ### Testing Loss: 0.000000314151521
2023-03-31 19:35:49,947 ===> Epoch[350/1000]
2023-03-31 19:36:13,387 >>> Training Loss: 0.000000201417365 ### Testing Loss: 0.000000307403155
2023-03-31 19:36:13,387 ===> Epoch[351/1000]
2023-03-31 19:36:36,917 >>> Training Loss: 0.000000204768966 ### Testing Loss: 0.000000313799546
2023-03-31 19:36:36,917 ===> Epoch[352/1000]
2023-03-31 19:37:00,327 >>> Training Loss: 0.000000199938924 ### Testing Loss: 0.000000307333664
2023-03-31 19:37:00,327 ===> Epoch[353/1000]
2023-03-31 19:37:23,787 >>> Training Loss: 0.000000194471525 ### Testing Loss: 0.000000307291799
2023-03-31 19:37:23,787 ===> Epoch[354/1000]
2023-03-31 19:37:47,232 >>> Training Loss: 0.000000202961985 ### Testing Loss: 0.000000313517916
2023-03-31 19:37:47,232 ===> Epoch[355/1000]
2023-03-31 19:38:10,662 >>> Training Loss: 0.000000197656846 ### Testing Loss: 0.000000348634359
2023-03-31 19:38:10,662 ===> Epoch[356/1000]
2023-03-31 19:38:34,142 >>> Training Loss: 0.000000213514397 ### Testing Loss: 0.000000309748458
2023-03-31 19:38:34,142 ===> Epoch[357/1000]
2023-03-31 19:38:57,662 >>> Training Loss: 0.000000191143172 ### Testing Loss: 0.000000312197557
2023-03-31 19:38:57,662 ===> Epoch[358/1000]
2023-03-31 19:39:21,112 >>> Training Loss: 0.000000194128930 ### Testing Loss: 0.000000307582525
2023-03-31 19:39:21,112 ===> Epoch[359/1000]
2023-03-31 19:39:44,512 >>> Training Loss: 0.000000197329683 ### Testing Loss: 0.000000309154331
2023-03-31 19:39:44,512 ===> Epoch[360/1000]
2023-03-31 19:40:07,912 >>> Training Loss: 0.000000190393990 ### Testing Loss: 0.000000310180383
2023-03-31 19:40:07,912 ===> Epoch[361/1000]
2023-03-31 19:40:31,272 >>> Training Loss: 0.000000188630892 ### Testing Loss: 0.000000308878441
2023-03-31 19:40:31,272 ===> Epoch[362/1000]
2023-03-31 19:40:54,682 >>> Training Loss: 0.000000190162169 ### Testing Loss: 0.000000308834359
2023-03-31 19:40:54,682 ===> Epoch[363/1000]
2023-03-31 19:41:18,319 >>> Training Loss: 0.000000194904146 ### Testing Loss: 0.000000308722662
2023-03-31 19:41:18,319 ===> Epoch[364/1000]
2023-03-31 19:41:41,739 >>> Training Loss: 0.000000191257868 ### Testing Loss: 0.000000309456794
2023-03-31 19:41:41,739 ===> Epoch[365/1000]
2023-03-31 19:42:05,269 >>> Training Loss: 0.000000199036364 ### Testing Loss: 0.000000308418265
2023-03-31 19:42:05,269 ===> Epoch[366/1000]
2023-03-31 19:42:28,779 >>> Training Loss: 0.000000192081970 ### Testing Loss: 0.000000312407906
2023-03-31 19:42:28,779 ===> Epoch[367/1000]
2023-03-31 19:42:52,279 >>> Training Loss: 0.000000195946498 ### Testing Loss: 0.000000306100304
2023-03-31 19:42:52,279 ===> Epoch[368/1000]
2023-03-31 19:43:15,759 >>> Training Loss: 0.000000196654142 ### Testing Loss: 0.000000308940514
2023-03-31 19:43:15,759 ===> Epoch[369/1000]
2023-03-31 19:43:39,270 >>> Training Loss: 0.000000192970106 ### Testing Loss: 0.000000314664874
2023-03-31 19:43:39,270 ===> Epoch[370/1000]
2023-03-31 19:44:02,830 >>> Training Loss: 0.000000189230931 ### Testing Loss: 0.000000312805582
2023-03-31 19:44:02,830 ===> Epoch[371/1000]
2023-03-31 19:44:26,339 >>> Training Loss: 0.000000192310608 ### Testing Loss: 0.000000307532929
2023-03-31 19:44:26,339 ===> Epoch[372/1000]
2023-03-31 19:44:49,793 >>> Training Loss: 0.000000194316641 ### Testing Loss: 0.000000308881511
2023-03-31 19:44:49,793 ===> Epoch[373/1000]
2023-03-31 19:45:13,349 >>> Training Loss: 0.000000193962222 ### Testing Loss: 0.000000314433777
2023-03-31 19:45:13,349 ===> Epoch[374/1000]
2023-03-31 19:45:36,919 >>> Training Loss: 0.000000199822097 ### Testing Loss: 0.000000306486925
2023-03-31 19:45:36,919 ===> Epoch[375/1000]
2023-03-31 19:46:00,409 >>> Training Loss: 0.000000190343059 ### Testing Loss: 0.000000308233552
2023-03-31 19:46:00,409 ===> Epoch[376/1000]
2023-03-31 19:46:23,810 >>> Training Loss: 0.000000188565195 ### Testing Loss: 0.000000305670852
2023-03-31 19:46:23,810 ===> Epoch[377/1000]
2023-03-31 19:46:47,250 >>> Training Loss: 0.000000185514594 ### Testing Loss: 0.000000311967568
2023-03-31 19:46:47,250 ===> Epoch[378/1000]
2023-03-31 19:47:10,690 >>> Training Loss: 0.000000185426416 ### Testing Loss: 0.000000309572670
2023-03-31 19:47:10,690 ===> Epoch[379/1000]
2023-03-31 19:47:34,290 >>> Training Loss: 0.000000183298937 ### Testing Loss: 0.000000307729806
2023-03-31 19:47:34,290 ===> Epoch[380/1000]
2023-03-31 19:47:57,940 >>> Training Loss: 0.000000189042922 ### Testing Loss: 0.000000307682171
2023-03-31 19:47:57,940 ===> Epoch[381/1000]
2023-03-31 19:48:21,560 >>> Training Loss: 0.000000189426132 ### Testing Loss: 0.000000307649600
2023-03-31 19:48:21,560 ===> Epoch[382/1000]
2023-03-31 19:48:45,193 >>> Training Loss: 0.000000187237646 ### Testing Loss: 0.000000307425438
2023-03-31 19:48:45,193 ===> Epoch[383/1000]
2023-03-31 19:49:08,793 >>> Training Loss: 0.000000190080669 ### Testing Loss: 0.000000308721695
2023-03-31 19:49:08,793 ===> Epoch[384/1000]
2023-03-31 19:49:33,235 >>> Training Loss: 0.000000184978134 ### Testing Loss: 0.000000309775459
2023-03-31 19:49:33,235 ===> Epoch[385/1000]
2023-03-31 19:49:56,825 >>> Training Loss: 0.000000190095847 ### Testing Loss: 0.000000310358615
2023-03-31 19:49:56,825 ===> Epoch[386/1000]
2023-03-31 19:50:20,685 >>> Training Loss: 0.000000187176582 ### Testing Loss: 0.000000330427014
2023-03-31 19:50:20,685 ===> Epoch[387/1000]
2023-03-31 19:50:44,485 >>> Training Loss: 0.000000212099280 ### Testing Loss: 0.000000305758675
2023-03-31 19:50:44,485 ===> Epoch[388/1000]
2023-03-31 19:51:08,325 >>> Training Loss: 0.000000189049445 ### Testing Loss: 0.000000310258173
2023-03-31 19:51:08,325 ===> Epoch[389/1000]
2023-03-31 19:51:32,155 >>> Training Loss: 0.000000189169270 ### Testing Loss: 0.000000307415917
2023-03-31 19:51:32,155 ===> Epoch[390/1000]
2023-03-31 19:51:56,035 >>> Training Loss: 0.000000179723330 ### Testing Loss: 0.000000309145776
2023-03-31 19:51:56,035 ===> Epoch[391/1000]
2023-03-31 19:52:19,909 >>> Training Loss: 0.000000193915753 ### Testing Loss: 0.000000309702671
2023-03-31 19:52:19,909 ===> Epoch[392/1000]
2023-03-31 19:52:43,749 >>> Training Loss: 0.000000180642203 ### Testing Loss: 0.000000305125013
2023-03-31 19:52:43,749 ===> Epoch[393/1000]
2023-03-31 19:53:07,549 >>> Training Loss: 0.000000180911670 ### Testing Loss: 0.000000305724143
2023-03-31 19:53:07,549 ===> Epoch[394/1000]
2023-03-31 19:53:31,399 >>> Training Loss: 0.000000179679475 ### Testing Loss: 0.000000308467577
2023-03-31 19:53:31,399 ===> Epoch[395/1000]
2023-03-31 19:53:55,239 >>> Training Loss: 0.000000180904436 ### Testing Loss: 0.000000304990664
2023-03-31 19:53:55,239 ===> Epoch[396/1000]
2023-03-31 19:54:19,080 >>> Training Loss: 0.000000180509048 ### Testing Loss: 0.000000306955513
2023-03-31 19:54:19,080 ===> Epoch[397/1000]
2023-03-31 19:54:42,860 >>> Training Loss: 0.000000181074441 ### Testing Loss: 0.000000312211341
2023-03-31 19:54:42,860 ===> Epoch[398/1000]
2023-03-31 19:55:06,720 >>> Training Loss: 0.000000195312126 ### Testing Loss: 0.000000311546501
2023-03-31 19:55:06,720 ===> Epoch[399/1000]
2023-03-31 19:55:30,589 >>> Training Loss: 0.000000188179101 ### Testing Loss: 0.000000317820735
2023-03-31 19:55:30,589 ===> Epoch[400/1000]
2023-03-31 19:55:54,372 >>> Training Loss: 0.000000195480226 ### Testing Loss: 0.000000314154221
2023-03-31 19:55:54,402 ===> Epoch[401/1000]
2023-03-31 19:56:18,262 >>> Training Loss: 0.000000191711933 ### Testing Loss: 0.000000306166470
2023-03-31 19:56:18,272 ===> Epoch[402/1000]
2023-03-31 19:56:42,022 >>> Training Loss: 0.000000183854141 ### Testing Loss: 0.000000307862422
2023-03-31 19:56:42,022 ===> Epoch[403/1000]
2023-03-31 19:57:05,873 >>> Training Loss: 0.000000177884431 ### Testing Loss: 0.000000306648218
2023-03-31 19:57:05,873 ===> Epoch[404/1000]
2023-03-31 19:57:29,722 >>> Training Loss: 0.000000177955201 ### Testing Loss: 0.000000317188864
2023-03-31 19:57:29,722 ===> Epoch[405/1000]
2023-03-31 19:57:53,542 >>> Training Loss: 0.000000182761596 ### Testing Loss: 0.000000308279652
2023-03-31 19:57:53,542 ===> Epoch[406/1000]
2023-03-31 19:58:17,332 >>> Training Loss: 0.000000181446850 ### Testing Loss: 0.000000310068913
2023-03-31 19:58:17,332 ===> Epoch[407/1000]
2023-03-31 19:58:41,072 >>> Training Loss: 0.000000178573202 ### Testing Loss: 0.000000314631109
2023-03-31 19:58:41,072 ===> Epoch[408/1000]
2023-03-31 19:59:04,793 >>> Training Loss: 0.000000182096954 ### Testing Loss: 0.000000306545559
2023-03-31 19:59:04,793 ===> Epoch[409/1000]
2023-03-31 19:59:28,587 >>> Training Loss: 0.000000182746447 ### Testing Loss: 0.000000312560502
2023-03-31 19:59:28,587 ===> Epoch[410/1000]
2023-03-31 19:59:52,385 >>> Training Loss: 0.000000180299949 ### Testing Loss: 0.000000307488875
2023-03-31 19:59:52,385 ===> Epoch[411/1000]
2023-03-31 20:00:16,155 >>> Training Loss: 0.000000177009198 ### Testing Loss: 0.000000308101477
2023-03-31 20:00:16,155 ===> Epoch[412/1000]
2023-03-31 20:00:39,915 >>> Training Loss: 0.000000203853077 ### Testing Loss: 0.000000308244807
2023-03-31 20:00:39,915 ===> Epoch[413/1000]
2023-03-31 20:01:03,755 >>> Training Loss: 0.000000177421839 ### Testing Loss: 0.000000308937416
2023-03-31 20:01:03,755 ===> Epoch[414/1000]
2023-03-31 20:01:27,504 >>> Training Loss: 0.000000183173654 ### Testing Loss: 0.000000312467506
2023-03-31 20:01:27,504 ===> Epoch[415/1000]
2023-03-31 20:01:51,324 >>> Training Loss: 0.000000180990781 ### Testing Loss: 0.000000316229176
2023-03-31 20:01:51,324 ===> Epoch[416/1000]
2023-03-31 20:02:15,134 >>> Training Loss: 0.000000180047394 ### Testing Loss: 0.000000310391584
2023-03-31 20:02:15,134 ===> Epoch[417/1000]
2023-03-31 20:02:38,874 >>> Training Loss: 0.000000176043301 ### Testing Loss: 0.000000308647344
2023-03-31 20:02:38,874 ===> Epoch[418/1000]
2023-03-31 20:03:02,739 >>> Training Loss: 0.000000180044111 ### Testing Loss: 0.000000310171913
2023-03-31 20:03:02,739 ===> Epoch[419/1000]
2023-03-31 20:03:26,538 >>> Training Loss: 0.000000175520597 ### Testing Loss: 0.000000309718558
2023-03-31 20:03:26,538 ===> Epoch[420/1000]
2023-03-31 20:03:50,328 >>> Training Loss: 0.000000177812424 ### Testing Loss: 0.000000308074448
2023-03-31 20:03:50,328 ===> Epoch[421/1000]
2023-03-31 20:04:14,108 >>> Training Loss: 0.000000177945793 ### Testing Loss: 0.000000309641734
2023-03-31 20:04:14,108 ===> Epoch[422/1000]
2023-03-31 20:04:38,898 >>> Training Loss: 0.000000181433222 ### Testing Loss: 0.000000307225605
2023-03-31 20:04:38,898 ===> Epoch[423/1000]
2023-03-31 20:05:02,828 >>> Training Loss: 0.000000178943750 ### Testing Loss: 0.000000310752796
2023-03-31 20:05:02,828 ===> Epoch[424/1000]
2023-03-31 20:05:26,658 >>> Training Loss: 0.000000186126982 ### Testing Loss: 0.000000324079423
2023-03-31 20:05:26,658 ===> Epoch[425/1000]
2023-03-31 20:05:50,518 >>> Training Loss: 0.000000179655459 ### Testing Loss: 0.000000304622858
2023-03-31 20:05:50,518 ===> Epoch[426/1000]
2023-03-31 20:06:14,368 >>> Training Loss: 0.000000182787218 ### Testing Loss: 0.000000309983164
2023-03-31 20:06:14,368 ===> Epoch[427/1000]
2023-03-31 20:06:38,224 >>> Training Loss: 0.000000176615615 ### Testing Loss: 0.000000308854055
2023-03-31 20:06:38,224 ===> Epoch[428/1000]
2023-03-31 20:07:02,051 >>> Training Loss: 0.000000172399353 ### Testing Loss: 0.000000309834377
2023-03-31 20:07:02,051 ===> Epoch[429/1000]
2023-03-31 20:07:25,891 >>> Training Loss: 0.000000178831002 ### Testing Loss: 0.000000307620041
2023-03-31 20:07:25,891 ===> Epoch[430/1000]
2023-03-31 20:07:49,751 >>> Training Loss: 0.000000187688755 ### Testing Loss: 0.000000318861026
2023-03-31 20:07:49,751 ===> Epoch[431/1000]
2023-03-31 20:08:13,682 >>> Training Loss: 0.000000176161038 ### Testing Loss: 0.000000309366982
2023-03-31 20:08:13,682 ===> Epoch[432/1000]
2023-03-31 20:08:37,522 >>> Training Loss: 0.000000185075109 ### Testing Loss: 0.000000305984969
2023-03-31 20:08:37,522 ===> Epoch[433/1000]
2023-03-31 20:09:01,382 >>> Training Loss: 0.000000176241798 ### Testing Loss: 0.000000308495146
2023-03-31 20:09:01,382 ===> Epoch[434/1000]
2023-03-31 20:09:25,222 >>> Training Loss: 0.000000177311421 ### Testing Loss: 0.000000308715130
2023-03-31 20:09:25,222 ===> Epoch[435/1000]
2023-03-31 20:09:49,122 >>> Training Loss: 0.000000177171245 ### Testing Loss: 0.000000311498184
2023-03-31 20:09:49,122 ===> Epoch[436/1000]
2023-03-31 20:10:12,932 >>> Training Loss: 0.000000175946866 ### Testing Loss: 0.000000310645845
2023-03-31 20:10:12,932 ===> Epoch[437/1000]
2023-03-31 20:10:36,778 >>> Training Loss: 0.000000174151879 ### Testing Loss: 0.000000311400754
2023-03-31 20:10:36,778 ===> Epoch[438/1000]
2023-03-31 20:11:00,588 >>> Training Loss: 0.000000175856158 ### Testing Loss: 0.000000309015121
2023-03-31 20:11:00,588 ===> Epoch[439/1000]
2023-03-31 20:11:24,418 >>> Training Loss: 0.000000176243006 ### Testing Loss: 0.000000305660024
2023-03-31 20:11:24,418 ===> Epoch[440/1000]
2023-03-31 20:11:48,128 >>> Training Loss: 0.000000174118370 ### Testing Loss: 0.000000308639187
2023-03-31 20:11:48,128 ===> Epoch[441/1000]
2023-03-31 20:12:11,928 >>> Training Loss: 0.000000174408456 ### Testing Loss: 0.000000304896986
2023-03-31 20:12:11,928 ===> Epoch[442/1000]
2023-03-31 20:12:35,738 >>> Training Loss: 0.000000169851944 ### Testing Loss: 0.000000308716807
2023-03-31 20:12:35,738 ===> Epoch[443/1000]
2023-03-31 20:12:59,698 >>> Training Loss: 0.000000178925021 ### Testing Loss: 0.000000305371771
2023-03-31 20:12:59,698 ===> Epoch[444/1000]
2023-03-31 20:13:23,638 >>> Training Loss: 0.000000171586180 ### Testing Loss: 0.000000305432081
2023-03-31 20:13:23,648 ===> Epoch[445/1000]
2023-03-31 20:13:47,553 >>> Training Loss: 0.000000170181821 ### Testing Loss: 0.000000309198725
2023-03-31 20:13:47,553 ===> Epoch[446/1000]
2023-03-31 20:14:11,523 >>> Training Loss: 0.000000171425725 ### Testing Loss: 0.000000306433094
2023-03-31 20:14:11,523 ===> Epoch[447/1000]
2023-03-31 20:14:35,383 >>> Training Loss: 0.000000169640032 ### Testing Loss: 0.000000307963603
2023-03-31 20:14:35,383 ===> Epoch[448/1000]
2023-03-31 20:14:59,213 >>> Training Loss: 0.000000174166786 ### Testing Loss: 0.000000306446452
2023-03-31 20:14:59,213 ===> Epoch[449/1000]
2023-03-31 20:15:23,023 >>> Training Loss: 0.000000173119489 ### Testing Loss: 0.000000307113311
2023-03-31 20:15:23,023 ===> Epoch[450/1000]
2023-03-31 20:15:46,913 >>> Training Loss: 0.000000197292437 ### Testing Loss: 0.000000318054305
2023-03-31 20:15:46,913 ===> Epoch[451/1000]
2023-03-31 20:16:10,733 >>> Training Loss: 0.000000173664063 ### Testing Loss: 0.000000307953655
2023-03-31 20:16:10,733 ===> Epoch[452/1000]
2023-03-31 20:16:34,653 >>> Training Loss: 0.000000174854492 ### Testing Loss: 0.000000306979075
2023-03-31 20:16:34,653 ===> Epoch[453/1000]
2023-03-31 20:16:58,443 >>> Training Loss: 0.000000170644384 ### Testing Loss: 0.000000308398910
2023-03-31 20:16:58,443 ===> Epoch[454/1000]
2023-03-31 20:17:22,323 >>> Training Loss: 0.000000168120366 ### Testing Loss: 0.000000306027289
2023-03-31 20:17:22,323 ===> Epoch[455/1000]
2023-03-31 20:17:46,158 >>> Training Loss: 0.000000167500986 ### Testing Loss: 0.000000307959823
2023-03-31 20:17:46,158 ===> Epoch[456/1000]
2023-03-31 20:18:10,078 >>> Training Loss: 0.000000166501948 ### Testing Loss: 0.000000306912341
2023-03-31 20:18:10,078 ===> Epoch[457/1000]
2023-03-31 20:18:33,968 >>> Training Loss: 0.000000169198955 ### Testing Loss: 0.000000306018364
2023-03-31 20:18:33,968 ===> Epoch[458/1000]
2023-03-31 20:18:57,728 >>> Training Loss: 0.000000174627743 ### Testing Loss: 0.000000315768716
2023-03-31 20:18:57,738 ===> Epoch[459/1000]
2023-03-31 20:19:22,909 >>> Training Loss: 0.000000168812349 ### Testing Loss: 0.000000307049959
2023-03-31 20:19:22,909 ===> Epoch[460/1000]
2023-03-31 20:19:46,639 >>> Training Loss: 0.000000168005087 ### Testing Loss: 0.000000307722189
2023-03-31 20:19:46,639 ===> Epoch[461/1000]
2023-03-31 20:20:10,479 >>> Training Loss: 0.000000175276782 ### Testing Loss: 0.000000306700827
2023-03-31 20:20:10,479 ===> Epoch[462/1000]
2023-03-31 20:20:34,369 >>> Training Loss: 0.000000176461270 ### Testing Loss: 0.000000312403898
2023-03-31 20:20:34,369 ===> Epoch[463/1000]
2023-03-31 20:20:58,205 >>> Training Loss: 0.000000185743758 ### Testing Loss: 0.000000305826006
2023-03-31 20:20:58,205 ===> Epoch[464/1000]
2023-03-31 20:21:22,115 >>> Training Loss: 0.000000173607361 ### Testing Loss: 0.000000309360217
2023-03-31 20:21:22,115 ===> Epoch[465/1000]
2023-03-31 20:21:45,975 >>> Training Loss: 0.000000165844710 ### Testing Loss: 0.000000305760068
2023-03-31 20:21:45,975 ===> Epoch[466/1000]
2023-03-31 20:22:09,755 >>> Training Loss: 0.000000170841176 ### Testing Loss: 0.000000320702213
2023-03-31 20:22:09,755 ===> Epoch[467/1000]
2023-03-31 20:22:33,635 >>> Training Loss: 0.000000172912081 ### Testing Loss: 0.000000313241003
2023-03-31 20:22:33,635 ===> Epoch[468/1000]
2023-03-31 20:22:57,535 >>> Training Loss: 0.000000172152127 ### Testing Loss: 0.000000311685000
2023-03-31 20:22:57,535 ===> Epoch[469/1000]
2023-03-31 20:23:21,415 >>> Training Loss: 0.000000168766533 ### Testing Loss: 0.000000305681112
2023-03-31 20:23:21,415 ===> Epoch[470/1000]
2023-03-31 20:23:45,304 >>> Training Loss: 0.000000185592825 ### Testing Loss: 0.000000307498567
2023-03-31 20:23:45,304 ===> Epoch[471/1000]
2023-03-31 20:24:09,154 >>> Training Loss: 0.000000168922284 ### Testing Loss: 0.000000308167529
2023-03-31 20:24:09,154 ===> Epoch[472/1000]
2023-03-31 20:24:33,155 >>> Training Loss: 0.000000170570246 ### Testing Loss: 0.000000307550295
2023-03-31 20:24:33,155 ===> Epoch[473/1000]
2023-03-31 20:24:56,979 >>> Training Loss: 0.000000173272511 ### Testing Loss: 0.000000310751915
2023-03-31 20:24:56,979 ===> Epoch[474/1000]
2023-03-31 20:25:20,800 >>> Training Loss: 0.000000170895774 ### Testing Loss: 0.000000312663957
2023-03-31 20:25:20,800 ===> Epoch[475/1000]
2023-03-31 20:25:44,650 >>> Training Loss: 0.000000173818407 ### Testing Loss: 0.000000307406651
2023-03-31 20:25:44,650 ===> Epoch[476/1000]
2023-03-31 20:26:08,470 >>> Training Loss: 0.000000174225534 ### Testing Loss: 0.000000311914988
2023-03-31 20:26:08,470 ===> Epoch[477/1000]
2023-03-31 20:26:32,300 >>> Training Loss: 0.000000172934577 ### Testing Loss: 0.000000306462056
2023-03-31 20:26:32,300 ===> Epoch[478/1000]
2023-03-31 20:26:56,110 >>> Training Loss: 0.000000169466844 ### Testing Loss: 0.000000305292019
2023-03-31 20:26:56,110 ===> Epoch[479/1000]
2023-03-31 20:27:19,960 >>> Training Loss: 0.000000166032933 ### Testing Loss: 0.000000306698013
2023-03-31 20:27:19,960 ===> Epoch[480/1000]
2023-03-31 20:27:43,800 >>> Training Loss: 0.000000166852146 ### Testing Loss: 0.000000307643035
2023-03-31 20:27:43,800 ===> Epoch[481/1000]
2023-03-31 20:28:07,620 >>> Training Loss: 0.000000165120881 ### Testing Loss: 0.000000306922743
2023-03-31 20:28:07,620 ===> Epoch[482/1000]
2023-03-31 20:28:31,481 >>> Training Loss: 0.000000167939518 ### Testing Loss: 0.000000316328197
2023-03-31 20:28:31,481 ===> Epoch[483/1000]
2023-03-31 20:28:55,341 >>> Training Loss: 0.000000178601525 ### Testing Loss: 0.000000307557144
2023-03-31 20:28:55,341 ===> Epoch[484/1000]
2023-03-31 20:29:19,261 >>> Training Loss: 0.000000163442749 ### Testing Loss: 0.000000306282260
2023-03-31 20:29:19,261 ===> Epoch[485/1000]
2023-03-31 20:29:43,181 >>> Training Loss: 0.000000163348545 ### Testing Loss: 0.000000309469300
2023-03-31 20:29:43,181 ===> Epoch[486/1000]
2023-03-31 20:30:07,071 >>> Training Loss: 0.000000162997509 ### Testing Loss: 0.000000305498503
2023-03-31 20:30:07,071 ===> Epoch[487/1000]
2023-03-31 20:30:30,910 >>> Training Loss: 0.000000162516372 ### Testing Loss: 0.000000305537810
2023-03-31 20:30:30,910 ===> Epoch[488/1000]
2023-03-31 20:30:54,721 >>> Training Loss: 0.000000165200674 ### Testing Loss: 0.000000306232238
2023-03-31 20:30:54,721 ===> Epoch[489/1000]
2023-03-31 20:31:18,571 >>> Training Loss: 0.000000178549726 ### Testing Loss: 0.000000312371156
2023-03-31 20:31:18,571 ===> Epoch[490/1000]
2023-03-31 20:31:42,401 >>> Training Loss: 0.000000176443933 ### Testing Loss: 0.000000304132953
2023-03-31 20:31:42,401 ===> Epoch[491/1000]
2023-03-31 20:32:06,245 >>> Training Loss: 0.000000170735134 ### Testing Loss: 0.000000310291142
2023-03-31 20:32:06,245 ===> Epoch[492/1000]
2023-03-31 20:32:30,064 >>> Training Loss: 0.000000165200234 ### Testing Loss: 0.000000308314952
2023-03-31 20:32:30,064 ===> Epoch[493/1000]
2023-03-31 20:32:53,935 >>> Training Loss: 0.000000162247034 ### Testing Loss: 0.000000303851436
2023-03-31 20:32:53,935 ===> Epoch[494/1000]
2023-03-31 20:33:17,745 >>> Training Loss: 0.000000163904900 ### Testing Loss: 0.000000306882441
2023-03-31 20:33:17,745 ===> Epoch[495/1000]
2023-03-31 20:33:41,645 >>> Training Loss: 0.000000163508304 ### Testing Loss: 0.000000305565578
2023-03-31 20:33:41,645 ===> Epoch[496/1000]
2023-03-31 20:34:05,525 >>> Training Loss: 0.000000166298136 ### Testing Loss: 0.000000308555201
2023-03-31 20:34:05,525 ===> Epoch[497/1000]
2023-03-31 20:34:30,395 >>> Training Loss: 0.000000164411261 ### Testing Loss: 0.000000305889529
2023-03-31 20:34:30,395 ===> Epoch[498/1000]
2023-03-31 20:34:54,204 >>> Training Loss: 0.000000160600308 ### Testing Loss: 0.000000306481553
2023-03-31 20:34:54,204 ===> Epoch[499/1000]
2023-03-31 20:35:18,054 >>> Training Loss: 0.000000165298857 ### Testing Loss: 0.000000306603738
2023-03-31 20:35:18,054 ===> Epoch[500/1000]
2023-03-31 20:35:41,980 >>> Training Loss: 0.000000179751567 ### Testing Loss: 0.000000311286612
2023-03-31 20:35:42,010 ===> Epoch[501/1000]
2023-03-31 20:36:05,859 >>> Training Loss: 0.000000173518387 ### Testing Loss: 0.000000319179975
2023-03-31 20:36:05,859 ===> Epoch[502/1000]
2023-03-31 20:36:29,669 >>> Training Loss: 0.000000172678469 ### Testing Loss: 0.000000308548010
2023-03-31 20:36:29,669 ===> Epoch[503/1000]
2023-03-31 20:36:53,529 >>> Training Loss: 0.000000212928427 ### Testing Loss: 0.000000310390249
2023-03-31 20:36:53,529 ===> Epoch[504/1000]
2023-03-31 20:37:17,380 >>> Training Loss: 0.000000207317669 ### Testing Loss: 0.000000306057103
2023-03-31 20:37:17,380 ===> Epoch[505/1000]
2023-03-31 20:37:41,240 >>> Training Loss: 0.000000166705220 ### Testing Loss: 0.000000304538503
2023-03-31 20:37:41,240 ===> Epoch[506/1000]
2023-03-31 20:38:04,949 >>> Training Loss: 0.000000161343067 ### Testing Loss: 0.000000306629090
2023-03-31 20:38:04,949 ===> Epoch[507/1000]
2023-03-31 20:38:28,769 >>> Training Loss: 0.000000156990154 ### Testing Loss: 0.000000302998814
2023-03-31 20:38:28,769 ===> Epoch[508/1000]
2023-03-31 20:38:52,559 >>> Training Loss: 0.000000156471884 ### Testing Loss: 0.000000303481329
2023-03-31 20:38:52,559 ===> Epoch[509/1000]
2023-03-31 20:39:16,330 >>> Training Loss: 0.000000157710772 ### Testing Loss: 0.000000305551652
2023-03-31 20:39:16,330 ===> Epoch[510/1000]
2023-03-31 20:39:40,143 >>> Training Loss: 0.000000157837363 ### Testing Loss: 0.000000303863601
2023-03-31 20:39:40,143 ===> Epoch[511/1000]
2023-03-31 20:40:03,893 >>> Training Loss: 0.000000160168966 ### Testing Loss: 0.000000307709655
2023-03-31 20:40:03,903 ===> Epoch[512/1000]
2023-03-31 20:40:27,713 >>> Training Loss: 0.000000164130796 ### Testing Loss: 0.000000304203468
2023-03-31 20:40:27,713 ===> Epoch[513/1000]
2023-03-31 20:40:51,533 >>> Training Loss: 0.000000160996933 ### Testing Loss: 0.000000308075897
2023-03-31 20:40:51,533 ===> Epoch[514/1000]
2023-03-31 20:41:15,413 >>> Training Loss: 0.000000163069132 ### Testing Loss: 0.000000306588475
2023-03-31 20:41:15,413 ===> Epoch[515/1000]
2023-03-31 20:41:39,303 >>> Training Loss: 0.000000162208352 ### Testing Loss: 0.000000306865871
2023-03-31 20:41:39,303 ===> Epoch[516/1000]
2023-03-31 20:42:03,143 >>> Training Loss: 0.000000159429931 ### Testing Loss: 0.000000307648719
2023-03-31 20:42:03,143 ===> Epoch[517/1000]
2023-03-31 20:42:26,963 >>> Training Loss: 0.000000158915441 ### Testing Loss: 0.000000305317513
2023-03-31 20:42:26,963 ===> Epoch[518/1000]
2023-03-31 20:42:50,809 >>> Training Loss: 0.000000160393952 ### Testing Loss: 0.000000307839798
2023-03-31 20:42:50,809 ===> Epoch[519/1000]
2023-03-31 20:43:14,665 >>> Training Loss: 0.000000160900157 ### Testing Loss: 0.000000304679901
2023-03-31 20:43:14,665 ===> Epoch[520/1000]
2023-03-31 20:43:38,795 >>> Training Loss: 0.000000158804866 ### Testing Loss: 0.000000305366882
2023-03-31 20:43:38,795 ===> Epoch[521/1000]
2023-03-31 20:44:02,645 >>> Training Loss: 0.000000159348133 ### Testing Loss: 0.000000307029779
2023-03-31 20:44:02,645 ===> Epoch[522/1000]
2023-03-31 20:44:26,535 >>> Training Loss: 0.000000158909302 ### Testing Loss: 0.000000306098457
2023-03-31 20:44:26,535 ===> Epoch[523/1000]
2023-03-31 20:44:50,365 >>> Training Loss: 0.000000178163063 ### Testing Loss: 0.000000488146043
2023-03-31 20:44:50,365 ===> Epoch[524/1000]
2023-03-31 20:45:14,185 >>> Training Loss: 0.000000173849955 ### Testing Loss: 0.000000305407951
2023-03-31 20:45:14,185 ===> Epoch[525/1000]
2023-03-31 20:45:37,986 >>> Training Loss: 0.000000162450831 ### Testing Loss: 0.000000308043042
2023-03-31 20:45:37,986 ===> Epoch[526/1000]
2023-03-31 20:46:01,866 >>> Training Loss: 0.000000161615603 ### Testing Loss: 0.000000306807607
2023-03-31 20:46:01,866 ===> Epoch[527/1000]
2023-03-31 20:46:25,616 >>> Training Loss: 0.000000162830730 ### Testing Loss: 0.000000305999379
2023-03-31 20:46:25,616 ===> Epoch[528/1000]
2023-03-31 20:46:49,572 >>> Training Loss: 0.000000162361275 ### Testing Loss: 0.000000304165582
2023-03-31 20:46:49,572 ===> Epoch[529/1000]
2023-03-31 20:47:13,362 >>> Training Loss: 0.000000163353022 ### Testing Loss: 0.000000306747268
2023-03-31 20:47:13,372 ===> Epoch[530/1000]
2023-03-31 20:47:37,172 >>> Training Loss: 0.000000161154119 ### Testing Loss: 0.000000304618482
2023-03-31 20:47:37,172 ===> Epoch[531/1000]
2023-03-31 20:48:01,082 >>> Training Loss: 0.000000158837295 ### Testing Loss: 0.000000304237915
2023-03-31 20:48:01,082 ===> Epoch[532/1000]
2023-03-31 20:48:24,892 >>> Training Loss: 0.000000157448582 ### Testing Loss: 0.000000305783715
2023-03-31 20:48:24,892 ===> Epoch[533/1000]
2023-03-31 20:48:48,802 >>> Training Loss: 0.000000156559906 ### Testing Loss: 0.000000307037993
2023-03-31 20:48:48,802 ===> Epoch[534/1000]
2023-03-31 20:49:12,612 >>> Training Loss: 0.000000165424524 ### Testing Loss: 0.000000304157851
2023-03-31 20:49:12,612 ===> Epoch[535/1000]
2023-03-31 20:49:37,413 >>> Training Loss: 0.000000164175020 ### Testing Loss: 0.000000308337661
2023-03-31 20:49:37,413 ===> Epoch[536/1000]
2023-03-31 20:50:01,333 >>> Training Loss: 0.000000164930697 ### Testing Loss: 0.000000304604669
2023-03-31 20:50:01,333 ===> Epoch[537/1000]
2023-03-31 20:50:25,008 >>> Training Loss: 0.000000161455489 ### Testing Loss: 0.000000307404434
2023-03-31 20:50:25,018 ===> Epoch[538/1000]
2023-03-31 20:50:48,668 >>> Training Loss: 0.000000247415784 ### Testing Loss: 0.000000305847635
2023-03-31 20:50:48,668 ===> Epoch[539/1000]
2023-03-31 20:51:12,408 >>> Training Loss: 0.000000165222431 ### Testing Loss: 0.000000306602630
2023-03-31 20:51:12,408 ===> Epoch[540/1000]
2023-03-31 20:51:36,088 >>> Training Loss: 0.000000162830972 ### Testing Loss: 0.000000306849785
2023-03-31 20:51:36,088 ===> Epoch[541/1000]
2023-03-31 20:51:59,918 >>> Training Loss: 0.000000162481570 ### Testing Loss: 0.000000304138950
2023-03-31 20:51:59,918 ===> Epoch[542/1000]
2023-03-31 20:52:23,608 >>> Training Loss: 0.000000155371168 ### Testing Loss: 0.000000303982148
2023-03-31 20:52:23,608 ===> Epoch[543/1000]
2023-03-31 20:52:47,338 >>> Training Loss: 0.000000165838529 ### Testing Loss: 0.000000305661047
2023-03-31 20:52:47,338 ===> Epoch[544/1000]
2023-03-31 20:53:11,018 >>> Training Loss: 0.000000155884180 ### Testing Loss: 0.000000303076604
2023-03-31 20:53:11,018 ===> Epoch[545/1000]
2023-03-31 20:53:34,678 >>> Training Loss: 0.000000153283850 ### Testing Loss: 0.000000305460844
2023-03-31 20:53:34,688 ===> Epoch[546/1000]
2023-03-31 20:53:58,452 >>> Training Loss: 0.000000152560830 ### Testing Loss: 0.000000303494971
2023-03-31 20:53:58,452 ===> Epoch[547/1000]
2023-03-31 20:54:22,272 >>> Training Loss: 0.000000152284940 ### Testing Loss: 0.000000304536343
2023-03-31 20:54:22,272 ===> Epoch[548/1000]
2023-03-31 20:54:46,112 >>> Training Loss: 0.000000151556279 ### Testing Loss: 0.000000305384958
2023-03-31 20:54:46,112 ===> Epoch[549/1000]
2023-03-31 20:55:09,802 >>> Training Loss: 0.000000156682304 ### Testing Loss: 0.000000324801732
2023-03-31 20:55:09,802 ===> Epoch[550/1000]
2023-03-31 20:55:33,642 >>> Training Loss: 0.000000160861987 ### Testing Loss: 0.000000306636451
2023-03-31 20:55:33,642 ===> Epoch[551/1000]
2023-03-31 20:55:57,542 >>> Training Loss: 0.000000156085051 ### Testing Loss: 0.000000307682399
2023-03-31 20:55:57,542 ===> Epoch[552/1000]
2023-03-31 20:56:21,392 >>> Training Loss: 0.000000161122188 ### Testing Loss: 0.000000304687177
2023-03-31 20:56:21,392 ===> Epoch[553/1000]
2023-03-31 20:56:45,242 >>> Training Loss: 0.000000159130309 ### Testing Loss: 0.000000306899295
2023-03-31 20:56:45,242 ===> Epoch[554/1000]
2023-03-31 20:57:09,092 >>> Training Loss: 0.000000187900866 ### Testing Loss: 0.000000317250795
2023-03-31 20:57:09,092 ===> Epoch[555/1000]
2023-03-31 20:57:32,887 >>> Training Loss: 0.000000165096466 ### Testing Loss: 0.000000310186749
2023-03-31 20:57:32,887 ===> Epoch[556/1000]
2023-03-31 20:57:56,707 >>> Training Loss: 0.000000156059542 ### Testing Loss: 0.000000305571263
2023-03-31 20:57:56,717 ===> Epoch[557/1000]
2023-03-31 20:58:20,607 >>> Training Loss: 0.000000152196165 ### Testing Loss: 0.000000307979860
2023-03-31 20:58:20,607 ===> Epoch[558/1000]
2023-03-31 20:58:44,437 >>> Training Loss: 0.000000150908051 ### Testing Loss: 0.000000304174762
2023-03-31 20:58:44,437 ===> Epoch[559/1000]
2023-03-31 20:59:08,227 >>> Training Loss: 0.000000152182153 ### Testing Loss: 0.000000303744741
2023-03-31 20:59:08,227 ===> Epoch[560/1000]
2023-03-31 20:59:32,087 >>> Training Loss: 0.000000152182153 ### Testing Loss: 0.000000306947896
2023-03-31 20:59:32,087 ===> Epoch[561/1000]
2023-03-31 20:59:55,967 >>> Training Loss: 0.000000155679331 ### Testing Loss: 0.000000304990152
2023-03-31 20:59:55,967 ===> Epoch[562/1000]
2023-03-31 21:00:19,777 >>> Training Loss: 0.000000157379105 ### Testing Loss: 0.000000307582098
2023-03-31 21:00:19,777 ===> Epoch[563/1000]
2023-03-31 21:00:43,637 >>> Training Loss: 0.000000158282788 ### Testing Loss: 0.000000308601471
2023-03-31 21:00:43,637 ===> Epoch[564/1000]
2023-03-31 21:01:07,437 >>> Training Loss: 0.000000155778238 ### Testing Loss: 0.000000306622013
2023-03-31 21:01:07,437 ===> Epoch[565/1000]
2023-03-31 21:01:31,293 >>> Training Loss: 0.000000156595206 ### Testing Loss: 0.000000305143743
2023-03-31 21:01:31,303 ===> Epoch[566/1000]
2023-03-31 21:01:55,183 >>> Training Loss: 0.000000154437515 ### Testing Loss: 0.000000310099637
2023-03-31 21:01:55,183 ===> Epoch[567/1000]
2023-03-31 21:02:18,995 >>> Training Loss: 0.000000164927229 ### Testing Loss: 0.000000306224621
2023-03-31 21:02:18,995 ===> Epoch[568/1000]
2023-03-31 21:02:42,755 >>> Training Loss: 0.000000156419034 ### Testing Loss: 0.000000306245909
2023-03-31 21:02:42,755 ===> Epoch[569/1000]
2023-03-31 21:03:06,685 >>> Training Loss: 0.000000159571471 ### Testing Loss: 0.000000309103712
2023-03-31 21:03:06,685 ===> Epoch[570/1000]
2023-03-31 21:03:30,505 >>> Training Loss: 0.000000188509802 ### Testing Loss: 0.000000305409088
2023-03-31 21:03:30,505 ===> Epoch[571/1000]
2023-03-31 21:03:54,385 >>> Training Loss: 0.000000155894213 ### Testing Loss: 0.000000307373483
2023-03-31 21:03:54,385 ===> Epoch[572/1000]
2023-03-31 21:04:18,396 >>> Training Loss: 0.000000152738068 ### Testing Loss: 0.000000305130044
2023-03-31 21:04:18,396 ===> Epoch[573/1000]
2023-03-31 21:04:43,194 >>> Training Loss: 0.000000151897765 ### Testing Loss: 0.000000303788880
2023-03-31 21:04:43,194 ===> Epoch[574/1000]
2023-03-31 21:05:07,084 >>> Training Loss: 0.000000151997938 ### Testing Loss: 0.000000304790234
2023-03-31 21:05:07,084 ===> Epoch[575/1000]
2023-03-31 21:05:30,853 >>> Training Loss: 0.000000154119434 ### Testing Loss: 0.000000304520228
2023-03-31 21:05:30,853 ===> Epoch[576/1000]
2023-03-31 21:05:54,653 >>> Training Loss: 0.000000157320997 ### Testing Loss: 0.000000307573146
2023-03-31 21:05:54,653 ===> Epoch[577/1000]
2023-03-31 21:06:18,533 >>> Training Loss: 0.000000156450938 ### Testing Loss: 0.000000309538791
2023-03-31 21:06:18,533 ===> Epoch[578/1000]
2023-03-31 21:06:42,344 >>> Training Loss: 0.000000158698057 ### Testing Loss: 0.000000308659992
2023-03-31 21:06:42,344 ===> Epoch[579/1000]
2023-03-31 21:07:06,223 >>> Training Loss: 0.000000152256689 ### Testing Loss: 0.000000306307015
2023-03-31 21:07:06,223 ===> Epoch[580/1000]
2023-03-31 21:07:30,023 >>> Training Loss: 0.000000155540434 ### Testing Loss: 0.000000306131369
2023-03-31 21:07:30,023 ===> Epoch[581/1000]
2023-03-31 21:07:53,753 >>> Training Loss: 0.000000153423471 ### Testing Loss: 0.000000306681471
2023-03-31 21:07:53,753 ===> Epoch[582/1000]
2023-03-31 21:08:17,554 >>> Training Loss: 0.000000152842304 ### Testing Loss: 0.000000306228230
2023-03-31 21:08:17,554 ===> Epoch[583/1000]
2023-03-31 21:08:41,330 >>> Training Loss: 0.000000151463510 ### Testing Loss: 0.000000305131607
2023-03-31 21:08:41,330 ===> Epoch[584/1000]
2023-03-31 21:09:05,120 >>> Training Loss: 0.000000149932788 ### Testing Loss: 0.000000308660958
2023-03-31 21:09:05,120 ===> Epoch[585/1000]
2023-03-31 21:09:28,910 >>> Training Loss: 0.000000150471720 ### Testing Loss: 0.000000304068891
2023-03-31 21:09:28,910 ===> Epoch[586/1000]
2023-03-31 21:09:52,710 >>> Training Loss: 0.000000150640105 ### Testing Loss: 0.000000307256187
2023-03-31 21:09:52,710 ===> Epoch[587/1000]
2023-03-31 21:10:16,590 >>> Training Loss: 0.000000157198286 ### Testing Loss: 0.000000310571608
2023-03-31 21:10:16,600 ===> Epoch[588/1000]
2023-03-31 21:10:40,409 >>> Training Loss: 0.000000158687754 ### Testing Loss: 0.000000304816979
2023-03-31 21:10:40,409 ===> Epoch[589/1000]
2023-03-31 21:11:04,170 >>> Training Loss: 0.000000167963535 ### Testing Loss: 0.000000304919070
2023-03-31 21:11:04,170 ===> Epoch[590/1000]
2023-03-31 21:11:28,020 >>> Training Loss: 0.000000150608770 ### Testing Loss: 0.000000305480796
2023-03-31 21:11:28,020 ===> Epoch[591/1000]
2023-03-31 21:11:51,860 >>> Training Loss: 0.000000148345649 ### Testing Loss: 0.000000304954625
2023-03-31 21:11:51,860 ===> Epoch[592/1000]
2023-03-31 21:12:15,716 >>> Training Loss: 0.000000157281661 ### Testing Loss: 0.000000304779718
2023-03-31 21:12:15,716 ===> Epoch[593/1000]
2023-03-31 21:12:39,477 >>> Training Loss: 0.000000151129342 ### Testing Loss: 0.000000307066813
2023-03-31 21:12:39,477 ===> Epoch[594/1000]
2023-03-31 21:13:03,287 >>> Training Loss: 0.000000150498337 ### Testing Loss: 0.000000307504877
2023-03-31 21:13:03,287 ===> Epoch[595/1000]
2023-03-31 21:13:27,077 >>> Training Loss: 0.000000151638048 ### Testing Loss: 0.000000305459849
2023-03-31 21:13:27,077 ===> Epoch[596/1000]
2023-03-31 21:13:50,857 >>> Training Loss: 0.000000152366937 ### Testing Loss: 0.000000308298581
2023-03-31 21:13:50,857 ===> Epoch[597/1000]
2023-03-31 21:14:14,637 >>> Training Loss: 0.000000152767853 ### Testing Loss: 0.000000306802349
2023-03-31 21:14:14,637 ===> Epoch[598/1000]
2023-03-31 21:14:38,337 >>> Training Loss: 0.000000157455020 ### Testing Loss: 0.000000306890058
2023-03-31 21:14:38,337 ===> Epoch[599/1000]
2023-03-31 21:15:02,117 >>> Training Loss: 0.000000153137890 ### Testing Loss: 0.000000306474817
2023-03-31 21:15:02,117 ===> Epoch[600/1000]
2023-03-31 21:15:25,927 >>> Training Loss: 0.000000157491712 ### Testing Loss: 0.000000308263054
2023-03-31 21:15:25,947 ===> Epoch[601/1000]
2023-03-31 21:15:49,781 >>> Training Loss: 0.000000153353923 ### Testing Loss: 0.000000305939807
2023-03-31 21:15:49,781 ===> Epoch[602/1000]
2023-03-31 21:16:13,481 >>> Training Loss: 0.000000156674133 ### Testing Loss: 0.000000313530819
2023-03-31 21:16:13,481 ===> Epoch[603/1000]
2023-03-31 21:16:37,291 >>> Training Loss: 0.000000152396396 ### Testing Loss: 0.000000311419797
2023-03-31 21:16:37,291 ===> Epoch[604/1000]
2023-03-31 21:17:01,091 >>> Training Loss: 0.000000156262914 ### Testing Loss: 0.000000303727290
2023-03-31 21:17:01,091 ===> Epoch[605/1000]
2023-03-31 21:17:24,871 >>> Training Loss: 0.000000153041370 ### Testing Loss: 0.000000305765639
2023-03-31 21:17:24,871 ===> Epoch[606/1000]
2023-03-31 21:17:48,661 >>> Training Loss: 0.000000152730351 ### Testing Loss: 0.000000305610911
2023-03-31 21:17:48,661 ===> Epoch[607/1000]
2023-03-31 21:18:12,531 >>> Training Loss: 0.000000151495129 ### Testing Loss: 0.000000310639365
2023-03-31 21:18:12,531 ===> Epoch[608/1000]
2023-03-31 21:18:36,501 >>> Training Loss: 0.000000147641430 ### Testing Loss: 0.000000306024788
2023-03-31 21:18:36,501 ===> Epoch[609/1000]
2023-03-31 21:19:00,321 >>> Training Loss: 0.000000151077430 ### Testing Loss: 0.000000306269044
2023-03-31 21:19:00,321 ===> Epoch[610/1000]
2023-03-31 21:19:25,361 >>> Training Loss: 0.000000151228306 ### Testing Loss: 0.000000304766900
2023-03-31 21:19:25,361 ===> Epoch[611/1000]
2023-03-31 21:19:49,081 >>> Training Loss: 0.000000155006660 ### Testing Loss: 0.000000308885120
2023-03-31 21:19:49,081 ===> Epoch[612/1000]
2023-03-31 21:20:12,841 >>> Training Loss: 0.000000147892194 ### Testing Loss: 0.000000308706689
2023-03-31 21:20:12,841 ===> Epoch[613/1000]
2023-03-31 21:20:36,651 >>> Training Loss: 0.000000150854746 ### Testing Loss: 0.000000307320988
2023-03-31 21:20:36,651 ===> Epoch[614/1000]
2023-03-31 21:21:00,421 >>> Training Loss: 0.000000158537915 ### Testing Loss: 0.000000309044651
2023-03-31 21:21:00,421 ===> Epoch[615/1000]
2023-03-31 21:21:24,161 >>> Training Loss: 0.000000149483370 ### Testing Loss: 0.000000304617089
2023-03-31 21:21:24,161 ===> Epoch[616/1000]
2023-03-31 21:21:47,921 >>> Training Loss: 0.000000149216504 ### Testing Loss: 0.000000307589687
2023-03-31 21:21:47,921 ===> Epoch[617/1000]
2023-03-31 21:22:11,701 >>> Training Loss: 0.000000154711373 ### Testing Loss: 0.000000304942375
2023-03-31 21:22:11,701 ===> Epoch[618/1000]
2023-03-31 21:22:35,501 >>> Training Loss: 0.000000148602155 ### Testing Loss: 0.000000304828802
2023-03-31 21:22:35,501 ===> Epoch[619/1000]
2023-03-31 21:22:59,274 >>> Training Loss: 0.000000150670672 ### Testing Loss: 0.000000337504446
2023-03-31 21:22:59,274 ===> Epoch[620/1000]
2023-03-31 21:23:22,794 >>> Training Loss: 0.000000159189000 ### Testing Loss: 0.000000308774929
2023-03-31 21:23:22,794 ===> Epoch[621/1000]
2023-03-31 21:23:46,235 >>> Training Loss: 0.000000156397235 ### Testing Loss: 0.000000311069698
2023-03-31 21:23:46,235 ===> Epoch[622/1000]
2023-03-31 21:24:09,605 >>> Training Loss: 0.000000148711891 ### Testing Loss: 0.000000306889859
2023-03-31 21:24:09,605 ===> Epoch[623/1000]
2023-03-31 21:24:33,044 >>> Training Loss: 0.000000147661751 ### Testing Loss: 0.000000309471829
2023-03-31 21:24:33,044 ===> Epoch[624/1000]
2023-03-31 21:24:56,524 >>> Training Loss: 0.000000146794108 ### Testing Loss: 0.000000303526406
2023-03-31 21:24:56,524 ===> Epoch[625/1000]
2023-03-31 21:25:19,974 >>> Training Loss: 0.000000145919685 ### Testing Loss: 0.000000305418780
2023-03-31 21:25:19,974 ===> Epoch[626/1000]
2023-03-31 21:25:43,394 >>> Training Loss: 0.000000145464327 ### Testing Loss: 0.000000304990010
2023-03-31 21:25:43,394 ===> Epoch[627/1000]
2023-03-31 21:26:06,894 >>> Training Loss: 0.000000146354068 ### Testing Loss: 0.000000306769834
2023-03-31 21:26:06,894 ===> Epoch[628/1000]
2023-03-31 21:26:30,359 >>> Training Loss: 0.000000147382366 ### Testing Loss: 0.000000307219608
2023-03-31 21:26:30,359 ===> Epoch[629/1000]
2023-03-31 21:26:53,819 >>> Training Loss: 0.000000179908710 ### Testing Loss: 0.000000304276398
2023-03-31 21:26:53,819 ===> Epoch[630/1000]
2023-03-31 21:27:17,289 >>> Training Loss: 0.000000163302317 ### Testing Loss: 0.000000307582667
2023-03-31 21:27:17,289 ===> Epoch[631/1000]
2023-03-31 21:27:40,789 >>> Training Loss: 0.000000161923708 ### Testing Loss: 0.000000307413956
2023-03-31 21:27:40,789 ===> Epoch[632/1000]
2023-03-31 21:28:04,239 >>> Training Loss: 0.000000148595703 ### Testing Loss: 0.000000306551925
2023-03-31 21:28:04,239 ===> Epoch[633/1000]
2023-03-31 21:28:27,739 >>> Training Loss: 0.000000144201522 ### Testing Loss: 0.000000304781707
2023-03-31 21:28:27,739 ===> Epoch[634/1000]
2023-03-31 21:28:51,199 >>> Training Loss: 0.000000155315107 ### Testing Loss: 0.000000303978595
2023-03-31 21:28:51,199 ===> Epoch[635/1000]
2023-03-31 21:29:14,635 >>> Training Loss: 0.000000146788551 ### Testing Loss: 0.000000306464869
2023-03-31 21:29:14,635 ===> Epoch[636/1000]
2023-03-31 21:29:38,060 >>> Training Loss: 0.000000149350157 ### Testing Loss: 0.000000304600519
2023-03-31 21:29:38,060 ===> Epoch[637/1000]
2023-03-31 21:30:01,490 >>> Training Loss: 0.000000153693435 ### Testing Loss: 0.000000307314963
2023-03-31 21:30:01,490 ===> Epoch[638/1000]
2023-03-31 21:30:25,093 >>> Training Loss: 0.000000149885778 ### Testing Loss: 0.000000306212996
2023-03-31 21:30:25,093 ===> Epoch[639/1000]
2023-03-31 21:30:48,563 >>> Training Loss: 0.000000148426352 ### Testing Loss: 0.000000305002089
2023-03-31 21:30:48,563 ===> Epoch[640/1000]
2023-03-31 21:31:12,023 >>> Training Loss: 0.000000147349411 ### Testing Loss: 0.000000305870657
2023-03-31 21:31:12,023 ===> Epoch[641/1000]
2023-03-31 21:31:35,513 >>> Training Loss: 0.000000150428946 ### Testing Loss: 0.000000305962317
2023-03-31 21:31:35,513 ===> Epoch[642/1000]
2023-03-31 21:31:58,993 >>> Training Loss: 0.000000149151262 ### Testing Loss: 0.000000305596330
2023-03-31 21:31:58,993 ===> Epoch[643/1000]
2023-03-31 21:32:22,423 >>> Training Loss: 0.000000147802211 ### Testing Loss: 0.000000304521564
2023-03-31 21:32:22,423 ===> Epoch[644/1000]
2023-03-31 21:32:45,803 >>> Training Loss: 0.000000144909876 ### Testing Loss: 0.000000307818027
2023-03-31 21:32:45,803 ===> Epoch[645/1000]
2023-03-31 21:33:09,193 >>> Training Loss: 0.000000143934344 ### Testing Loss: 0.000000305418894
2023-03-31 21:33:09,193 ===> Epoch[646/1000]
2023-03-31 21:33:32,659 >>> Training Loss: 0.000000158691606 ### Testing Loss: 0.000000307165408
2023-03-31 21:33:32,659 ===> Epoch[647/1000]
2023-03-31 21:33:56,099 >>> Training Loss: 0.000000146704892 ### Testing Loss: 0.000000306826308
2023-03-31 21:33:56,099 ===> Epoch[648/1000]
2023-03-31 21:34:19,709 >>> Training Loss: 0.000000146139044 ### Testing Loss: 0.000000306423487
2023-03-31 21:34:19,709 ===> Epoch[649/1000]
2023-03-31 21:34:44,055 >>> Training Loss: 0.000000145572756 ### Testing Loss: 0.000000304924441
2023-03-31 21:34:44,055 ===> Epoch[650/1000]
2023-03-31 21:35:07,585 >>> Training Loss: 0.000000144770709 ### Testing Loss: 0.000000304189996
2023-03-31 21:35:07,585 ===> Epoch[651/1000]
2023-03-31 21:35:31,105 >>> Training Loss: 0.000000142724261 ### Testing Loss: 0.000000303382677
2023-03-31 21:35:31,105 ===> Epoch[652/1000]
2023-03-31 21:35:54,615 >>> Training Loss: 0.000000144795351 ### Testing Loss: 0.000000306286864
2023-03-31 21:35:54,615 ===> Epoch[653/1000]
2023-03-31 21:36:18,185 >>> Training Loss: 0.000000147059239 ### Testing Loss: 0.000000307539750
2023-03-31 21:36:18,185 ===> Epoch[654/1000]
2023-03-31 21:36:41,666 >>> Training Loss: 0.000000145949443 ### Testing Loss: 0.000000306165049
2023-03-31 21:36:41,666 ===> Epoch[655/1000]
2023-03-31 21:37:05,106 >>> Training Loss: 0.000000150015566 ### Testing Loss: 0.000000305860425
2023-03-31 21:37:05,106 ===> Epoch[656/1000]
2023-03-31 21:37:28,612 >>> Training Loss: 0.000000143291743 ### Testing Loss: 0.000000305838313
2023-03-31 21:37:28,612 ===> Epoch[657/1000]
2023-03-31 21:37:52,052 >>> Training Loss: 0.000000143699310 ### Testing Loss: 0.000000306500226
2023-03-31 21:37:52,052 ===> Epoch[658/1000]
2023-03-31 21:38:15,472 >>> Training Loss: 0.000000141061861 ### Testing Loss: 0.000000309680189
2023-03-31 21:38:15,472 ===> Epoch[659/1000]
2023-03-31 21:38:38,972 >>> Training Loss: 0.000000142358843 ### Testing Loss: 0.000000309097857
2023-03-31 21:38:38,982 ===> Epoch[660/1000]
2023-03-31 21:39:02,492 >>> Training Loss: 0.000000145555873 ### Testing Loss: 0.000000317084641
2023-03-31 21:39:02,492 ===> Epoch[661/1000]
2023-03-31 21:39:26,052 >>> Training Loss: 0.000000153229323 ### Testing Loss: 0.000000305434753
2023-03-31 21:39:26,052 ===> Epoch[662/1000]
2023-03-31 21:39:49,542 >>> Training Loss: 0.000000147233550 ### Testing Loss: 0.000000307266276
2023-03-31 21:39:49,542 ===> Epoch[663/1000]
2023-03-31 21:40:13,033 >>> Training Loss: 0.000000144714136 ### Testing Loss: 0.000000307646189
2023-03-31 21:40:13,033 ===> Epoch[664/1000]
2023-03-31 21:40:36,523 >>> Training Loss: 0.000000144161731 ### Testing Loss: 0.000000304252012
2023-03-31 21:40:36,523 ===> Epoch[665/1000]
2023-03-31 21:40:59,976 >>> Training Loss: 0.000000147306451 ### Testing Loss: 0.000000307506923
2023-03-31 21:40:59,976 ===> Epoch[666/1000]
2023-03-31 21:41:23,431 >>> Training Loss: 0.000000144936109 ### Testing Loss: 0.000000306872550
2023-03-31 21:41:23,431 ===> Epoch[667/1000]
2023-03-31 21:41:46,941 >>> Training Loss: 0.000000145187258 ### Testing Loss: 0.000000307307289
2023-03-31 21:41:46,941 ===> Epoch[668/1000]
2023-03-31 21:42:10,430 >>> Training Loss: 0.000000144532507 ### Testing Loss: 0.000000307074998
2023-03-31 21:42:10,430 ===> Epoch[669/1000]
2023-03-31 21:42:33,911 >>> Training Loss: 0.000000147925491 ### Testing Loss: 0.000000305812108
2023-03-31 21:42:33,911 ===> Epoch[670/1000]
2023-03-31 21:42:57,451 >>> Training Loss: 0.000000145624568 ### Testing Loss: 0.000000307222138
2023-03-31 21:42:57,451 ===> Epoch[671/1000]
2023-03-31 21:43:20,981 >>> Training Loss: 0.000000147536923 ### Testing Loss: 0.000000306622468
2023-03-31 21:43:20,981 ===> Epoch[672/1000]
2023-03-31 21:43:44,391 >>> Training Loss: 0.000000144099801 ### Testing Loss: 0.000000311353375
2023-03-31 21:43:44,391 ===> Epoch[673/1000]
2023-03-31 21:44:07,871 >>> Training Loss: 0.000000144183602 ### Testing Loss: 0.000000308315862
2023-03-31 21:44:07,871 ===> Epoch[674/1000]
2023-03-31 21:44:31,301 >>> Training Loss: 0.000000153675572 ### Testing Loss: 0.000000307072327
2023-03-31 21:44:31,301 ===> Epoch[675/1000]
2023-03-31 21:44:54,784 >>> Training Loss: 0.000000145722453 ### Testing Loss: 0.000000306264639
2023-03-31 21:44:54,784 ===> Epoch[676/1000]
2023-03-31 21:45:18,394 >>> Training Loss: 0.000000143285249 ### Testing Loss: 0.000000305970076
2023-03-31 21:45:18,394 ===> Epoch[677/1000]
2023-03-31 21:45:41,884 >>> Training Loss: 0.000000144160467 ### Testing Loss: 0.000000304539299
2023-03-31 21:45:41,884 ===> Epoch[678/1000]
2023-03-31 21:46:05,334 >>> Training Loss: 0.000000140239294 ### Testing Loss: 0.000000305324221
2023-03-31 21:46:05,334 ===> Epoch[679/1000]
2023-03-31 21:46:28,795 >>> Training Loss: 0.000000139130066 ### Testing Loss: 0.000000305569614
2023-03-31 21:46:28,795 ===> Epoch[680/1000]
2023-03-31 21:46:52,295 >>> Training Loss: 0.000000142730855 ### Testing Loss: 0.000000306528904
2023-03-31 21:46:52,295 ===> Epoch[681/1000]
2023-03-31 21:47:15,835 >>> Training Loss: 0.000000143041873 ### Testing Loss: 0.000000306851831
2023-03-31 21:47:15,835 ===> Epoch[682/1000]
2023-03-31 21:47:39,255 >>> Training Loss: 0.000000151344594 ### Testing Loss: 0.000000304974947
2023-03-31 21:47:39,255 ===> Epoch[683/1000]
2023-03-31 21:48:02,725 >>> Training Loss: 0.000000151600531 ### Testing Loss: 0.000000309774236
2023-03-31 21:48:02,725 ===> Epoch[684/1000]
2023-03-31 21:48:26,233 >>> Training Loss: 0.000000147643973 ### Testing Loss: 0.000000307730090
2023-03-31 21:48:26,233 ===> Epoch[685/1000]
2023-03-31 21:48:49,683 >>> Training Loss: 0.000000147988303 ### Testing Loss: 0.000000309422518
2023-03-31 21:48:49,683 ===> Epoch[686/1000]
2023-03-31 21:49:13,183 >>> Training Loss: 0.000000145077379 ### Testing Loss: 0.000000314938205
2023-03-31 21:49:13,183 ===> Epoch[687/1000]
2023-03-31 21:49:37,564 >>> Training Loss: 0.000000143950714 ### Testing Loss: 0.000000308549261
2023-03-31 21:49:37,564 ===> Epoch[688/1000]
2023-03-31 21:50:01,194 >>> Training Loss: 0.000000140078882 ### Testing Loss: 0.000000304500986
2023-03-31 21:50:01,194 ===> Epoch[689/1000]
2023-03-31 21:50:24,834 >>> Training Loss: 0.000000141617406 ### Testing Loss: 0.000000305163013
2023-03-31 21:50:24,834 ===> Epoch[690/1000]
2023-03-31 21:50:48,474 >>> Training Loss: 0.000000141761035 ### Testing Loss: 0.000000306776144
2023-03-31 21:50:48,474 ===> Epoch[691/1000]
2023-03-31 21:51:12,174 >>> Training Loss: 0.000000166585536 ### Testing Loss: 0.000000305919258
2023-03-31 21:51:12,174 ===> Epoch[692/1000]
2023-03-31 21:51:35,824 >>> Training Loss: 0.000000138678814 ### Testing Loss: 0.000000305389534
2023-03-31 21:51:35,824 ===> Epoch[693/1000]
2023-03-31 21:51:59,401 >>> Training Loss: 0.000000138344788 ### Testing Loss: 0.000000305099491
2023-03-31 21:51:59,401 ===> Epoch[694/1000]
2023-03-31 21:52:23,011 >>> Training Loss: 0.000000139774940 ### Testing Loss: 0.000000306287177
2023-03-31 21:52:23,011 ===> Epoch[695/1000]
2023-03-31 21:52:46,601 >>> Training Loss: 0.000000139641713 ### Testing Loss: 0.000000306303406
2023-03-31 21:52:46,601 ===> Epoch[696/1000]
2023-03-31 21:53:10,231 >>> Training Loss: 0.000000141238075 ### Testing Loss: 0.000000306839979
2023-03-31 21:53:10,231 ===> Epoch[697/1000]
2023-03-31 21:53:33,891 >>> Training Loss: 0.000000141110291 ### Testing Loss: 0.000000305403432
2023-03-31 21:53:33,891 ===> Epoch[698/1000]
2023-03-31 21:53:57,431 >>> Training Loss: 0.000000140028291 ### Testing Loss: 0.000000307040267
2023-03-31 21:53:57,431 ===> Epoch[699/1000]
2023-03-31 21:54:21,131 >>> Training Loss: 0.000000150081476 ### Testing Loss: 0.000000306162804
2023-03-31 21:54:21,131 ===> Epoch[700/1000]
2023-03-31 21:54:44,811 >>> Training Loss: 0.000000144454404 ### Testing Loss: 0.000000307245870
2023-03-31 21:54:44,831 ===> Epoch[701/1000]
2023-03-31 21:55:08,571 >>> Training Loss: 0.000000140186955 ### Testing Loss: 0.000000306067619
2023-03-31 21:55:08,571 ===> Epoch[702/1000]
2023-03-31 21:55:32,204 >>> Training Loss: 0.000000139551020 ### Testing Loss: 0.000000304100979
2023-03-31 21:55:32,204 ===> Epoch[703/1000]
2023-03-31 21:55:55,984 >>> Training Loss: 0.000000139682470 ### Testing Loss: 0.000000305043187
2023-03-31 21:55:55,984 ===> Epoch[704/1000]
2023-03-31 21:56:19,735 >>> Training Loss: 0.000000143808961 ### Testing Loss: 0.000000308129302
2023-03-31 21:56:19,735 ===> Epoch[705/1000]
2023-03-31 21:56:43,545 >>> Training Loss: 0.000000145043046 ### Testing Loss: 0.000000310414123
2023-03-31 21:56:43,545 ===> Epoch[706/1000]
2023-03-31 21:57:07,285 >>> Training Loss: 0.000000138472529 ### Testing Loss: 0.000000309723788
2023-03-31 21:57:07,285 ===> Epoch[707/1000]
2023-03-31 21:57:30,994 >>> Training Loss: 0.000000137989872 ### Testing Loss: 0.000000306252161
2023-03-31 21:57:30,994 ===> Epoch[708/1000]
2023-03-31 21:57:54,864 >>> Training Loss: 0.000000140448833 ### Testing Loss: 0.000000306621331
2023-03-31 21:57:54,864 ===> Epoch[709/1000]
2023-03-31 21:58:18,635 >>> Training Loss: 0.000000137297576 ### Testing Loss: 0.000000304014122
2023-03-31 21:58:18,635 ===> Epoch[710/1000]
2023-03-31 21:58:42,445 >>> Training Loss: 0.000000148345691 ### Testing Loss: 0.000000306529387
2023-03-31 21:58:42,445 ===> Epoch[711/1000]
2023-03-31 21:59:06,260 >>> Training Loss: 0.000000139026170 ### Testing Loss: 0.000000304579487
2023-03-31 21:59:06,260 ===> Epoch[712/1000]
2023-03-31 21:59:30,030 >>> Training Loss: 0.000000139289284 ### Testing Loss: 0.000000304844406
2023-03-31 21:59:30,030 ===> Epoch[713/1000]
2023-03-31 21:59:53,809 >>> Training Loss: 0.000000141609831 ### Testing Loss: 0.000000305556227
2023-03-31 21:59:53,809 ===> Epoch[714/1000]
2023-03-31 22:00:17,639 >>> Training Loss: 0.000000143472647 ### Testing Loss: 0.000000308850986
2023-03-31 22:00:17,639 ===> Epoch[715/1000]
2023-03-31 22:00:41,419 >>> Training Loss: 0.000000147290663 ### Testing Loss: 0.000000314125344
2023-03-31 22:00:41,419 ===> Epoch[716/1000]
2023-03-31 22:01:05,269 >>> Training Loss: 0.000000144551237 ### Testing Loss: 0.000000329894561
2023-03-31 22:01:05,269 ===> Epoch[717/1000]
2023-03-31 22:01:29,060 >>> Training Loss: 0.000000139154551 ### Testing Loss: 0.000000306658592
2023-03-31 22:01:29,060 ===> Epoch[718/1000]
2023-03-31 22:01:52,910 >>> Training Loss: 0.000000147188089 ### Testing Loss: 0.000000311369661
2023-03-31 22:01:52,910 ===> Epoch[719/1000]
2023-03-31 22:02:16,680 >>> Training Loss: 0.000000144982579 ### Testing Loss: 0.000000318462099
2023-03-31 22:02:16,680 ===> Epoch[720/1000]
2023-03-31 22:02:40,533 >>> Training Loss: 0.000000143709997 ### Testing Loss: 0.000000309162431
2023-03-31 22:02:40,533 ===> Epoch[721/1000]
2023-03-31 22:03:04,323 >>> Training Loss: 0.000000139526023 ### Testing Loss: 0.000000307556348
2023-03-31 22:03:04,323 ===> Epoch[722/1000]
2023-03-31 22:03:28,103 >>> Training Loss: 0.000000146128741 ### Testing Loss: 0.000000306343281
2023-03-31 22:03:28,103 ===> Epoch[723/1000]
2023-03-31 22:03:51,883 >>> Training Loss: 0.000000140817008 ### Testing Loss: 0.000000306349790
2023-03-31 22:03:51,883 ===> Epoch[724/1000]
2023-03-31 22:04:15,683 >>> Training Loss: 0.000000139073975 ### Testing Loss: 0.000000305053504
2023-03-31 22:04:15,683 ===> Epoch[725/1000]
2023-03-31 22:04:40,416 >>> Training Loss: 0.000000174374819 ### Testing Loss: 0.000000307062464
2023-03-31 22:04:40,416 ===> Epoch[726/1000]
2023-03-31 22:05:04,236 >>> Training Loss: 0.000000142222120 ### Testing Loss: 0.000000305176314
2023-03-31 22:05:04,236 ===> Epoch[727/1000]
2023-03-31 22:05:28,116 >>> Training Loss: 0.000000136581249 ### Testing Loss: 0.000000304981768
2023-03-31 22:05:28,116 ===> Epoch[728/1000]
2023-03-31 22:05:52,016 >>> Training Loss: 0.000000137981985 ### Testing Loss: 0.000000306096410
2023-03-31 22:05:52,016 ===> Epoch[729/1000]
2023-03-31 22:06:15,940 >>> Training Loss: 0.000000135939999 ### Testing Loss: 0.000000305650445
2023-03-31 22:06:15,940 ===> Epoch[730/1000]
2023-03-31 22:06:39,810 >>> Training Loss: 0.000000135114220 ### Testing Loss: 0.000000305119784
2023-03-31 22:06:39,810 ===> Epoch[731/1000]
2023-03-31 22:07:03,690 >>> Training Loss: 0.000000136774148 ### Testing Loss: 0.000000311459246
2023-03-31 22:07:03,690 ===> Epoch[732/1000]
2023-03-31 22:07:27,650 >>> Training Loss: 0.000000135441084 ### Testing Loss: 0.000000306214247
2023-03-31 22:07:27,650 ===> Epoch[733/1000]
2023-03-31 22:07:51,420 >>> Training Loss: 0.000000142061225 ### Testing Loss: 0.000000303221299
2023-03-31 22:07:51,420 ===> Epoch[734/1000]
2023-03-31 22:08:15,310 >>> Training Loss: 0.000000135461491 ### Testing Loss: 0.000000307445163
2023-03-31 22:08:15,310 ===> Epoch[735/1000]
2023-03-31 22:08:39,041 >>> Training Loss: 0.000000139042257 ### Testing Loss: 0.000000304653156
2023-03-31 22:08:39,041 ===> Epoch[736/1000]
2023-03-31 22:09:02,671 >>> Training Loss: 0.000000141407440 ### Testing Loss: 0.000000313764019
2023-03-31 22:09:02,671 ===> Epoch[737/1000]
2023-03-31 22:09:26,241 >>> Training Loss: 0.000000145315695 ### Testing Loss: 0.000000306825996
2023-03-31 22:09:26,241 ===> Epoch[738/1000]
2023-03-31 22:09:49,825 >>> Training Loss: 0.000000150241874 ### Testing Loss: 0.000000309117183
2023-03-31 22:09:49,825 ===> Epoch[739/1000]
2023-03-31 22:10:13,405 >>> Training Loss: 0.000000141623445 ### Testing Loss: 0.000000308122850
2023-03-31 22:10:13,405 ===> Epoch[740/1000]
2023-03-31 22:10:37,075 >>> Training Loss: 0.000000143023712 ### Testing Loss: 0.000000305192515
2023-03-31 22:10:37,075 ===> Epoch[741/1000]
2023-03-31 22:11:00,656 >>> Training Loss: 0.000000136666472 ### Testing Loss: 0.000000305622564
2023-03-31 22:11:00,656 ===> Epoch[742/1000]
2023-03-31 22:11:24,286 >>> Training Loss: 0.000000135790458 ### Testing Loss: 0.000000306741583
2023-03-31 22:11:24,286 ===> Epoch[743/1000]
2023-03-31 22:11:47,896 >>> Training Loss: 0.000000138198729 ### Testing Loss: 0.000000308575608
2023-03-31 22:11:47,896 ===> Epoch[744/1000]
2023-03-31 22:12:11,466 >>> Training Loss: 0.000000157229778 ### Testing Loss: 0.000000307559105
2023-03-31 22:12:11,466 ===> Epoch[745/1000]
2023-03-31 22:12:35,085 >>> Training Loss: 0.000000153351181 ### Testing Loss: 0.000000309268785
2023-03-31 22:12:35,085 ===> Epoch[746/1000]
2023-03-31 22:12:58,685 >>> Training Loss: 0.000000141732329 ### Testing Loss: 0.000000306340780
2023-03-31 22:12:58,685 ===> Epoch[747/1000]
2023-03-31 22:13:22,221 >>> Training Loss: 0.000000145979371 ### Testing Loss: 0.000000305835869
2023-03-31 22:13:22,221 ===> Epoch[748/1000]
2023-03-31 22:13:45,762 >>> Training Loss: 0.000000137060738 ### Testing Loss: 0.000000307331192
2023-03-31 22:13:45,762 ===> Epoch[749/1000]
2023-03-31 22:14:09,322 >>> Training Loss: 0.000000135544425 ### Testing Loss: 0.000000305106283
2023-03-31 22:14:09,322 ===> Epoch[750/1000]
2023-03-31 22:14:32,872 >>> Training Loss: 0.000000147544768 ### Testing Loss: 0.000000312929757
2023-03-31 22:14:32,872 ===> Epoch[751/1000]
2023-03-31 22:14:56,452 >>> Training Loss: 0.000000139913098 ### Testing Loss: 0.000000310520761
2023-03-31 22:14:56,452 ===> Epoch[752/1000]
2023-03-31 22:15:20,052 >>> Training Loss: 0.000000136701075 ### Testing Loss: 0.000000306533281
2023-03-31 22:15:20,052 ===> Epoch[753/1000]
2023-03-31 22:15:43,542 >>> Training Loss: 0.000000136143427 ### Testing Loss: 0.000000305586525
2023-03-31 22:15:43,542 ===> Epoch[754/1000]
2023-03-31 22:16:07,202 >>> Training Loss: 0.000000137843955 ### Testing Loss: 0.000000308314554
2023-03-31 22:16:07,202 ===> Epoch[755/1000]
2023-03-31 22:16:30,802 >>> Training Loss: 0.000000142210610 ### Testing Loss: 0.000000305311232
2023-03-31 22:16:30,802 ===> Epoch[756/1000]
2023-03-31 22:16:54,420 >>> Training Loss: 0.000000141750974 ### Testing Loss: 0.000000307423363
2023-03-31 22:16:54,420 ===> Epoch[757/1000]
2023-03-31 22:17:17,990 >>> Training Loss: 0.000000138796850 ### Testing Loss: 0.000000307934016
2023-03-31 22:17:17,990 ===> Epoch[758/1000]
2023-03-31 22:17:41,690 >>> Training Loss: 0.000000140279496 ### Testing Loss: 0.000000308661100
2023-03-31 22:17:41,690 ===> Epoch[759/1000]
2023-03-31 22:18:05,290 >>> Training Loss: 0.000000136939747 ### Testing Loss: 0.000000307982845
2023-03-31 22:18:05,290 ===> Epoch[760/1000]
2023-03-31 22:18:29,080 >>> Training Loss: 0.000000142117031 ### Testing Loss: 0.000000305384248
2023-03-31 22:18:29,080 ===> Epoch[761/1000]
2023-03-31 22:18:52,811 >>> Training Loss: 0.000000141939054 ### Testing Loss: 0.000000312824767
2023-03-31 22:18:52,811 ===> Epoch[762/1000]
2023-03-31 22:19:16,581 >>> Training Loss: 0.000000142379974 ### Testing Loss: 0.000000304665804
2023-03-31 22:19:16,581 ===> Epoch[763/1000]
2023-03-31 22:19:41,313 >>> Training Loss: 0.000000135603543 ### Testing Loss: 0.000000306224791
2023-03-31 22:19:41,313 ===> Epoch[764/1000]
2023-03-31 22:20:05,093 >>> Training Loss: 0.000000133811440 ### Testing Loss: 0.000000306417377
2023-03-31 22:20:05,093 ===> Epoch[765/1000]
2023-03-31 22:20:28,919 >>> Training Loss: 0.000000137933810 ### Testing Loss: 0.000000305065470
2023-03-31 22:20:28,919 ===> Epoch[766/1000]
2023-03-31 22:20:52,739 >>> Training Loss: 0.000000151807683 ### Testing Loss: 0.000000325556982
2023-03-31 22:20:52,739 ===> Epoch[767/1000]
2023-03-31 22:21:16,099 >>> Training Loss: 0.000000142082087 ### Testing Loss: 0.000000306224450
2023-03-31 22:21:16,099 ===> Epoch[768/1000]
2023-03-31 22:21:39,479 >>> Training Loss: 0.000000140620386 ### Testing Loss: 0.000000308917578
2023-03-31 22:21:39,479 ===> Epoch[769/1000]
2023-03-31 22:22:02,889 >>> Training Loss: 0.000000140427645 ### Testing Loss: 0.000000304473872
2023-03-31 22:22:02,889 ===> Epoch[770/1000]
2023-03-31 22:22:26,389 >>> Training Loss: 0.000000137138500 ### Testing Loss: 0.000000303539423
2023-03-31 22:22:26,389 ===> Epoch[771/1000]
2023-03-31 22:22:49,849 >>> Training Loss: 0.000000135170794 ### Testing Loss: 0.000000304454176
2023-03-31 22:22:49,849 ===> Epoch[772/1000]
2023-03-31 22:23:13,290 >>> Training Loss: 0.000000135585836 ### Testing Loss: 0.000000306562725
2023-03-31 22:23:13,290 ===> Epoch[773/1000]
2023-03-31 22:23:36,710 >>> Training Loss: 0.000000134216279 ### Testing Loss: 0.000000305722239
2023-03-31 22:23:36,710 ===> Epoch[774/1000]
2023-03-31 22:24:00,194 >>> Training Loss: 0.000000141948973 ### Testing Loss: 0.000000308197286
2023-03-31 22:24:00,194 ===> Epoch[775/1000]
2023-03-31 22:24:23,664 >>> Training Loss: 0.000000140891515 ### Testing Loss: 0.000000307954252
2023-03-31 22:24:23,664 ===> Epoch[776/1000]
2023-03-31 22:24:47,174 >>> Training Loss: 0.000000141613697 ### Testing Loss: 0.000000306136201
2023-03-31 22:24:47,174 ===> Epoch[777/1000]
2023-03-31 22:25:10,624 >>> Training Loss: 0.000000135499874 ### Testing Loss: 0.000000305669431
2023-03-31 22:25:10,624 ===> Epoch[778/1000]
2023-03-31 22:25:33,994 >>> Training Loss: 0.000000138487053 ### Testing Loss: 0.000000305564981
2023-03-31 22:25:33,994 ===> Epoch[779/1000]
2023-03-31 22:25:57,394 >>> Training Loss: 0.000000132370687 ### Testing Loss: 0.000000307372460
2023-03-31 22:25:57,394 ===> Epoch[780/1000]
2023-03-31 22:26:20,783 >>> Training Loss: 0.000000133312923 ### Testing Loss: 0.000000306718874
2023-03-31 22:26:20,793 ===> Epoch[781/1000]
2023-03-31 22:26:44,213 >>> Training Loss: 0.000000136257086 ### Testing Loss: 0.000000308335586
2023-03-31 22:26:44,213 ===> Epoch[782/1000]
2023-03-31 22:27:07,674 >>> Training Loss: 0.000000135461306 ### Testing Loss: 0.000000306637617
2023-03-31 22:27:07,674 ===> Epoch[783/1000]
2023-03-31 22:27:31,029 >>> Training Loss: 0.000000133433787 ### Testing Loss: 0.000000305570893
2023-03-31 22:27:31,029 ===> Epoch[784/1000]
2023-03-31 22:27:54,578 >>> Training Loss: 0.000000131690413 ### Testing Loss: 0.000000313012833
2023-03-31 22:27:54,578 ===> Epoch[785/1000]
2023-03-31 22:28:17,958 >>> Training Loss: 0.000000135754092 ### Testing Loss: 0.000000309687636
2023-03-31 22:28:17,958 ===> Epoch[786/1000]
2023-03-31 22:28:41,348 >>> Training Loss: 0.000000138997578 ### Testing Loss: 0.000000311327028
2023-03-31 22:28:41,348 ===> Epoch[787/1000]
2023-03-31 22:29:04,778 >>> Training Loss: 0.000000136225083 ### Testing Loss: 0.000000306919361
2023-03-31 22:29:04,778 ===> Epoch[788/1000]
2023-03-31 22:29:28,148 >>> Training Loss: 0.000000140388465 ### Testing Loss: 0.000000307905594
2023-03-31 22:29:28,148 ===> Epoch[789/1000]
2023-03-31 22:29:51,538 >>> Training Loss: 0.000000135052971 ### Testing Loss: 0.000000308709332
2023-03-31 22:29:51,538 ===> Epoch[790/1000]
2023-03-31 22:30:14,948 >>> Training Loss: 0.000000192223268 ### Testing Loss: 0.000000305200274
2023-03-31 22:30:14,948 ===> Epoch[791/1000]
2023-03-31 22:30:38,388 >>> Training Loss: 0.000000142265463 ### Testing Loss: 0.000000305791019
2023-03-31 22:30:38,388 ===> Epoch[792/1000]
2023-03-31 22:31:01,773 >>> Training Loss: 0.000000134111446 ### Testing Loss: 0.000000308780301
2023-03-31 22:31:01,773 ===> Epoch[793/1000]
2023-03-31 22:31:25,133 >>> Training Loss: 0.000000146903076 ### Testing Loss: 0.000000305541761
2023-03-31 22:31:25,133 ===> Epoch[794/1000]
2023-03-31 22:31:48,623 >>> Training Loss: 0.000000146048947 ### Testing Loss: 0.000000307073464
2023-03-31 22:31:48,623 ===> Epoch[795/1000]
2023-03-31 22:32:12,103 >>> Training Loss: 0.000000139785641 ### Testing Loss: 0.000000306200064
2023-03-31 22:32:12,103 ===> Epoch[796/1000]
2023-03-31 22:32:35,443 >>> Training Loss: 0.000000146632985 ### Testing Loss: 0.000000316069077
2023-03-31 22:32:35,443 ===> Epoch[797/1000]
2023-03-31 22:32:58,943 >>> Training Loss: 0.000000138066895 ### Testing Loss: 0.000000309870956
2023-03-31 22:32:58,943 ===> Epoch[798/1000]
2023-03-31 22:33:22,433 >>> Training Loss: 0.000000140562264 ### Testing Loss: 0.000000305217043
2023-03-31 22:33:22,433 ===> Epoch[799/1000]
2023-03-31 22:33:45,963 >>> Training Loss: 0.000000136888801 ### Testing Loss: 0.000000305266383
2023-03-31 22:33:45,963 ===> Epoch[800/1000]
2023-03-31 22:34:09,753 >>> Training Loss: 0.000000136222823 ### Testing Loss: 0.000000306148024
2023-03-31 22:34:09,773 ===> Epoch[801/1000]
2023-03-31 22:34:34,488 >>> Training Loss: 0.000000135484044 ### Testing Loss: 0.000000310719713
2023-03-31 22:34:34,488 ===> Epoch[802/1000]
2023-03-31 22:34:58,218 >>> Training Loss: 0.000000138352092 ### Testing Loss: 0.000000307709513
2023-03-31 22:34:58,218 ===> Epoch[803/1000]
2023-03-31 22:35:21,978 >>> Training Loss: 0.000000132237616 ### Testing Loss: 0.000000309607401
2023-03-31 22:35:21,978 ===> Epoch[804/1000]
2023-03-31 22:35:45,768 >>> Training Loss: 0.000000133021217 ### Testing Loss: 0.000000306767163
2023-03-31 22:35:45,768 ===> Epoch[805/1000]
2023-03-31 22:36:09,628 >>> Training Loss: 0.000000146479280 ### Testing Loss: 0.000000308074931
2023-03-31 22:36:09,628 ===> Epoch[806/1000]
2023-03-31 22:36:33,298 >>> Training Loss: 0.000000136170371 ### Testing Loss: 0.000000306349563
2023-03-31 22:36:33,298 ===> Epoch[807/1000]
2023-03-31 22:36:56,918 >>> Training Loss: 0.000000134254819 ### Testing Loss: 0.000000307504877
2023-03-31 22:36:56,918 ===> Epoch[808/1000]
2023-03-31 22:37:20,628 >>> Training Loss: 0.000000132623370 ### Testing Loss: 0.000000321965416
2023-03-31 22:37:20,628 ===> Epoch[809/1000]
2023-03-31 22:37:44,338 >>> Training Loss: 0.000000135417153 ### Testing Loss: 0.000000307673275
2023-03-31 22:37:44,338 ===> Epoch[810/1000]
2023-03-31 22:38:07,932 >>> Training Loss: 0.000000139252833 ### Testing Loss: 0.000000321242595
2023-03-31 22:38:07,932 ===> Epoch[811/1000]
2023-03-31 22:38:31,562 >>> Training Loss: 0.000000144015701 ### Testing Loss: 0.000000313751826
2023-03-31 22:38:31,562 ===> Epoch[812/1000]
2023-03-31 22:38:55,252 >>> Training Loss: 0.000000139523578 ### Testing Loss: 0.000000311121880
2023-03-31 22:38:55,252 ===> Epoch[813/1000]
2023-03-31 22:39:19,022 >>> Training Loss: 0.000000138268874 ### Testing Loss: 0.000000310315357
2023-03-31 22:39:19,022 ===> Epoch[814/1000]
2023-03-31 22:39:42,692 >>> Training Loss: 0.000000132860549 ### Testing Loss: 0.000000306264241
2023-03-31 22:39:42,692 ===> Epoch[815/1000]
2023-03-31 22:40:06,462 >>> Training Loss: 0.000000136555698 ### Testing Loss: 0.000000308059185
2023-03-31 22:40:06,462 ===> Epoch[816/1000]
2023-03-31 22:40:30,152 >>> Training Loss: 0.000000130356483 ### Testing Loss: 0.000000308824042
2023-03-31 22:40:30,152 ===> Epoch[817/1000]
2023-03-31 22:40:53,802 >>> Training Loss: 0.000000131289724 ### Testing Loss: 0.000000302892147
2023-03-31 22:40:53,802 ===> Epoch[818/1000]
2023-03-31 22:41:17,322 >>> Training Loss: 0.000000128766914 ### Testing Loss: 0.000000305412470
2023-03-31 22:41:17,322 ===> Epoch[819/1000]
2023-03-31 22:41:40,948 >>> Training Loss: 0.000000130609720 ### Testing Loss: 0.000000305258880
2023-03-31 22:41:40,948 ===> Epoch[820/1000]
2023-03-31 22:42:05,248 >>> Training Loss: 0.000000129039606 ### Testing Loss: 0.000000307719461
2023-03-31 22:42:05,248 ===> Epoch[821/1000]
2023-03-31 22:42:29,558 >>> Training Loss: 0.000000136664326 ### Testing Loss: 0.000000308473801
2023-03-31 22:42:29,558 ===> Epoch[822/1000]
2023-03-31 22:42:53,868 >>> Training Loss: 0.000000131466052 ### Testing Loss: 0.000000304347680
2023-03-31 22:42:53,868 ===> Epoch[823/1000]
2023-03-31 22:43:18,248 >>> Training Loss: 0.000000135723838 ### Testing Loss: 0.000000306079244
2023-03-31 22:43:18,248 ===> Epoch[824/1000]
2023-03-31 22:43:42,768 >>> Training Loss: 0.000000137182667 ### Testing Loss: 0.000000306988994
2023-03-31 22:43:42,768 ===> Epoch[825/1000]
2023-03-31 22:44:07,288 >>> Training Loss: 0.000000132707953 ### Testing Loss: 0.000000309431329
2023-03-31 22:44:07,288 ===> Epoch[826/1000]
2023-03-31 22:44:31,738 >>> Training Loss: 0.000000136614730 ### Testing Loss: 0.000000307253458
2023-03-31 22:44:31,738 ===> Epoch[827/1000]
2023-03-31 22:44:56,102 >>> Training Loss: 0.000000131547537 ### Testing Loss: 0.000000312287341
2023-03-31 22:44:56,102 ===> Epoch[828/1000]
2023-03-31 22:45:20,542 >>> Training Loss: 0.000000130752383 ### Testing Loss: 0.000000305593574
2023-03-31 22:45:20,542 ===> Epoch[829/1000]
2023-03-31 22:45:44,872 >>> Training Loss: 0.000000129706024 ### Testing Loss: 0.000000305744805
2023-03-31 22:45:44,872 ===> Epoch[830/1000]
2023-03-31 22:46:09,242 >>> Training Loss: 0.000000132294502 ### Testing Loss: 0.000000304685585
2023-03-31 22:46:09,242 ===> Epoch[831/1000]
2023-03-31 22:46:33,632 >>> Training Loss: 0.000000129353310 ### Testing Loss: 0.000000305227644
2023-03-31 22:46:33,632 ===> Epoch[832/1000]
2023-03-31 22:46:58,012 >>> Training Loss: 0.000000131421814 ### Testing Loss: 0.000000307190106
2023-03-31 22:46:58,012 ===> Epoch[833/1000]
2023-03-31 22:47:22,512 >>> Training Loss: 0.000000139390139 ### Testing Loss: 0.000000305182169
2023-03-31 22:47:22,512 ===> Epoch[834/1000]
2023-03-31 22:47:46,872 >>> Training Loss: 0.000000132893362 ### Testing Loss: 0.000000309037858
2023-03-31 22:47:46,872 ===> Epoch[835/1000]
2023-03-31 22:48:11,122 >>> Training Loss: 0.000000138272952 ### Testing Loss: 0.000000309263072
2023-03-31 22:48:11,122 ===> Epoch[836/1000]
2023-03-31 22:48:35,289 >>> Training Loss: 0.000000129897856 ### Testing Loss: 0.000000308051426
2023-03-31 22:48:35,289 ===> Epoch[837/1000]
2023-03-31 22:48:59,059 >>> Training Loss: 0.000000127529916 ### Testing Loss: 0.000000306537146
2023-03-31 22:48:59,059 ===> Epoch[838/1000]
2023-03-31 22:49:23,979 >>> Training Loss: 0.000000127365936 ### Testing Loss: 0.000000306915297
2023-03-31 22:49:23,979 ===> Epoch[839/1000]
2023-03-31 22:49:47,679 >>> Training Loss: 0.000000130496417 ### Testing Loss: 0.000000307265395
2023-03-31 22:49:47,679 ===> Epoch[840/1000]
2023-03-31 22:50:11,409 >>> Training Loss: 0.000000130746869 ### Testing Loss: 0.000000304040071
2023-03-31 22:50:11,409 ===> Epoch[841/1000]
2023-03-31 22:50:35,229 >>> Training Loss: 0.000000131118057 ### Testing Loss: 0.000000310144088
2023-03-31 22:50:35,229 ===> Epoch[842/1000]
2023-03-31 22:50:59,090 >>> Training Loss: 0.000000195096106 ### Testing Loss: 0.000000312699854
2023-03-31 22:50:59,090 ===> Epoch[843/1000]
2023-03-31 22:51:22,900 >>> Training Loss: 0.000000146481469 ### Testing Loss: 0.000000310830643
2023-03-31 22:51:22,900 ===> Epoch[844/1000]
2023-03-31 22:51:46,690 >>> Training Loss: 0.000000141343605 ### Testing Loss: 0.000000307116778
2023-03-31 22:51:46,690 ===> Epoch[845/1000]
2023-03-31 22:52:10,445 >>> Training Loss: 0.000000131372772 ### Testing Loss: 0.000000306616926
2023-03-31 22:52:10,445 ===> Epoch[846/1000]
2023-03-31 22:52:34,145 >>> Training Loss: 0.000000128409980 ### Testing Loss: 0.000000306612435
2023-03-31 22:52:34,145 ===> Epoch[847/1000]
2023-03-31 22:52:57,894 >>> Training Loss: 0.000000126956337 ### Testing Loss: 0.000000307304873
2023-03-31 22:52:57,894 ===> Epoch[848/1000]
2023-03-31 22:53:21,704 >>> Training Loss: 0.000000127501778 ### Testing Loss: 0.000000305113446
2023-03-31 22:53:21,704 ===> Epoch[849/1000]
2023-03-31 22:53:45,444 >>> Training Loss: 0.000000128248900 ### Testing Loss: 0.000000305223466
2023-03-31 22:53:45,444 ===> Epoch[850/1000]
2023-03-31 22:54:09,185 >>> Training Loss: 0.000000127154465 ### Testing Loss: 0.000000307448715
2023-03-31 22:54:09,185 ===> Epoch[851/1000]
2023-03-31 22:54:32,855 >>> Training Loss: 0.000000129850463 ### Testing Loss: 0.000000305243020
2023-03-31 22:54:32,855 ===> Epoch[852/1000]
2023-03-31 22:54:56,555 >>> Training Loss: 0.000000142577235 ### Testing Loss: 0.000000305562708
2023-03-31 22:54:56,555 ===> Epoch[853/1000]
2023-03-31 22:55:20,304 >>> Training Loss: 0.000000135527500 ### Testing Loss: 0.000000307583463
2023-03-31 22:55:20,304 ===> Epoch[854/1000]
2023-03-31 22:55:44,049 >>> Training Loss: 0.000000132171706 ### Testing Loss: 0.000000305624326
2023-03-31 22:55:44,049 ===> Epoch[855/1000]
2023-03-31 22:56:07,759 >>> Training Loss: 0.000000129966068 ### Testing Loss: 0.000000308500120
2023-03-31 22:56:07,759 ===> Epoch[856/1000]
2023-03-31 22:56:31,489 >>> Training Loss: 0.000000129362149 ### Testing Loss: 0.000000307949563
2023-03-31 22:56:31,489 ===> Epoch[857/1000]
2023-03-31 22:56:55,179 >>> Training Loss: 0.000000129041439 ### Testing Loss: 0.000000305125695
2023-03-31 22:56:55,179 ===> Epoch[858/1000]
2023-03-31 22:57:18,929 >>> Training Loss: 0.000000183855235 ### Testing Loss: 0.000000306835290
2023-03-31 22:57:18,929 ===> Epoch[859/1000]
2023-03-31 22:57:42,659 >>> Training Loss: 0.000000135164527 ### Testing Loss: 0.000000305979427
2023-03-31 22:57:42,659 ===> Epoch[860/1000]
2023-03-31 22:58:06,440 >>> Training Loss: 0.000000132587601 ### Testing Loss: 0.000000308137686
2023-03-31 22:58:06,440 ===> Epoch[861/1000]
2023-03-31 22:58:30,180 >>> Training Loss: 0.000000129592422 ### Testing Loss: 0.000000305118704
2023-03-31 22:58:30,180 ===> Epoch[862/1000]
2023-03-31 22:58:54,019 >>> Training Loss: 0.000000127384880 ### Testing Loss: 0.000000306980581
2023-03-31 22:58:54,019 ===> Epoch[863/1000]
2023-03-31 22:59:17,775 >>> Training Loss: 0.000000127936786 ### Testing Loss: 0.000000307269090
2023-03-31 22:59:17,775 ===> Epoch[864/1000]
2023-03-31 22:59:41,485 >>> Training Loss: 0.000000128244253 ### Testing Loss: 0.000000306073190
2023-03-31 22:59:41,485 ===> Epoch[865/1000]
2023-03-31 23:00:05,324 >>> Training Loss: 0.000000126341760 ### Testing Loss: 0.000000308161674
2023-03-31 23:00:05,324 ===> Epoch[866/1000]
2023-03-31 23:00:29,044 >>> Training Loss: 0.000000126504631 ### Testing Loss: 0.000000306634035
2023-03-31 23:00:29,044 ===> Epoch[867/1000]
2023-03-31 23:00:52,784 >>> Training Loss: 0.000000128375873 ### Testing Loss: 0.000000306015181
2023-03-31 23:00:52,784 ===> Epoch[868/1000]
2023-03-31 23:01:16,534 >>> Training Loss: 0.000000138465822 ### Testing Loss: 0.000000312479386
2023-03-31 23:01:16,534 ===> Epoch[869/1000]
2023-03-31 23:01:40,325 >>> Training Loss: 0.000000131377391 ### Testing Loss: 0.000000306642590
2023-03-31 23:01:40,325 ===> Epoch[870/1000]
2023-03-31 23:02:04,175 >>> Training Loss: 0.000000130394810 ### Testing Loss: 0.000000306313751
2023-03-31 23:02:04,175 ===> Epoch[871/1000]
2023-03-31 23:02:27,945 >>> Training Loss: 0.000000129539544 ### Testing Loss: 0.000000311701285
2023-03-31 23:02:27,945 ===> Epoch[872/1000]
2023-03-31 23:02:51,677 >>> Training Loss: 0.000000131875225 ### Testing Loss: 0.000000307407959
2023-03-31 23:02:51,677 ===> Epoch[873/1000]
2023-03-31 23:03:15,478 >>> Training Loss: 0.000000133956817 ### Testing Loss: 0.000000310593009
2023-03-31 23:03:15,478 ===> Epoch[874/1000]
2023-03-31 23:03:39,338 >>> Training Loss: 0.000000134146362 ### Testing Loss: 0.000000310653490
2023-03-31 23:03:39,338 ===> Epoch[875/1000]
2023-03-31 23:04:03,088 >>> Training Loss: 0.000000135657359 ### Testing Loss: 0.000000306359453
2023-03-31 23:04:03,088 ===> Epoch[876/1000]
2023-03-31 23:04:27,839 >>> Training Loss: 0.000000132334876 ### Testing Loss: 0.000000305720562
2023-03-31 23:04:27,839 ===> Epoch[877/1000]
2023-03-31 23:04:51,539 >>> Training Loss: 0.000000131633996 ### Testing Loss: 0.000000314459413
2023-03-31 23:04:51,539 ===> Epoch[878/1000]
2023-03-31 23:05:15,269 >>> Training Loss: 0.000000132274124 ### Testing Loss: 0.000000304804274
2023-03-31 23:05:15,269 ===> Epoch[879/1000]
2023-03-31 23:05:39,059 >>> Training Loss: 0.000000151822590 ### Testing Loss: 0.000000308417498
2023-03-31 23:05:39,059 ===> Epoch[880/1000]
2023-03-31 23:06:02,749 >>> Training Loss: 0.000000140908085 ### Testing Loss: 0.000000306426983
2023-03-31 23:06:02,759 ===> Epoch[881/1000]
2023-03-31 23:06:26,533 >>> Training Loss: 0.000000130575813 ### Testing Loss: 0.000000309589694
2023-03-31 23:06:26,533 ===> Epoch[882/1000]
2023-03-31 23:06:50,363 >>> Training Loss: 0.000000132635307 ### Testing Loss: 0.000000304723869
2023-03-31 23:06:50,363 ===> Epoch[883/1000]
2023-03-31 23:07:14,183 >>> Training Loss: 0.000000127240838 ### Testing Loss: 0.000000311930137
2023-03-31 23:07:14,183 ===> Epoch[884/1000]
2023-03-31 23:07:38,003 >>> Training Loss: 0.000000128029470 ### Testing Loss: 0.000000304515993
2023-03-31 23:07:38,003 ===> Epoch[885/1000]
2023-03-31 23:08:01,743 >>> Training Loss: 0.000000124936292 ### Testing Loss: 0.000000304036661
2023-03-31 23:08:01,743 ===> Epoch[886/1000]
2023-03-31 23:08:25,543 >>> Training Loss: 0.000000124637381 ### Testing Loss: 0.000000307116636
2023-03-31 23:08:25,543 ===> Epoch[887/1000]
2023-03-31 23:08:49,343 >>> Training Loss: 0.000000124980801 ### Testing Loss: 0.000000305518569
2023-03-31 23:08:49,343 ===> Epoch[888/1000]
2023-03-31 23:09:13,134 >>> Training Loss: 0.000000125335191 ### Testing Loss: 0.000000304733248
2023-03-31 23:09:13,134 ===> Epoch[889/1000]
2023-03-31 23:09:36,944 >>> Training Loss: 0.000000133357673 ### Testing Loss: 0.000000304825761
2023-03-31 23:09:36,944 ===> Epoch[890/1000]
2023-03-31 23:10:00,717 >>> Training Loss: 0.000000125846753 ### Testing Loss: 0.000000306133558
2023-03-31 23:10:00,717 ===> Epoch[891/1000]
2023-03-31 23:10:24,537 >>> Training Loss: 0.000000126832674 ### Testing Loss: 0.000000305800739
2023-03-31 23:10:24,537 ===> Epoch[892/1000]
2023-03-31 23:10:48,257 >>> Training Loss: 0.000000127551971 ### Testing Loss: 0.000000308827225
2023-03-31 23:10:48,257 ===> Epoch[893/1000]
2023-03-31 23:11:11,997 >>> Training Loss: 0.000000129455557 ### Testing Loss: 0.000000313694841
2023-03-31 23:11:11,997 ===> Epoch[894/1000]
2023-03-31 23:11:35,747 >>> Training Loss: 0.000000132378489 ### Testing Loss: 0.000000306108575
2023-03-31 23:11:35,747 ===> Epoch[895/1000]
2023-03-31 23:11:59,497 >>> Training Loss: 0.000000134226596 ### Testing Loss: 0.000000307368310
2023-03-31 23:11:59,497 ===> Epoch[896/1000]
2023-03-31 23:12:23,307 >>> Training Loss: 0.000000130886605 ### Testing Loss: 0.000000306483571
2023-03-31 23:12:23,307 ===> Epoch[897/1000]
2023-03-31 23:12:47,017 >>> Training Loss: 0.000000131715069 ### Testing Loss: 0.000000305863182
2023-03-31 23:12:47,017 ===> Epoch[898/1000]
2023-03-31 23:13:10,764 >>> Training Loss: 0.000000129840117 ### Testing Loss: 0.000000306013874
2023-03-31 23:13:10,764 ===> Epoch[899/1000]
2023-03-31 23:13:34,576 >>> Training Loss: 0.000000136931448 ### Testing Loss: 0.000000309357205
2023-03-31 23:13:34,576 ===> Epoch[900/1000]
2023-03-31 23:13:58,346 >>> Training Loss: 0.000000134979288 ### Testing Loss: 0.000000305115606
2023-03-31 23:13:58,376 ===> Epoch[901/1000]
2023-03-31 23:14:22,156 >>> Training Loss: 0.000000131642651 ### Testing Loss: 0.000000318441920
2023-03-31 23:14:22,166 ===> Epoch[902/1000]
2023-03-31 23:14:45,936 >>> Training Loss: 0.000000128480423 ### Testing Loss: 0.000000306406662
2023-03-31 23:14:45,936 ===> Epoch[903/1000]
2023-03-31 23:15:09,707 >>> Training Loss: 0.000000125951900 ### Testing Loss: 0.000000304910685
2023-03-31 23:15:09,707 ===> Epoch[904/1000]
2023-03-31 23:15:33,427 >>> Training Loss: 0.000000127835733 ### Testing Loss: 0.000000306615533
2023-03-31 23:15:33,427 ===> Epoch[905/1000]
2023-03-31 23:15:57,068 >>> Training Loss: 0.000000125515896 ### Testing Loss: 0.000000304754366
2023-03-31 23:15:57,068 ===> Epoch[906/1000]
2023-03-31 23:16:20,838 >>> Training Loss: 0.000000129909154 ### Testing Loss: 0.000000304221885
2023-03-31 23:16:20,838 ===> Epoch[907/1000]
2023-03-31 23:16:44,498 >>> Training Loss: 0.000000124836035 ### Testing Loss: 0.000000307555240
2023-03-31 23:16:44,498 ===> Epoch[908/1000]
2023-03-31 23:17:08,350 >>> Training Loss: 0.000000128476032 ### Testing Loss: 0.000000306142198
2023-03-31 23:17:08,350 ===> Epoch[909/1000]
2023-03-31 23:17:32,110 >>> Training Loss: 0.000000128290608 ### Testing Loss: 0.000000308001177
2023-03-31 23:17:32,110 ===> Epoch[910/1000]
2023-03-31 23:17:55,990 >>> Training Loss: 0.000000130565027 ### Testing Loss: 0.000000307383317
2023-03-31 23:17:55,990 ===> Epoch[911/1000]
2023-03-31 23:18:19,780 >>> Training Loss: 0.000000130632444 ### Testing Loss: 0.000000307620581
2023-03-31 23:18:19,780 ===> Epoch[912/1000]
2023-03-31 23:18:43,430 >>> Training Loss: 0.000000129579760 ### Testing Loss: 0.000000312639713
2023-03-31 23:18:43,430 ===> Epoch[913/1000]
2023-03-31 23:19:07,231 >>> Training Loss: 0.000000129468191 ### Testing Loss: 0.000000306273023
2023-03-31 23:19:07,231 ===> Epoch[914/1000]
2023-03-31 23:19:32,143 >>> Training Loss: 0.000000128825747 ### Testing Loss: 0.000000308124271
2023-03-31 23:19:32,143 ===> Epoch[915/1000]
2023-03-31 23:19:55,823 >>> Training Loss: 0.000000129772715 ### Testing Loss: 0.000000306757983
2023-03-31 23:19:55,823 ===> Epoch[916/1000]
2023-03-31 23:20:19,608 >>> Training Loss: 0.000000135098773 ### Testing Loss: 0.000000307642267
2023-03-31 23:20:19,608 ===> Epoch[917/1000]
2023-03-31 23:20:43,238 >>> Training Loss: 0.000000127310926 ### Testing Loss: 0.000000308389104
2023-03-31 23:20:43,238 ===> Epoch[918/1000]
2023-03-31 23:21:07,088 >>> Training Loss: 0.000000125701320 ### Testing Loss: 0.000000305460986
2023-03-31 23:21:07,088 ===> Epoch[919/1000]
2023-03-31 23:21:30,848 >>> Training Loss: 0.000000127849830 ### Testing Loss: 0.000000307376979
2023-03-31 23:21:30,848 ===> Epoch[920/1000]
2023-03-31 23:21:54,608 >>> Training Loss: 0.000000133618158 ### Testing Loss: 0.000000306399755
2023-03-31 23:21:54,608 ===> Epoch[921/1000]
2023-03-31 23:22:18,378 >>> Training Loss: 0.000000125639104 ### Testing Loss: 0.000000308596583
2023-03-31 23:22:18,378 ===> Epoch[922/1000]
2023-03-31 23:22:42,138 >>> Training Loss: 0.000000125658758 ### Testing Loss: 0.000000305950408
2023-03-31 23:22:42,138 ===> Epoch[923/1000]
2023-03-31 23:23:05,408 >>> Training Loss: 0.000000125045418 ### Testing Loss: 0.000000307641614
2023-03-31 23:23:05,408 ===> Epoch[924/1000]
2023-03-31 23:23:28,748 >>> Training Loss: 0.000000128260112 ### Testing Loss: 0.000000310618958
2023-03-31 23:23:28,748 ===> Epoch[925/1000]
2023-03-31 23:23:52,162 >>> Training Loss: 0.000000136664170 ### Testing Loss: 0.000000328102061
2023-03-31 23:23:52,162 ===> Epoch[926/1000]
2023-03-31 23:24:15,602 >>> Training Loss: 0.000000146983695 ### Testing Loss: 0.000000307150088
2023-03-31 23:24:15,602 ===> Epoch[927/1000]
2023-03-31 23:24:38,992 >>> Training Loss: 0.000000129706805 ### Testing Loss: 0.000000304967273
2023-03-31 23:24:38,992 ===> Epoch[928/1000]
2023-03-31 23:25:02,382 >>> Training Loss: 0.000000125198213 ### Testing Loss: 0.000000308518594
2023-03-31 23:25:02,382 ===> Epoch[929/1000]
2023-03-31 23:25:25,772 >>> Training Loss: 0.000000123418175 ### Testing Loss: 0.000000305059046
2023-03-31 23:25:25,772 ===> Epoch[930/1000]
2023-03-31 23:25:49,122 >>> Training Loss: 0.000000134578144 ### Testing Loss: 0.000000306925187
2023-03-31 23:25:49,122 ===> Epoch[931/1000]
2023-03-31 23:26:12,532 >>> Training Loss: 0.000000131656336 ### Testing Loss: 0.000000308125351
2023-03-31 23:26:12,532 ===> Epoch[932/1000]
2023-03-31 23:26:36,002 >>> Training Loss: 0.000000125974211 ### Testing Loss: 0.000000311372332
2023-03-31 23:26:36,002 ===> Epoch[933/1000]
2023-03-31 23:26:59,402 >>> Training Loss: 0.000000125952809 ### Testing Loss: 0.000000307828884
2023-03-31 23:26:59,402 ===> Epoch[934/1000]
2023-03-31 23:27:22,817 >>> Training Loss: 0.000000125793775 ### Testing Loss: 0.000000304459093
2023-03-31 23:27:22,817 ===> Epoch[935/1000]
2023-03-31 23:27:46,159 >>> Training Loss: 0.000000124608022 ### Testing Loss: 0.000000305055636
2023-03-31 23:27:46,159 ===> Epoch[936/1000]
2023-03-31 23:28:09,489 >>> Training Loss: 0.000000125870400 ### Testing Loss: 0.000000306245084
2023-03-31 23:28:09,489 ===> Epoch[937/1000]
2023-03-31 23:28:32,809 >>> Training Loss: 0.000000129573280 ### Testing Loss: 0.000000306943150
2023-03-31 23:28:32,809 ===> Epoch[938/1000]
2023-03-31 23:28:56,239 >>> Training Loss: 0.000000137529710 ### Testing Loss: 0.000000308995084
2023-03-31 23:28:56,239 ===> Epoch[939/1000]
2023-03-31 23:29:19,620 >>> Training Loss: 0.000000130207155 ### Testing Loss: 0.000000304811294
2023-03-31 23:29:19,620 ===> Epoch[940/1000]
2023-03-31 23:29:42,960 >>> Training Loss: 0.000000131063146 ### Testing Loss: 0.000000311228405
2023-03-31 23:29:42,960 ===> Epoch[941/1000]
2023-03-31 23:30:06,430 >>> Training Loss: 0.000000127415177 ### Testing Loss: 0.000000308461694
2023-03-31 23:30:06,430 ===> Epoch[942/1000]
2023-03-31 23:30:29,842 >>> Training Loss: 0.000000125133070 ### Testing Loss: 0.000000305974140
2023-03-31 23:30:29,842 ===> Epoch[943/1000]
2023-03-31 23:30:53,122 >>> Training Loss: 0.000000122455958 ### Testing Loss: 0.000000307718580
2023-03-31 23:30:53,122 ===> Epoch[944/1000]
2023-03-31 23:31:16,527 >>> Training Loss: 0.000000132401098 ### Testing Loss: 0.000000306512817
2023-03-31 23:31:16,527 ===> Epoch[945/1000]
2023-03-31 23:31:40,027 >>> Training Loss: 0.000000126734875 ### Testing Loss: 0.000000306655380
2023-03-31 23:31:40,027 ===> Epoch[946/1000]
2023-03-31 23:32:03,447 >>> Training Loss: 0.000000129294023 ### Testing Loss: 0.000000305910049
2023-03-31 23:32:03,447 ===> Epoch[947/1000]
2023-03-31 23:32:26,917 >>> Training Loss: 0.000000134269740 ### Testing Loss: 0.000000306462510
2023-03-31 23:32:26,917 ===> Epoch[948/1000]
2023-03-31 23:32:50,327 >>> Training Loss: 0.000000124320835 ### Testing Loss: 0.000000307858045
2023-03-31 23:32:50,327 ===> Epoch[949/1000]
2023-03-31 23:33:13,727 >>> Training Loss: 0.000000124629096 ### Testing Loss: 0.000000306755766
2023-03-31 23:33:13,727 ===> Epoch[950/1000]
2023-03-31 23:33:37,107 >>> Training Loss: 0.000000123605346 ### Testing Loss: 0.000000306427154
2023-03-31 23:33:37,107 ===> Epoch[951/1000]
2023-03-31 23:34:00,547 >>> Training Loss: 0.000000123705632 ### Testing Loss: 0.000000308384898
2023-03-31 23:34:00,547 ===> Epoch[952/1000]
2023-03-31 23:34:25,119 >>> Training Loss: 0.000000127389853 ### Testing Loss: 0.000000311022347
2023-03-31 23:34:25,119 ===> Epoch[953/1000]
2023-03-31 23:34:48,456 >>> Training Loss: 0.000000129805755 ### Testing Loss: 0.000000312638605
2023-03-31 23:34:48,456 ===> Epoch[954/1000]
2023-03-31 23:35:11,776 >>> Training Loss: 0.000000131414481 ### Testing Loss: 0.000000308132826
2023-03-31 23:35:11,776 ===> Epoch[955/1000]
2023-03-31 23:35:35,066 >>> Training Loss: 0.000000126076699 ### Testing Loss: 0.000000309976372
2023-03-31 23:35:35,066 ===> Epoch[956/1000]
2023-03-31 23:35:58,456 >>> Training Loss: 0.000000124688142 ### Testing Loss: 0.000000308903026
2023-03-31 23:35:58,456 ===> Epoch[957/1000]
2023-03-31 23:36:21,965 >>> Training Loss: 0.000000125988876 ### Testing Loss: 0.000000309679507
2023-03-31 23:36:21,965 ===> Epoch[958/1000]
2023-03-31 23:36:45,376 >>> Training Loss: 0.000000130963315 ### Testing Loss: 0.000000307933192
2023-03-31 23:36:45,376 ===> Epoch[959/1000]
2023-03-31 23:37:08,776 >>> Training Loss: 0.000000127846505 ### Testing Loss: 0.000000307855146
2023-03-31 23:37:08,776 ===> Epoch[960/1000]
2023-03-31 23:37:32,276 >>> Training Loss: 0.000000127772523 ### Testing Loss: 0.000000305806537
2023-03-31 23:37:32,276 ===> Epoch[961/1000]
2023-03-31 23:37:55,583 >>> Training Loss: 0.000000126813219 ### Testing Loss: 0.000000305851358
2023-03-31 23:37:55,583 ===> Epoch[962/1000]
2023-03-31 23:38:18,894 >>> Training Loss: 0.000000125119058 ### Testing Loss: 0.000000306080210
2023-03-31 23:38:18,894 ===> Epoch[963/1000]
2023-03-31 23:38:42,253 >>> Training Loss: 0.000000126274358 ### Testing Loss: 0.000000314063897
2023-03-31 23:38:42,253 ===> Epoch[964/1000]
2023-03-31 23:39:05,663 >>> Training Loss: 0.000000124849151 ### Testing Loss: 0.000000306086775
2023-03-31 23:39:05,663 ===> Epoch[965/1000]
2023-03-31 23:39:29,093 >>> Training Loss: 0.000000126705515 ### Testing Loss: 0.000000323289584
2023-03-31 23:39:29,093 ===> Epoch[966/1000]
2023-03-31 23:39:52,503 >>> Training Loss: 0.000000130408665 ### Testing Loss: 0.000000304631527
2023-03-31 23:39:52,503 ===> Epoch[967/1000]
2023-03-31 23:40:15,853 >>> Training Loss: 0.000000130328786 ### Testing Loss: 0.000000311323930
2023-03-31 23:40:15,853 ===> Epoch[968/1000]
2023-03-31 23:40:39,224 >>> Training Loss: 0.000000126880238 ### Testing Loss: 0.000000308639045
2023-03-31 23:40:39,224 ===> Epoch[969/1000]
2023-03-31 23:41:02,614 >>> Training Loss: 0.000000123304176 ### Testing Loss: 0.000000304977988
2023-03-31 23:41:02,614 ===> Epoch[970/1000]
2023-03-31 23:41:26,000 >>> Training Loss: 0.000000122379930 ### Testing Loss: 0.000000306566221
2023-03-31 23:41:26,000 ===> Epoch[971/1000]
2023-03-31 23:41:49,446 >>> Training Loss: 0.000000121842405 ### Testing Loss: 0.000000356707829
2023-03-31 23:41:49,446 ===> Epoch[972/1000]
2023-03-31 23:42:12,967 >>> Training Loss: 0.000000128524590 ### Testing Loss: 0.000000309595123
2023-03-31 23:42:12,967 ===> Epoch[973/1000]
2023-03-31 23:42:36,327 >>> Training Loss: 0.000000124551633 ### Testing Loss: 0.000000310263914
2023-03-31 23:42:36,327 ===> Epoch[974/1000]
2023-03-31 23:42:59,777 >>> Training Loss: 0.000000123678220 ### Testing Loss: 0.000000306558860
2023-03-31 23:42:59,777 ===> Epoch[975/1000]
2023-03-31 23:43:23,157 >>> Training Loss: 0.000000124683169 ### Testing Loss: 0.000000309448410
2023-03-31 23:43:23,157 ===> Epoch[976/1000]
2023-03-31 23:43:46,607 >>> Training Loss: 0.000000125028919 ### Testing Loss: 0.000000305890381
2023-03-31 23:43:46,607 ===> Epoch[977/1000]
2023-03-31 23:44:09,997 >>> Training Loss: 0.000000125172733 ### Testing Loss: 0.000000308649078
2023-03-31 23:44:09,997 ===> Epoch[978/1000]
2023-03-31 23:44:33,397 >>> Training Loss: 0.000000129480469 ### Testing Loss: 0.000000304889085
2023-03-31 23:44:33,397 ===> Epoch[979/1000]
2023-03-31 23:44:56,821 >>> Training Loss: 0.000000124737127 ### Testing Loss: 0.000000306214417
2023-03-31 23:44:56,821 ===> Epoch[980/1000]
2023-03-31 23:45:20,251 >>> Training Loss: 0.000000123368224 ### Testing Loss: 0.000000308167614
2023-03-31 23:45:20,251 ===> Epoch[981/1000]
2023-03-31 23:45:43,791 >>> Training Loss: 0.000000124021241 ### Testing Loss: 0.000000312987879
2023-03-31 23:45:43,791 ===> Epoch[982/1000]
2023-03-31 23:46:07,871 >>> Training Loss: 0.000000135901189 ### Testing Loss: 0.000000307783068
2023-03-31 23:46:07,871 ===> Epoch[983/1000]
2023-03-31 23:46:31,292 >>> Training Loss: 0.000000125774321 ### Testing Loss: 0.000000307710792
2023-03-31 23:46:31,292 ===> Epoch[984/1000]
2023-03-31 23:46:54,622 >>> Training Loss: 0.000000126175919 ### Testing Loss: 0.000000307675521
2023-03-31 23:46:54,622 ===> Epoch[985/1000]
2023-03-31 23:47:17,922 >>> Training Loss: 0.000000126374260 ### Testing Loss: 0.000000311510433
2023-03-31 23:47:17,922 ===> Epoch[986/1000]
2023-03-31 23:47:41,142 >>> Training Loss: 0.000000129779153 ### Testing Loss: 0.000000307774513
2023-03-31 23:47:41,142 ===> Epoch[987/1000]
2023-03-31 23:48:04,402 >>> Training Loss: 0.000000130581228 ### Testing Loss: 0.000000306736212
2023-03-31 23:48:04,402 ===> Epoch[988/1000]
2023-03-31 23:48:27,630 >>> Training Loss: 0.000000123363932 ### Testing Loss: 0.000000305727866
2023-03-31 23:48:27,630 ===> Epoch[989/1000]
2023-03-31 23:48:51,020 >>> Training Loss: 0.000000123110482 ### Testing Loss: 0.000000306564999
2023-03-31 23:48:51,020 ===> Epoch[990/1000]
2023-03-31 23:49:14,410 >>> Training Loss: 0.000000123934541 ### Testing Loss: 0.000000305106482
2023-03-31 23:49:14,410 ===> Epoch[991/1000]
2023-03-31 23:49:38,682 >>> Training Loss: 0.000000121813969 ### Testing Loss: 0.000000309251362
2023-03-31 23:49:38,682 ===> Epoch[992/1000]
2023-03-31 23:50:02,062 >>> Training Loss: 0.000000122592368 ### Testing Loss: 0.000000307195563
2023-03-31 23:50:02,062 ===> Epoch[993/1000]
2023-03-31 23:50:25,442 >>> Training Loss: 0.000000124942758 ### Testing Loss: 0.000000311208225
2023-03-31 23:50:25,442 ===> Epoch[994/1000]
2023-03-31 23:50:48,852 >>> Training Loss: 0.000000130602544 ### Testing Loss: 0.000000307595087
2023-03-31 23:50:48,852 ===> Epoch[995/1000]
2023-03-31 23:51:12,182 >>> Training Loss: 0.000000122018406 ### Testing Loss: 0.000000305462009
2023-03-31 23:51:12,182 ===> Epoch[996/1000]
2023-03-31 23:51:35,542 >>> Training Loss: 0.000000124458595 ### Testing Loss: 0.000000304594494
2023-03-31 23:51:35,542 ===> Epoch[997/1000]
2023-03-31 23:51:58,867 >>> Training Loss: 0.000000128307661 ### Testing Loss: 0.000000306724928
2023-03-31 23:51:58,867 ===> Epoch[998/1000]
2023-03-31 23:52:22,287 >>> Training Loss: 0.000000123193317 ### Testing Loss: 0.000000305951687
2023-03-31 23:52:22,287 ===> Epoch[999/1000]
2023-03-31 23:52:45,647 >>> Training Loss: 0.000000122092018 ### Testing Loss: 0.000000309275237
2023-03-31 23:52:45,647 ===> Epoch[1000/1000]
2023-03-31 23:53:09,057 >>> Training Loss: 0.000000122702232 ### Testing Loss: 0.000000306564203
